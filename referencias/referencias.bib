@article{baumer2015,
  title = {R {{Markdown}}},
  author = {Baumer, Benjamin and Udwin, Dana},
  year = {2015},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {7},
  number = {3},
  pages = {167--177},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1348},
  urldate = {2025-10-10},
  abstract = {Reproducibility is increasingly important to statistical research, but many details are often omitted from the published version of complex statistical analyses. A reader's comprehension is limited to what the author concludes, without exposure to the computational process. Often, the industrious reader cannot expand upon or validate the author's results. Even the author may struggle to reproduce their own results upon revisiting them. R Markdown is an authoring syntax that combines the ease of Markdown with the statistical programming language R. An R Markdown document or presentation interweaves computation, output and written analysis to the effect of transparency, clarity and an inherent invitation to reproduce (especially as sharing data is now as easy as the click of a button). It is an open-source tool that can be used either on its own or through the               RStudio               integrated development environment (               IDE               ). In addition to facilitating reproducible research, R Markdown is a boon to collaboratively minded data analysts, whose workflow can be streamlined by sharing only one master document that contains both code and content. Statistics educators may also find that R Markdown is helpful as a homework template, for both ease-of-use and in discouraging students from copy-and-pasting results from classmates. Training students in R Markdown will introduce to the workforce a new class of data analysts with an ingrained, foundational inclination toward reproducible research.               WIREs Comput Stat               2015, 7:167--177. doi: 10.1002/wics.1348                                         This article is categorized under:                                                   Software for Computational Statistics {$>$} Software/Statistical Software},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/santi/Zotero/storage/A6Q6W7YF/Baumer and Udwin - 2015 - R Markdown.pdf}
}

@article{benson2013,
  title = {{{GenBank}}},
  author = {Benson, Dennis A. and Cavanaugh, Mark and Clark, Karen and {Karsch-Mizrachi}, Ilene and Lipman, David J. and Ostell, James and Sayers, Eric W.},
  year = {2013},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {41},
  number = {D1},
  pages = {D36-D42},
  issn = {0305-1048},
  doi = {10.1093/nar/gks1195},
  urldate = {2025-10-10},
  abstract = {GenBank{\textregistered} (http://www.ncbi.nlm.nih.gov) is a comprehensive database that contains publicly available nucleotide sequences for almost 260 000 formally described species. These sequences are obtained primarily through submissions from individual laboratories and batch submissions from large-scale sequencing projects, including whole-genome shotgun (WGS) and environmental sampling projects. Most submissions are made using the web-based BankIt or standalone Sequin programs, and GenBank staff assigns accession numbers upon data receipt. Daily data exchange with the European Nucleotide Archive (ENA) and the DNA Data Bank of Japan (DDBJ) ensures worldwide coverage. GenBank is accessible through the NCBI Entrez retrieval system, which integrates data from the major DNA and protein sequence databases along with taxonomy, genome, mapping, protein structure and domain information, and the biomedical journal literature via PubMed. BLAST provides sequence similarity searches of GenBank and other sequence databases. Complete bimonthly releases and daily updates of the GenBank database are available by FTP. To access GenBank and its related retrieval and analysis services, begin at the NCBI home page: www.ncbi.nlm.nih.gov.},
  file = {/home/santi/Zotero/storage/EC8LD5KR/Benson et al. - 2013 - GenBank.pdf}
}

@article{boyle2009,
  title = {Adaptable Data Management for Systems Biology Investigations},
  author = {Boyle, John and Rovira, Hector and Cavnor, Chris and Burdick, David and Killcoyne, Sarah and Shmulevich, Ilya},
  year = {2009},
  month = mar,
  journal = {BMC Bioinformatics},
  volume = {10},
  number = {1},
  pages = {79},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-10-79},
  urldate = {2025-10-11},
  abstract = {Within research each experiment is different, the focus changes and the data is generated from a continually evolving barrage of technologies. There is a continual introduction of new techniques whose usage ranges from in-house protocols through to high-throughput instrumentation. To support these requirements data management systems are needed that can be rapidly built and readily adapted for new usage.},
  keywords = {Aggregation System,Content Management System,Data Management System,Rich Functionality,XPATH Query},
  file = {/home/santi/Zotero/storage/JASNGZ87/Boyle et al. - 2009 - Adaptable data management for systems biology investigations.pdf;/home/santi/Zotero/storage/UUKWJ4P9/1471-2105-10-79.html}
}

@article{brusic2006,
  title = {The Growth of Bioinformatics},
  author = {Brusic, V.},
  year = {2006},
  month = dec,
  journal = {Briefings in Bioinformatics},
  volume = {8},
  number = {2},
  pages = {69--70},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbm008},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/BMKBRTFT/Brusic - 2006 - The growth of bioinformatics.pdf}
}

@article{chiu2019,
  title = {Clinical Metagenomics},
  author = {Chiu, Charles Y. and Miller, Steven A.},
  year = {2019},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {20},
  number = {6},
  pages = {341--355},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/s41576-019-0113-7},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/KV6GWUAD/Chiu and Miller - 2019 - Clinical metagenomics.pdf}
}

@article{daly2008,
  title = {Physiographically Sensitive Mapping of Climatological Temperature and Precipitation across the Conterminous {{United States}}},
  author = {Daly, Christopher and Halbleib, Michael and Smith, Joseph I. and Gibson, Wayne P. and Doggett, Matthew K. and Taylor, George H. and Curtis, Jan and Pasteris, Phillip P.},
  year = {2008},
  month = dec,
  journal = {International Journal of Climatology},
  volume = {28},
  number = {15},
  pages = {2031--2064},
  issn = {0899-8418, 1097-0088},
  doi = {10.1002/joc.1688},
  urldate = {2025-09-30},
  abstract = {Abstract             Spatial climate data sets of 1971--2000 mean monthly precipitation and minimum and maximum temperature were developed for the conterminous United States. These 30-arcsec ({$\sim$}800-m) grids are the official spatial climate data sets of the U.S. Department of Agriculture. The PRISM (Parameter-elevation Relationships on Independent Slopes Model) interpolation method was used to develop data sets that reflected, as closely as possible, the current state of knowledge of spatial climate patterns in the United States. PRISM calculates a climate--elevation regression for each digital elevation model (DEM) grid cell, and stations entering the regression are assigned weights based primarily on the physiographic similarity of the station to the grid cell. Factors considered are location, elevation, coastal proximity, topographic facet orientation, vertical atmospheric layer, topographic position, and orographic effectiveness of the terrain. Surface stations used in the analysis numbered nearly 13 000 for precipitation and 10 000 for temperature. Station data were spatially quality controlled, and short-period-of-record averages adjusted to better reflect the 1971--2000 period.             PRISM interpolation uncertainties were estimated with cross-validation (C-V) mean absolute error (MAE) and the 70\% prediction interval of the climate--elevation regression function. The two measures were not well correlated at the point level, but were similar when averaged over large regions. The PRISM data set was compared with the WorldClim and Daymet spatial climate data sets. The comparison demonstrated that using a relatively dense station data set and the physiographically sensitive PRISM interpolation process resulted in substantially improved climate grids over those of WorldClim and Daymet. The improvement varied, however, depending on the complexity of the region. Mountainous and coastal areas of the western United States, characterized by sparse data coverage, large elevation gradients, rain shadows, inversions, cold air drainage, and coastal effects, showed the greatest improvement. The PRISM data set benefited from a peer review procedure that incorporated local knowledge and data into the development process. Copyright {\copyright} 2008 Royal Meteorological Society},
  langid = {english},
  file = {/home/santi/Zotero/storage/SA4R6Y5K/Daly et al. - 2008 - Physiographically sensitive mapping of climatological temperature and precipitation across the conte.pdf}
}

@article{daswito2023,
  title = {Analysis {{Using R Software}}: {{A Big Opportunity}} for {{Epidemiology}} and {{Public Health Data Analysis}}},
  shorttitle = {Analysis {{Using R Software}}},
  author = {Daswito, Rinaldi and Besral, Besral and Ilmaskal, Radian},
  year = {2023},
  month = jul,
  journal = {Journal of Health Sciences and Epidemiology},
  volume = {1},
  number = {1},
  pages = {1--5},
  issn = {2988-7283},
  doi = {10.62404/jhse.v1i1.9},
  urldate = {2025-10-10},
  abstract = {R is a programming language, open-source, developed by various of the world's most active statisticians with powerful function and visualization for data analysis from simple to complex data such as machine learning and artificial intelligence. Data visualization technologies have the ability to assist public health professionals with decision-making. Visualization appears to help decision making by increasing the quantity of information communicated and reducing the cognitive and intellectual strain of processing information. There are numerous commercially available statistical software packages that are widely utilized by epidemiologists worldwide. For industrialized nations, the price of software is not a significant issue. However, for underdeveloped nations, the true expenses are frequently excessive. Some academics in developing nations rely on software that has been illegally copied a copy of the software program. There are several benefits to using R, including the possibility of using software packages for free (open source) and the volume and availability of software packages. It is simple to retain and repeat commands on the same data analysis with multiple data frames, facilitating the work of health monitoring officers who frequently analyze data with similar variables but at different times.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  file = {/home/santi/Zotero/storage/B35F2QJZ/Daswito et al. - 2023 - Analysis Using R Software A Big Opportunity for Epidemiology and Public Health Data Analysis.pdf}
}

@book{dolstra2006,
  title = {The Purely Functional Software Deployment Model},
  author = {Dolstra, Eelco},
  year = {2006},
  publisher = {s.n.},
  address = {S.l.},
  isbn = {978-90-393-4130-8},
  langid = {english},
  annotation = {OCLC: 71702886},
  file = {/home/santi/Zotero/storage/CJNYAKZ5/Dolstra - 2006 - The purely functional software deployment model.pdf}
}

@article{drost2017,
  title = {Biomartr: Genomic Data Retrieval with {{R}}},
  shorttitle = {Biomartr},
  author = {Drost, Hajk-Georg and Paszkowski, Jerzy},
  year = {2017},
  month = apr,
  journal = {Bioinformatics},
  volume = {33},
  number = {8},
  pages = {1216--1217},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btw821},
  urldate = {2025-10-10},
  abstract = {Motivation Retrieval and reproducible functional annotation of genomic data are crucial in biology. However, the current poor usability and transparency of retrieval methods hinders reproducibility. Here we present an open source R package, biomartr, which provides a comprehensive easy-to-use framework for automating data retrieval and functional annotation for meta-genomic approaches. The functions of biomartr achieve a high degree of clarity, transparency and reproducibility of analyses. Results The biomartr package implements straightforward functions for bulk retrieval of all genomic data or data for selected genomes, proteomes, coding sequences and annotation files present in databases hosted by the National Center for Biotechnology Information (NCBI) and European Bioinformatics Institute (EMBL-EBI). In addition, biomartr communicates with the BioMart database for functional annotation of retrieved sequences. Comprehensive documentation of biomartr functions and five tutorial vignettes provide step-by-step instructions on how to use the package in a reproducible manner. Availability and Implementation The open source biomartr package is available at https://github.com/HajkD/biomartr and https://cran.r-project.org/web/packages/biomartr/index.html. Supplementary information  are available at Bioinformatics online.},
  pmcid = {PMC5408848},
  pmid = {28110292},
  file = {/home/santi/Zotero/storage/7HQI4RMX/Drost and Paszkowski - 2017 - Biomartr genomic data retrieval with R.pdf}
}

@article{drost2018,
  title = {{{myTAI}}: Evolutionary Transcriptomics with {{R}}},
  shorttitle = {{{myTAI}}},
  author = {Drost, Hajk-Georg and Gabel, Alexander and Liu, Jialin and Quint, Marcel and Grosse, Ivo},
  year = {2018},
  month = may,
  journal = {Bioinformatics},
  volume = {34},
  number = {9},
  pages = {1589--1590},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btx835},
  urldate = {2025-09-30},
  abstract = {Next Generation Sequencing (NGS) technologies generate a large amount of high quality transcriptome datasets enabling the investigation of molecular processes on a genomic and metagenomic scale. These transcriptomics studies aim to quantify and compare the molecular phenotypes of the biological processes at hand. Despite the vast increase of available transcriptome datasets, little is known about the evolutionary conservation of those characterized transcriptomes.The myTAI package implements exploratory analysis functions to infer transcriptome conservation patterns in any transcriptome dataset. Comprehensive documentation of myTAI functions and tutorial vignettes provide step-by-step instructions on how to use the package in an exploratory and computationally reproducible manner.The open source myTAI package is available at https://github.com/HajkD/myTAI and https://cran.r-project.org/web/packages/myTAI/index.html.Supplementary data are available at Bioinformatics online.},
  file = {/home/santi/Zotero/storage/K2S6ZL3Z/Drost et al. - 2018 - myTAI evolutionary transcriptomics with R.pdf;/home/santi/Zotero/storage/QJXLLA2X/btx835.html}
}

@article{erickson2024,
  title = {A {{Reproducible Manuscript Workflow With}} a {{Quarto Template}}},
  author = {Erickson, Richard A. and Archer, Althea A. and Fienen, Michael N.},
  year = {2024},
  month = jun,
  journal = {Journal of Fish and Wildlife Management},
  volume = {15},
  number = {1},
  pages = {251--258},
  issn = {1944-687X},
  doi = {10.3996/JFWM-24-003},
  urldate = {2025-10-10},
  abstract = {Abstract             Scientists and resource managers increasingly use Markdown-based tools to create reproducible reports and manuscripts. These workflows allow people to use standardized methods that are more reproducible, efficient, and transparent than other standard office tools. We present a Quarto template and demonstrate how this template may be used for the Journal of Fish and Wildlife Management. This template may also be readily adapted to other journals that use Microsoft Word-based workflows and for other product types such as annual reports. We also provide a high-level overview of Quarto and other Markdown-based workflows. Last, we provide examples of some features of the Quarto publishing system that may be helpful for authors when customizing Quarto templates for specific journal formatting requirements and other product types.},
  langid = {english},
  file = {/home/santi/Zotero/storage/YFQHWJJQ/Erickson et al. - 2024 - A Reproducible Manuscript Workflow With a Quarto Template.pdf}
}

@inproceedings{fang2025,
  title = {Enhancing {{Computational Notebooks}} with {{Code}}+{{Data Space Versioning}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fang, Hanxi and Chockchowwat, Supawit and Sundaram, Hari and Park, Yongjoo},
  year = {2025},
  month = apr,
  pages = {1--17},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714141},
  urldate = {2025-10-10},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/home/santi/Zotero/storage/ZZXU6WEB/Fang et al. - 2025 - Enhancing Computational Notebooks with Code+Data Space Versioning.pdf}
}

@misc{finak2018,
  title = {{{DataPackageR}}: {{Reproducible}} Data Preprocessing, Standardization and Sharing Using {{R}}/{{Bioconductor}} for Collaborative Data Analysis: {{A Practical Approach}} to {{Wrangling Multi-Assay Data Sets}} for {{Standardized}} and {{Reproducible Data Analysis}}},
  shorttitle = {{{DataPackageR}}},
  author = {Finak, Greg and Mayer, Bryan T. and Fulp, William and Obrecht, Paul and Sato, Alicia and Chung, Eva and Holman, Drienna and Gottardo, Raphael},
  year = {2018},
  month = jun,
  publisher = {Bioinformatics},
  doi = {10.1101/342907},
  urldate = {2025-10-10},
  abstract = {Abstract                        A central tenet of reproducible research is that scientific results are published along with the underlying data and software code necessary to reproduce and verify the findings. A host of tools and software have been released that facilitate such work-flows and scientific journals have increasingly demanded that code and primary data be made available with publications. There has been little practical advice on implementing reproducible research work-flows for large `omics' or systems biology data sets used by teams of analysts working in collaboration. In such instances it is important to ensure all analysts use the same version of a data set for their analyses. Yet, instantiating relational databases and standard operating procedures can be unwieldy, with high ``startup'' costs and poor adherence to procedures when they deviate substantially from an analyst's usual work-flow. Ideally a reproducible research work-flow should fit naturally into an individual's existing work-flow, with minimal disruption. Here, we provide an overview of how we have leveraged popular open source tools, including Bioconductor, Rmarkdown, git version control, R, and specifically R's package system combined with a new tool             DataPackageR,             to implement a lightweight reproducible research work-flow for preprocessing large data sets, suitable for sharing among small-to-medium sized teams of computational scientists. Our primary contribution is the             DataPackageR             tool, which decouples time-consuming data processing from data analysis while leaving a traceable record of how raw data is processed into analysis-ready data sets. The software ensures packaged data objects are properly documented and performs checksum verification of these along with basic package version management, and importantly, leaves a record of data processing code in the form of package vignettes. Our group has implemented this work-flow to manage, analyze and report on pre-clinical immunological trial data from multi-center, multi-assay studies for the past three years.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  file = {/home/santi/Zotero/storage/4XEYAFDD/Finak et al. - 2018 - DataPackageR Reproducible data preprocessing, standardization and sharing using RBioconductor for.pdf}
}

@misc{foster2018,
  title = {Taxa: {{An R}} Package Implementing Data Standards and Methods for Taxonomic Data},
  shorttitle = {Taxa},
  author = {Foster, Zachary S. L. and Chamberlain, Scott and Gr{\"u}nwald, Niklaus J.},
  year = {2018},
  month = mar,
  publisher = {F1000Research},
  doi = {10.12688/f1000research.14013.1},
  urldate = {2025-09-30},
  abstract = {The\&nbsp;taxa R package provides a set of tools for defining and manipulating taxonomic data. The recent and widespread application of DNA sequencing to community composition studies is making large data sets with taxonomic information commonplace. However, compared to typical tabular data, this information is encoded in many different ways and the hierarchical nature of taxonomic classifications makes it difficult to work with. There are many R packages that use taxonomic data to varying degrees but there is currently no cross-package standard for how this information is encoded and manipulated. We developed the R package\&nbsp;taxa to provide a robust and flexible solution to storing and manipulating taxonomic data in R and any application-specific information associated with it.\&nbsp;Taxa provides parsers that can read common sources of taxonomic information (taxon IDs, sequence IDs, taxon names, and classifications) from nearly any format while preserving associated data. Once parsed, the taxonomic data and any associated data can be manipulated using a cohesive set of functions modeled after the popular R package\&nbsp;dplyr. These functions take into account the hierarchical nature of taxa and can modify the taxonomy or associated data in such a way that both are kept in sync.\&nbsp;Taxa is currently being used by the\&nbsp;metacoder and\&nbsp;taxize packages, which provide broadly useful functionality that we hope will speed adoption by users and developers.},
  archiveprefix = {F1000Research},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {metacoder,R language,R package,rOpenSci,taxa,taxize,taxonomy},
  file = {/home/santi/Zotero/storage/P4CTUFU9/Foster et al. - 2018 - Taxa An R package implementing data standards and methods for taxonomic data.pdf}
}

@article{giorgi2022,
  title = {The {{R Language}}: {{An Engine}} for {{Bioinformatics}} and {{Data Science}}},
  shorttitle = {The {{R Language}}},
  author = {Giorgi, Federico M. and Ceraolo, Carmine and Mercatelli, Daniele},
  year = {2022},
  month = apr,
  journal = {Life},
  volume = {12},
  number = {5},
  pages = {648},
  issn = {2075-1729},
  doi = {10.3390/life12050648},
  urldate = {2025-10-10},
  abstract = {The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/santi/Zotero/storage/AY9PYQRY/Giorgi et al. - 2022 - The R Language An Engine for Bioinformatics and Data Science.pdf}
}

@article{goodwin2016,
  title = {Coming of Age: Ten Years of next-Generation Sequencing Technologies},
  shorttitle = {Coming of Age},
  author = {Goodwin, Sara and McPherson, John D. and McCombie, W. Richard},
  year = {2016},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {17},
  number = {6},
  pages = {333--351},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg.2016.49},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/WS3TITGS/Goodwin et al. - 2016 - Coming of age ten years of next-generation sequencing technologies.pdf}
}

@article{greene2014,
  title = {Big {{Data Bioinformatics}}},
  author = {Greene, Casey S. and Tan, Jie and Ung, Matthew and Moore, Jason H. and Cheng, Chao},
  year = {2014},
  month = dec,
  journal = {Journal of Cellular Physiology},
  volume = {229},
  number = {12},
  pages = {1896--1900},
  issn = {0021-9541, 1097-4652},
  doi = {10.1002/jcp.24662},
  urldate = {2025-10-10},
  abstract = {Recent technological advances allow for high throughput profiling of biological systems in a cost-efficient manner. The low cost of data generation is leading us to the ``big data'' era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both ``machine learning'' algorithms as well as ``unsupervised'' and ``supervised'' examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia. J. Cell. Physiol. 229: 1896--1900, 2014. {\copyright} 2014 Wiley Periodicals, Inc.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/santi/Zotero/storage/BACUS3MN/Greene et al. - 2014 - Big Data Bioinformatics.pdf}
}

@inproceedings{huang2018,
  title = {A {{Case Study}} of {{R Performance Analysis}} and {{Optimization}}},
  booktitle = {Proceedings of the {{Practice}} and {{Experience}} on {{Advanced Research Computing}}},
  author = {Huang, Ruizhu and Xu, Weijia and Liverani, Silvia and Hiltbrand, Dave and Stapleton, Ann E.},
  year = {2018},
  month = jul,
  pages = {1--6},
  publisher = {ACM},
  address = {Pittsburgh PA USA},
  doi = {10.1145/3219104.3219156},
  urldate = {2025-10-10},
  isbn = {978-1-4503-6446-1},
  langid = {english},
  file = {/home/santi/Zotero/storage/QNAGS6NV/Huang et al. - 2018 - A Case Study of R Performance Analysis and Optimization.pdf}
}

@article{jones2006,
  title = {The {{New Bioinformatics}}: {{Integrating Ecological Data}} from the {{Gene}} to the {{Biosphere}}},
  shorttitle = {The {{New Bioinformatics}}},
  author = {Jones, Matthew B. and Schildhauer, Mark P. and Reichman, O. J. and Bowers, Shawn},
  year = {2006},
  month = dec,
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  volume = {37},
  number = {Volume 37, 2006},
  pages = {519--544},
  publisher = {Annual Reviews},
  issn = {1543-592X, 1545-2069},
  doi = {10.1146/annurev.ecolsys.37.091305.110031},
  urldate = {2025-09-30},
  abstract = {Abstract Bioinformatics, the application of computational tools to the management and analysis of biological data, has stimulated rapid research advances in genomics through the development of data archives such as GenBank, and similar progress is just beginning within ecology. One reason for the belated adoption of informatics approaches in ecology is the breadth of ecologically pertinent data (from genes to the biosphere) and its highly heterogeneous nature. The variety of formats, logical structures, and sampling methods in ecology create significant challenges. Cultural barriers further impede progress, especially for the creation and adoption of data standards. Here we describe informatics frameworks for ecology, from subject-specific data warehouses, to generic data collections that use detailed metadata descriptions and formal ontologies to catalog and cross-reference information. Combining these approaches with automated data integration techniques and scientific workflow systems will maximize the value of data and open new frontiers for research in ecology.},
  langid = {english},
  file = {/home/santi/Zotero/storage/UNS7XW6P/Jones et al. - 2006 - The New Bioinformatics Integrating Ecological Data from the Gene to the Biosphere.pdf;/home/santi/Zotero/storage/JL7YM9D8/annurev.ecolsys.37.091305.html}
}

@article{landau2021,
  title = {The Targets {{R}} Package: A Dynamic {{Make-like}} Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing},
  shorttitle = {The Targets {{R}} Package},
  author = {Landau, William Michael},
  year = {2021},
  month = jan,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {57},
  pages = {2959},
  issn = {2475-9066},
  doi = {10.21105/joss.02959},
  urldate = {2025-10-11},
  abstract = {Landau, W. M., (2021). The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software, 6(57), 2959, https://doi.org/10.21105/joss.02959},
  langid = {english},
  file = {/home/santi/Zotero/storage/547QVIE3/Landau - 2021 - The targets R package a dynamic Make-like function-oriented pipeline toolkit for reproducibility an.pdf}
}

@article{luo2024,
  title = {Multiomics {{Research}}: {{Principles}} and {{Challenges}} in {{Integrated Analysis}}},
  shorttitle = {Multiomics {{Research}}},
  author = {Luo, Yunqing and Zhao, Chengjun and Chen, Fei},
  year = {2024},
  journal = {BioDesign Research},
  volume = {6},
  pages = {0059},
  issn = {26931257},
  doi = {10.34133/bdr.0059},
  urldate = {2025-10-10},
  langid = {english}
}

@article{marwick2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375986},
  urldate = {2025-10-11},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  keywords = {Computational science,Data science,Open source software,Reproducible research},
  file = {/home/santi/Zotero/storage/EGNU5VVB/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using R (and Friends).pdf}
}

@article{metzker2010,
  title = {Sequencing Technologies --- the next Generation},
  author = {Metzker, Michael L.},
  year = {2010},
  month = jan,
  journal = {Nature Reviews Genetics},
  volume = {11},
  number = {1},
  pages = {31--46},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg2626},
  urldate = {2025-10-10},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/santi/Zotero/storage/YMGPXKKP/Metzker - 2010 - Sequencing technologies — the next generation.pdf}
}

@article{nam2023,
  title = {Metagenomics: {{An Effective Approach}} for {{Exploring Microbial Diversity}} and {{Functions}}},
  shorttitle = {Metagenomics},
  author = {Nam, Nguyen and Do, Hoang and Loan Trinh, Kieu and Lee, Nae},
  year = {2023},
  month = may,
  journal = {Foods},
  volume = {12},
  number = {11},
  pages = {2140},
  issn = {2304-8158},
  doi = {10.3390/foods12112140},
  urldate = {2025-10-10},
  abstract = {Various fields have been identified in the ``omics'' era, such as genomics, proteomics, transcriptomics, metabolomics, phenomics, and metagenomics. Among these, metagenomics has enabled a significant increase in discoveries related to the microbial world. Newly discovered microbiomes in different ecologies provide meaningful information on the diversity and functions of microorganisms on the Earth. Therefore, the results of metagenomic studies have enabled new microbe-based applications in human health, agriculture, and the food industry, among others. This review summarizes the fundamental procedures on recent advances in bioinformatic tools. It also explores up-to-date applications of metagenomics in human health, food study, plant research, environmental sciences, and other fields. Finally, metagenomics is a powerful tool for studying the microbial world, and it still has numerous applications that are currently hidden and awaiting discovery. Therefore, this review also discusses the future perspectives of metagenomics.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/santi/Zotero/storage/8UW6AIGG/Nam et al. - 2023 - Metagenomics An Effective Approach for Exploring Microbial Diversity and Functions.pdf}
}

@article{noble2009,
  title = {A {{Quick Guide}} to {{Organizing Computational Biology Projects}}},
  author = {Noble, William Stafford},
  year = {2009},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {7},
  pages = {e1000424},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000424},
  urldate = {2025-10-11},
  langid = {english},
  keywords = {Bioinformatics,Biologists,Careers in research,Computational biology,Computer software,Human learning,Software engineering,Source code},
  file = {/home/santi/Zotero/storage/CGAP7E62/Noble - 2009 - A Quick Guide to Organizing Computational Biology Projects.pdf}
}

@article{pal2020,
  title = {Big Data in Biology: {{The}} Hope and Present-Day Challenges in It},
  shorttitle = {Big Data in Biology},
  author = {Pal, Subhajit and Mondal, Sudip and Das, Gourab and Khatua, Sunirmal and Ghosh, Zhumur},
  year = {2020},
  month = dec,
  journal = {Gene Reports},
  volume = {21},
  pages = {100869},
  issn = {24520144},
  doi = {10.1016/j.genrep.2020.100869},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/KEH3HJAX/Pal et al. - 2020 - Big data in biology The hope and present-day challenges in it.pdf}
}

@article{pruitt2007,
  title = {{{NCBI}} Reference Sequences ({{RefSeq}}): A Curated Non-Redundant Sequence Database of Genomes, Transcripts and Proteins},
  shorttitle = {{{NCBI}} Reference Sequences ({{RefSeq}})},
  author = {Pruitt, Kim D. and Tatusova, Tatiana and Maglott, Donna R.},
  year = {2007},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {35},
  number = {suppl\_1},
  pages = {D61-D65},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl842},
  urldate = {2025-09-30},
  abstract = {NCBI's reference sequence (RefSeq) database () is a curated non-redundant collection of sequences representing genomes, transcripts and proteins. The database includes 3774 organisms spanning prokaryotes, eukaryotes and viruses, and has records for 2 879 860 proteins (RefSeq release 19). RefSeq records integrate information from multiple sources, when additional data are available from those sources and therefore represent a current description of the sequence and its features. Annotations include coding regions, conserved domains, tRNAs, sequence tagged sites (STS), variation, references, gene and protein product names, and database cross-references. Sequence is reviewed and features are added using a combined approach of collaboration and other input from the scientific community, prediction, propagation from GenBank and curation by NCBI staff. The format of all RefSeq records is validated, and an increasing number of tests are being applied to evaluate the quality of sequence and annotation, especially in the context of complete genomic sequence.},
  file = {/home/santi/Zotero/storage/VYCT7U3T/Pruitt et al. - 2007 - NCBI reference sequences (RefSeq) a curated non-redundant sequence database of genomes, transcripts.pdf;/home/santi/Zotero/storage/KYDHF3Q8/gkl842.html}
}

@article{pruitt2007a,
  title = {{{NCBI}} Reference Sequences ({{RefSeq}}): A Curated Non-Redundant Sequence Database of Genomes, Transcripts and Proteins},
  shorttitle = {{{NCBI}} Reference Sequences ({{RefSeq}})},
  author = {Pruitt, Kim D. and Tatusova, Tatiana and Maglott, Donna R.},
  year = {2007},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {35},
  number = {suppl\_1},
  pages = {D61-D65},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl842},
  urldate = {2025-10-10},
  abstract = {NCBI's reference sequence (RefSeq) database () is a curated non-redundant collection of sequences representing genomes, transcripts and proteins. The database includes 3774 organisms spanning prokaryotes, eukaryotes and viruses, and has records for 2 879 860 proteins (RefSeq release 19). RefSeq records integrate information from multiple sources, when additional data are available from those sources and therefore represent a current description of the sequence and its features. Annotations include coding regions, conserved domains, tRNAs, sequence tagged sites (STS), variation, references, gene and protein product names, and database cross-references. Sequence is reviewed and features are added using a combined approach of collaboration and other input from the scientific community, prediction, propagation from GenBank and curation by NCBI staff. The format of all RefSeq records is validated, and an increasing number of tests are being applied to evaluate the quality of sequence and annotation, especially in the context of complete genomic sequence.},
  file = {/home/santi/Zotero/storage/UI5N9VXH/Pruitt et al. - 2007 - NCBI reference sequences (RefSeq) a curated non-redundant sequence database of genomes, transcripts.pdf}
}

@article{satam2023,
  title = {Next-{{Generation Sequencing Technology}}: {{Current Trends}} and {{Advancements}}},
  shorttitle = {Next-{{Generation Sequencing Technology}}},
  author = {Satam, Heena and Joshi, Kandarp and Mangrolia, Upasana and Waghoo, Sanober and Zaidi, Gulnaz and Rawool, Shravani and Thakare, Ritesh P. and Banday, Shahid and Mishra, Alok K. and Das, Gautam and Malonia, Sunil K.},
  year = {2023},
  month = jul,
  journal = {Biology},
  volume = {12},
  number = {7},
  pages = {997},
  issn = {2079-7737},
  doi = {10.3390/biology12070997},
  urldate = {2025-10-10},
  abstract = {The advent of next-generation sequencing (NGS) has brought about a paradigm shift in genomics research, offering unparalleled capabilities for analyzing DNA and RNA molecules in a high-throughput and cost-effective manner. This transformative technology has swiftly propelled genomics advancements across diverse domains. NGS allows for the rapid sequencing of millions of DNA fragments simultaneously, providing comprehensive insights into genome structure, genetic variations, gene expression profiles, and epigenetic modifications. The versatility of NGS platforms has expanded the scope of genomics research, facilitating studies on rare genetic diseases, cancer genomics, microbiome analysis, infectious diseases, and population genetics. Moreover, NGS has enabled the development of targeted therapies, precision medicine approaches, and improved diagnostic methods. This review provides an insightful overview of the current trends and recent advancements in NGS technology, highlighting its potential impact on diverse areas of genomic research. Moreover, the review delves into the challenges encountered and future directions of NGS technology, including endeavors to enhance the accuracy and sensitivity of sequencing data, the development of novel algorithms for data analysis, and the pursuit of more efficient, scalable, and cost-effective solutions that lie ahead.},
  langid = {english},
  file = {/home/santi/Zotero/storage/GHD7TF8Z/Satam et al. - 2023 - Next-Generation Sequencing Technology Current Trends and Advancements.pdf}
}

@article{smedley2009,
  title = {{{BioMart}} -- Biological Queries Made Easy},
  author = {Smedley, Damian and Haider, Syed and Ballester, Benoit and Holland, Richard and London, Darin and Thorisson, Gudmundur and Kasprzyk, Arek},
  year = {2009},
  month = jan,
  journal = {BMC Genomics},
  volume = {10},
  number = {1},
  pages = {22},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-10-22},
  urldate = {2025-10-10},
  abstract = {Biologists need to perform complex queries, often across a variety of databases. Typically, each data resource provides an advanced query interface, each of which must be learnt by the biologist before they can begin to query them. Frequently, more than one data source is required and for high-throughput analysis, cutting and pasting results between websites is certainly very time consuming. Therefore, many groups rely on local bioinformatics support to process queries by accessing the resource's programmatic interfaces if they exist. This is not an efficient solution in terms of cost and time. Instead, it would be better if the biologist only had to learn one generic interface. BioMart provides such a solution.},
  keywords = {Arrhythmogenic Right Ventricular Dysplasia,Central Portal,Complex Query,Distribute Annotation System,Ensembl Genome Browser},
  file = {/home/santi/Zotero/storage/SVL6KZSJ/Smedley et al. - 2009 - BioMart – biological queries made easy.pdf;/home/santi/Zotero/storage/68T3P8RX/1471-2164-10-22.html}
}

@article{wilson2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2025-10-11},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Metadata,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/santi/Zotero/storage/Y9D7QYXF/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf}
}

@incollection{xu2016,
  title = {Empowering {{R}} with {{High Performance Computing Resources}} for {{Big Data Analytics}}},
  booktitle = {Conquering {{Big Data}} with {{High Performance Computing}}},
  author = {Xu, Weijia and Huang, Ruizhu and Zhang, Hui and {El-Khamra}, Yaakoub and Walling, David},
  editor = {Arora, Ritu},
  year = {2016},
  pages = {191--217},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-33742-5_9},
  urldate = {2025-10-10},
  isbn = {978-3-319-33740-1 978-3-319-33742-5},
  langid = {english},
  file = {/home/santi/Zotero/storage/JH2XRYKJ/Xu et al. - 2016 - Empowering R with High Performance Computing Resources for Big Data Analytics.pdf}
}

@article{zhang2017,
  title = {{{BarcodingR}}: An Integrated r Package for Species Identification Using {{DNA}} Barcodes},
  shorttitle = {{{BarcodingR}}},
  author = {Zhang, Ai-bing and Hao, Meng-di and Yang, Cai-qing and Shi, Zhi-yong},
  year = {2017},
  journal = {Methods in Ecology and Evolution},
  volume = {8},
  number = {5},
  pages = {627--634},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12682},
  urldate = {2025-09-30},
  abstract = {Species identification via DNA barcodes has recently become an important and routine task in many biodiversity projects using DNA sequence data. Here, we present BarcodingR, an integrated software package that provides a comprehensive implementation of species identification methods, including artificial intelligence, fuzzy-set, Bayesian and kmer-based methods, that are not readily available in other packages. BarcodingR additionally provides new functions for barcode evaluation, barcoding gap analysis, delimitation comparison analysis, species membership analysis and consensus identification. Comparison with other barcoding methods using 11 empirical data sets indicates that on average, FZKMER (implemented in BarcodingR) and one extant barcoding method BRONX outperform all other methods examined in this study. Two other methods, BP and FZ (both implemented in BarcodingR), present similar performance as SVM and BLOG, respectively, and all display better performance than that of Jrip. The software of BarcodingR is open source under GNU General Public License and freely available for all major operating systems.},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {artificial intelligence,barcodes evaluation,BP-based species identification,DNA barcoding,DNA barcoding gaps,DNA taxonomy,fuzzy-set-theory-based approach,species identification},
  file = {/home/santi/Zotero/storage/CAASIQ3X/Zhang et al. - 2017 - BarcodingR an integrated r package for species identification using DNA barcodes.pdf;/home/santi/Zotero/storage/HA8LCNLY/2041-210X.html}
}

@article{zizka2019,
  title = {{{CoordinateCleaner}}: {{Standardized}} Cleaning of Occurrence Records from Biological Collection Databases},
  shorttitle = {{{CoordinateCleaner}}},
  author = {Zizka, Alexander and Silvestro, Daniele and Andermann, Tobias and Azevedo, Josu{\'e} and Duarte Ritter, Camila and Edler, Daniel and Farooq, Harith and Herdean, Andrei and Ariza, Mar{\'i}a and Scharn, Ruud and Svantesson, Sten and Wengstr{\"o}m, Niklas and Zizka, Vera and Antonelli, Alexandre},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {5},
  pages = {744--751},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13152},
  urldate = {2025-09-30},
  abstract = {Species occurrence records from online databases are an indispensable resource in ecological, biogeographical and palaeontological research. However, issues with data quality, especially incorrect geo-referencing or dating, can diminish their usefulness. Manual cleaning is time-consuming, error prone, difficult to reproduce and limited to known geographical areas and taxonomic groups, making it impractical for datasets with thousands or millions of records. Here, we present CoordinateCleaner, an r-package to scan datasets of species occurrence records for geo-referencing and dating imprecisions and data entry errors in a standardized and reproducible way. CoordinateCleaner is tailored to problems common in biological and palaeontological databases and can handle datasets with millions of records. The software includes (a) functions to flag potentially problematic coordinate records based on geographical gazetteers, (b) a global database of 9,691 geo-referenced biodiversity institutions to identify records that are likely from horticulture or captivity, (c) novel algorithms to identify datasets with rasterized data, conversion errors and strong decimal rounding and (d) spatio-temporal tests for fossils. We describe the individual functions available in CoordinateCleaner and demonstrate them on more than 90 million occurrences of flowering plants from the Global Biodiversity Information Facility (GBIF) and 19,000 fossil occurrences from the Palaeobiology Database (PBDB). We find that in GBIF more than 3.4 million records (3.7\%) are potentially problematic and that 179 of the tested contributing datasets (18.5\%) might be biased by rasterized coordinates. In PBDB, 1205 records (6.3\%) are potentially problematic. All cleaning functions and the biodiversity institution database are open-source and available within the CoordinateCleaner r-package.},
  copyright = {{\copyright} 2019 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {biodiversity institutions,data quality,fossils,GBIF,geo-referencing,palaeobiology database (PBDB),r package,species distribution modelling},
  file = {/home/santi/Zotero/storage/CWBPXEB8/Zizka et al. - 2019 - CoordinateCleaner Standardized cleaning of occurrence records from biological collection databases.pdf;/home/santi/Zotero/storage/YKA27GLV/2041-210X.html}
}
