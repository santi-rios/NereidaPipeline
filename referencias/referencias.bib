@incollection{2014,
  title = {Understanding {{SRA Search Results}}},
  booktitle = {{{SRA Knowledge Base}} [{{Internet}}]},
  year = {2014},
  month = mar,
  publisher = {National Center for Biotechnology Information (US)},
  urldate = {2025-10-16},
  langid = {english},
  file = {/home/santi/Zotero/storage/H68Y9XWB/NBK569234.html}
}

@article{banerjee2024,
  title = {Protocol for the Construction and Functional Profiling of Metagenome-Assembled Genomes for Microbiome Analyses},
  author = {Banerjee, Goutam and Papri, Suraya Rahman and Banerjee, Pratik},
  year = {2024},
  month = sep,
  journal = {STAR Protocols},
  volume = {5},
  number = {3},
  pages = {103167},
  issn = {26661667},
  doi = {10.1016/j.xpro.2024.103167},
  urldate = {2025-10-14},
  langid = {english}
}

@article{baumer2015,
  title = {R {{Markdown}}},
  author = {Baumer, Benjamin and Udwin, Dana},
  year = {2015},
  month = may,
  journal = {WIREs Computational Statistics},
  volume = {7},
  number = {3},
  pages = {167--177},
  issn = {1939-5108, 1939-0068},
  doi = {10.1002/wics.1348},
  urldate = {2025-10-10},
  abstract = {Reproducibility is increasingly important to statistical research, but many details are often omitted from the published version of complex statistical analyses. A reader's comprehension is limited to what the author concludes, without exposure to the computational process. Often, the industrious reader cannot expand upon or validate the author's results. Even the author may struggle to reproduce their own results upon revisiting them. R Markdown is an authoring syntax that combines the ease of Markdown with the statistical programming language R. An R Markdown document or presentation interweaves computation, output and written analysis to the effect of transparency, clarity and an inherent invitation to reproduce (especially as sharing data is now as easy as the click of a button). It is an open-source tool that can be used either on its own or through the               RStudio               integrated development environment (               IDE               ). In addition to facilitating reproducible research, R Markdown is a boon to collaboratively minded data analysts, whose workflow can be streamlined by sharing only one master document that contains both code and content. Statistics educators may also find that R Markdown is helpful as a homework template, for both ease-of-use and in discouraging students from copy-and-pasting results from classmates. Training students in R Markdown will introduce to the workforce a new class of data analysts with an ingrained, foundational inclination toward reproducible research.               WIREs Comput Stat               2015, 7:167--177. doi: 10.1002/wics.1348                                         This article is categorized under:                                                   Software for Computational Statistics {$>$} Software/Statistical Software},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/santi/Zotero/storage/A6Q6W7YF/Baumer and Udwin - 2015 - R Markdown.pdf}
}

@article{benson2013,
  title = {{{GenBank}}},
  author = {Benson, Dennis A. and Cavanaugh, Mark and Clark, Karen and {Karsch-Mizrachi}, Ilene and Lipman, David J. and Ostell, James and Sayers, Eric W.},
  year = {2013},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {41},
  number = {D1},
  pages = {D36-D42},
  issn = {0305-1048},
  doi = {10.1093/nar/gks1195},
  urldate = {2025-10-10},
  abstract = {GenBank{\textregistered} (http://www.ncbi.nlm.nih.gov) is a comprehensive database that contains publicly available nucleotide sequences for almost 260 000 formally described species. These sequences are obtained primarily through submissions from individual laboratories and batch submissions from large-scale sequencing projects, including whole-genome shotgun (WGS) and environmental sampling projects. Most submissions are made using the web-based BankIt or standalone Sequin programs, and GenBank staff assigns accession numbers upon data receipt. Daily data exchange with the European Nucleotide Archive (ENA) and the DNA Data Bank of Japan (DDBJ) ensures worldwide coverage. GenBank is accessible through the NCBI Entrez retrieval system, which integrates data from the major DNA and protein sequence databases along with taxonomy, genome, mapping, protein structure and domain information, and the biomedical journal literature via PubMed. BLAST provides sequence similarity searches of GenBank and other sequence databases. Complete bimonthly releases and daily updates of the GenBank database are available by FTP. To access GenBank and its related retrieval and analysis services, begin at the NCBI home page: www.ncbi.nlm.nih.gov.},
  file = {/home/santi/Zotero/storage/EC8LD5KR/Benson et al. - 2013 - GenBank.pdf}
}

@article{boyle2009,
  title = {Adaptable Data Management for Systems Biology Investigations},
  author = {Boyle, John and Rovira, Hector and Cavnor, Chris and Burdick, David and Killcoyne, Sarah and Shmulevich, Ilya},
  year = {2009},
  month = mar,
  journal = {BMC Bioinformatics},
  volume = {10},
  number = {1},
  pages = {79},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-10-79},
  urldate = {2025-10-11},
  abstract = {Within research each experiment is different, the focus changes and the data is generated from a continually evolving barrage of technologies. There is a continual introduction of new techniques whose usage ranges from in-house protocols through to high-throughput instrumentation. To support these requirements data management systems are needed that can be rapidly built and readily adapted for new usage.},
  keywords = {Aggregation System,Content Management System,Data Management System,Rich Functionality,XPATH Query},
  file = {/home/santi/Zotero/storage/JASNGZ87/Boyle et al. - 2009 - Adaptable data management for systems biology investigations.pdf;/home/santi/Zotero/storage/UUKWJ4P9/1471-2105-10-79.html}
}

@article{brusic2006,
  title = {The Growth of Bioinformatics},
  author = {Brusic, V.},
  year = {2006},
  month = dec,
  journal = {Briefings in Bioinformatics},
  volume = {8},
  number = {2},
  pages = {69--70},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbm008},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/BMKBRTFT/Brusic - 2006 - The growth of bioinformatics.pdf}
}

@article{chiu2019,
  title = {Clinical Metagenomics},
  author = {Chiu, Charles Y. and Miller, Steven A.},
  year = {2019},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {20},
  number = {6},
  pages = {341--355},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/s41576-019-0113-7},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/KV6GWUAD/Chiu and Miller - 2019 - Clinical metagenomics.pdf}
}

@article{daly2008,
  title = {Physiographically Sensitive Mapping of Climatological Temperature and Precipitation across the Conterminous {{United States}}},
  author = {Daly, Christopher and Halbleib, Michael and Smith, Joseph I. and Gibson, Wayne P. and Doggett, Matthew K. and Taylor, George H. and Curtis, Jan and Pasteris, Phillip P.},
  year = {2008},
  month = dec,
  journal = {International Journal of Climatology},
  volume = {28},
  number = {15},
  pages = {2031--2064},
  issn = {0899-8418, 1097-0088},
  doi = {10.1002/joc.1688},
  urldate = {2025-09-30},
  abstract = {Abstract             Spatial climate data sets of 1971--2000 mean monthly precipitation and minimum and maximum temperature were developed for the conterminous United States. These 30-arcsec ({$\sim$}800-m) grids are the official spatial climate data sets of the U.S. Department of Agriculture. The PRISM (Parameter-elevation Relationships on Independent Slopes Model) interpolation method was used to develop data sets that reflected, as closely as possible, the current state of knowledge of spatial climate patterns in the United States. PRISM calculates a climate--elevation regression for each digital elevation model (DEM) grid cell, and stations entering the regression are assigned weights based primarily on the physiographic similarity of the station to the grid cell. Factors considered are location, elevation, coastal proximity, topographic facet orientation, vertical atmospheric layer, topographic position, and orographic effectiveness of the terrain. Surface stations used in the analysis numbered nearly 13 000 for precipitation and 10 000 for temperature. Station data were spatially quality controlled, and short-period-of-record averages adjusted to better reflect the 1971--2000 period.             PRISM interpolation uncertainties were estimated with cross-validation (C-V) mean absolute error (MAE) and the 70\% prediction interval of the climate--elevation regression function. The two measures were not well correlated at the point level, but were similar when averaged over large regions. The PRISM data set was compared with the WorldClim and Daymet spatial climate data sets. The comparison demonstrated that using a relatively dense station data set and the physiographically sensitive PRISM interpolation process resulted in substantially improved climate grids over those of WorldClim and Daymet. The improvement varied, however, depending on the complexity of the region. Mountainous and coastal areas of the western United States, characterized by sparse data coverage, large elevation gradients, rain shadows, inversions, cold air drainage, and coastal effects, showed the greatest improvement. The PRISM data set benefited from a peer review procedure that incorporated local knowledge and data into the development process. Copyright {\copyright} 2008 Royal Meteorological Society},
  langid = {english},
  file = {/home/santi/Zotero/storage/SA4R6Y5K/Daly et al. - 2008 - Physiographically sensitive mapping of climatological temperature and precipitation across the conte.pdf}
}

@article{daswito2023,
  title = {Analysis {{Using R Software}}: {{A Big Opportunity}} for {{Epidemiology}} and {{Public Health Data Analysis}}},
  shorttitle = {Analysis {{Using R Software}}},
  author = {Daswito, Rinaldi and Besral, Besral and Ilmaskal, Radian},
  year = {2023},
  month = jul,
  journal = {Journal of Health Sciences and Epidemiology},
  volume = {1},
  number = {1},
  pages = {1--5},
  issn = {2988-7283},
  doi = {10.62404/jhse.v1i1.9},
  urldate = {2025-10-10},
  abstract = {R is a programming language, open-source, developed by various of the world's most active statisticians with powerful function and visualization for data analysis from simple to complex data such as machine learning and artificial intelligence. Data visualization technologies have the ability to assist public health professionals with decision-making. Visualization appears to help decision making by increasing the quantity of information communicated and reducing the cognitive and intellectual strain of processing information. There are numerous commercially available statistical software packages that are widely utilized by epidemiologists worldwide. For industrialized nations, the price of software is not a significant issue. However, for underdeveloped nations, the true expenses are frequently excessive. Some academics in developing nations rely on software that has been illegally copied a copy of the software program. There are several benefits to using R, including the possibility of using software packages for free (open source) and the volume and availability of software packages. It is simple to retain and repeat commands on the same data analysis with multiple data frames, facilitating the work of health monitoring officers who frequently analyze data with similar variables but at different times.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  file = {/home/santi/Zotero/storage/B35F2QJZ/Daswito et al. - 2023 - Analysis Using R Software A Big Opportunity for Epidemiology and Public Health Data Analysis.pdf}
}

@article{dhungel2021,
  title = {{{MegaR}}: An Interactive {{R}} Package for Rapid Sample Classification and Phenotype Prediction Using Metagenome Profiles and Machine Learning},
  shorttitle = {{{MegaR}}},
  author = {Dhungel, Eliza and Mreyoud, Yassin and Gwak, Ho-Jin and Rajeh, Ahmad and Rho, Mina and Ahn, Tae-Hyuk},
  year = {2021},
  month = jan,
  journal = {BMC Bioinformatics},
  volume = {22},
  number = {1},
  pages = {25},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03933-4},
  urldate = {2025-10-16},
  abstract = {Diverse microbiome communities drive biogeochemical processes and evolution of animals in their ecosystems. Many microbiome projects have demonstrated the power of using metagenomics to understand the structures and factors influencing the function of the microbiomes in their environments. In order to characterize the effects from microbiome composition for human health, diseases, and even ecosystems, one must first understand the relationship of microbes and their environment in different samples. Running machine learning model with metagenomic sequencing data is encouraged for this purpose, but it is not an easy task to make an appropriate machine learning model for all diverse metagenomic datasets.},
  keywords = {Machine learning,Metagenomics,Phenotype prediction,R-package,Sample classification},
  file = {/home/santi/Zotero/storage/MTPT44QL/Dhungel et al. - 2021 - MegaR an interactive R package for rapid sample classification and phenotype prediction using metag.pdf;/home/santi/Zotero/storage/ABGYAGLX/s12859-020-03933-4.html}
}

@book{dolstra2006,
  title = {The Purely Functional Software Deployment Model},
  author = {Dolstra, Eelco},
  year = {2006},
  publisher = {s.n.},
  address = {S.l.},
  isbn = {978-90-393-4130-8},
  langid = {english},
  annotation = {OCLC: 71702886},
  file = {/home/santi/Zotero/storage/CJNYAKZ5/Dolstra - 2006 - The purely functional software deployment model.pdf}
}

@article{drost2017,
  title = {Biomartr: Genomic Data Retrieval with {{R}}},
  shorttitle = {Biomartr},
  author = {Drost, Hajk-Georg and Paszkowski, Jerzy},
  year = {2017},
  month = apr,
  journal = {Bioinformatics},
  volume = {33},
  number = {8},
  pages = {1216--1217},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btw821},
  urldate = {2025-10-10},
  abstract = {Motivation Retrieval and reproducible functional annotation of genomic data are crucial in biology. However, the current poor usability and transparency of retrieval methods hinders reproducibility. Here we present an open source R package, biomartr, which provides a comprehensive easy-to-use framework for automating data retrieval and functional annotation for meta-genomic approaches. The functions of biomartr achieve a high degree of clarity, transparency and reproducibility of analyses. Results The biomartr package implements straightforward functions for bulk retrieval of all genomic data or data for selected genomes, proteomes, coding sequences and annotation files present in databases hosted by the National Center for Biotechnology Information (NCBI) and European Bioinformatics Institute (EMBL-EBI). In addition, biomartr communicates with the BioMart database for functional annotation of retrieved sequences. Comprehensive documentation of biomartr functions and five tutorial vignettes provide step-by-step instructions on how to use the package in a reproducible manner. Availability and Implementation The open source biomartr package is available at https://github.com/HajkD/biomartr and https://cran.r-project.org/web/packages/biomartr/index.html. Supplementary information  are available at Bioinformatics online.},
  pmcid = {PMC5408848},
  pmid = {28110292},
  file = {/home/santi/Zotero/storage/7HQI4RMX/Drost and Paszkowski - 2017 - Biomartr genomic data retrieval with R.pdf}
}

@article{drost2018,
  title = {{{myTAI}}: Evolutionary Transcriptomics with {{R}}},
  shorttitle = {{{myTAI}}},
  author = {Drost, Hajk-Georg and Gabel, Alexander and Liu, Jialin and Quint, Marcel and Grosse, Ivo},
  year = {2018},
  month = may,
  journal = {Bioinformatics},
  volume = {34},
  number = {9},
  pages = {1589--1590},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btx835},
  urldate = {2025-09-30},
  abstract = {Next Generation Sequencing (NGS) technologies generate a large amount of high quality transcriptome datasets enabling the investigation of molecular processes on a genomic and metagenomic scale. These transcriptomics studies aim to quantify and compare the molecular phenotypes of the biological processes at hand. Despite the vast increase of available transcriptome datasets, little is known about the evolutionary conservation of those characterized transcriptomes.The myTAI package implements exploratory analysis functions to infer transcriptome conservation patterns in any transcriptome dataset. Comprehensive documentation of myTAI functions and tutorial vignettes provide step-by-step instructions on how to use the package in an exploratory and computationally reproducible manner.The open source myTAI package is available at https://github.com/HajkD/myTAI and https://cran.r-project.org/web/packages/myTAI/index.html.Supplementary data are available at Bioinformatics online.},
  file = {/home/santi/Zotero/storage/K2S6ZL3Z/Drost et al. - 2018 - myTAI evolutionary transcriptomics with R.pdf;/home/santi/Zotero/storage/QJXLLA2X/btx835.html}
}

@article{erickson2024,
  title = {A {{Reproducible Manuscript Workflow With}} a {{Quarto Template}}},
  author = {Erickson, Richard A. and Archer, Althea A. and Fienen, Michael N.},
  year = {2024},
  month = jun,
  journal = {Journal of Fish and Wildlife Management},
  volume = {15},
  number = {1},
  pages = {251--258},
  issn = {1944-687X},
  doi = {10.3996/JFWM-24-003},
  urldate = {2025-10-10},
  abstract = {Abstract             Scientists and resource managers increasingly use Markdown-based tools to create reproducible reports and manuscripts. These workflows allow people to use standardized methods that are more reproducible, efficient, and transparent than other standard office tools. We present a Quarto template and demonstrate how this template may be used for the Journal of Fish and Wildlife Management. This template may also be readily adapted to other journals that use Microsoft Word-based workflows and for other product types such as annual reports. We also provide a high-level overview of Quarto and other Markdown-based workflows. Last, we provide examples of some features of the Quarto publishing system that may be helpful for authors when customizing Quarto templates for specific journal formatting requirements and other product types.},
  langid = {english},
  file = {/home/santi/Zotero/storage/YFQHWJJQ/Erickson et al. - 2024 - A Reproducible Manuscript Workflow With a Quarto Template.pdf}
}

@inproceedings{fang2025,
  title = {Enhancing {{Computational Notebooks}} with {{Code}}+{{Data Space Versioning}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Fang, Hanxi and Chockchowwat, Supawit and Sundaram, Hari and Park, Yongjoo},
  year = {2025},
  month = apr,
  pages = {1--17},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714141},
  urldate = {2025-10-10},
  isbn = {979-8-4007-1394-1},
  langid = {english},
  file = {/home/santi/Zotero/storage/ZZXU6WEB/Fang et al. - 2025 - Enhancing Computational Notebooks with Code+Data Space Versioning.pdf}
}

@misc{finak2018,
  title = {{{DataPackageR}}: {{Reproducible}} Data Preprocessing, Standardization and Sharing Using {{R}}/{{Bioconductor}} for Collaborative Data Analysis: {{A Practical Approach}} to {{Wrangling Multi-Assay Data Sets}} for {{Standardized}} and {{Reproducible Data Analysis}}},
  shorttitle = {{{DataPackageR}}},
  author = {Finak, Greg and Mayer, Bryan T. and Fulp, William and Obrecht, Paul and Sato, Alicia and Chung, Eva and Holman, Drienna and Gottardo, Raphael},
  year = {2018},
  month = jun,
  publisher = {Bioinformatics},
  doi = {10.1101/342907},
  urldate = {2025-10-10},
  abstract = {Abstract                        A central tenet of reproducible research is that scientific results are published along with the underlying data and software code necessary to reproduce and verify the findings. A host of tools and software have been released that facilitate such work-flows and scientific journals have increasingly demanded that code and primary data be made available with publications. There has been little practical advice on implementing reproducible research work-flows for large `omics' or systems biology data sets used by teams of analysts working in collaboration. In such instances it is important to ensure all analysts use the same version of a data set for their analyses. Yet, instantiating relational databases and standard operating procedures can be unwieldy, with high ``startup'' costs and poor adherence to procedures when they deviate substantially from an analyst's usual work-flow. Ideally a reproducible research work-flow should fit naturally into an individual's existing work-flow, with minimal disruption. Here, we provide an overview of how we have leveraged popular open source tools, including Bioconductor, Rmarkdown, git version control, R, and specifically R's package system combined with a new tool             DataPackageR,             to implement a lightweight reproducible research work-flow for preprocessing large data sets, suitable for sharing among small-to-medium sized teams of computational scientists. Our primary contribution is the             DataPackageR             tool, which decouples time-consuming data processing from data analysis while leaving a traceable record of how raw data is processed into analysis-ready data sets. The software ensures packaged data objects are properly documented and performs checksum verification of these along with basic package version management, and importantly, leaves a record of data processing code in the form of package vignettes. Our group has implemented this work-flow to manage, analyze and report on pre-clinical immunological trial data from multi-center, multi-assay studies for the past three years.},
  archiveprefix = {Bioinformatics},
  langid = {english},
  file = {/home/santi/Zotero/storage/4XEYAFDD/Finak et al. - 2018 - DataPackageR Reproducible data preprocessing, standardization and sharing using RBioconductor for.pdf}
}

@misc{foster2018,
  title = {Taxa: {{An R}} Package Implementing Data Standards and Methods for Taxonomic Data},
  shorttitle = {Taxa},
  author = {Foster, Zachary S. L. and Chamberlain, Scott and Gr{\"u}nwald, Niklaus J.},
  year = {2018},
  month = mar,
  publisher = {F1000Research},
  doi = {10.12688/f1000research.14013.1},
  urldate = {2025-09-30},
  abstract = {The\&nbsp;taxa R package provides a set of tools for defining and manipulating taxonomic data. The recent and widespread application of DNA sequencing to community composition studies is making large data sets with taxonomic information commonplace. However, compared to typical tabular data, this information is encoded in many different ways and the hierarchical nature of taxonomic classifications makes it difficult to work with. There are many R packages that use taxonomic data to varying degrees but there is currently no cross-package standard for how this information is encoded and manipulated. We developed the R package\&nbsp;taxa to provide a robust and flexible solution to storing and manipulating taxonomic data in R and any application-specific information associated with it.\&nbsp;Taxa provides parsers that can read common sources of taxonomic information (taxon IDs, sequence IDs, taxon names, and classifications) from nearly any format while preserving associated data. Once parsed, the taxonomic data and any associated data can be manipulated using a cohesive set of functions modeled after the popular R package\&nbsp;dplyr. These functions take into account the hierarchical nature of taxa and can modify the taxonomy or associated data in such a way that both are kept in sync.\&nbsp;Taxa is currently being used by the\&nbsp;metacoder and\&nbsp;taxize packages, which provide broadly useful functionality that we hope will speed adoption by users and developers.},
  archiveprefix = {F1000Research},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  keywords = {metacoder,R language,R package,rOpenSci,taxa,taxize,taxonomy},
  file = {/home/santi/Zotero/storage/P4CTUFU9/Foster et al. - 2018 - Taxa An R package implementing data standards and methods for taxonomic data.pdf}
}

@article{giorgi2022,
  title = {The {{R Language}}: {{An Engine}} for {{Bioinformatics}} and {{Data Science}}},
  shorttitle = {The {{R Language}}},
  author = {Giorgi, Federico M. and Ceraolo, Carmine and Mercatelli, Daniele},
  year = {2022},
  month = apr,
  journal = {Life},
  volume = {12},
  number = {5},
  pages = {648},
  issn = {2075-1729},
  doi = {10.3390/life12050648},
  urldate = {2025-10-10},
  abstract = {The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/santi/Zotero/storage/AY9PYQRY/Giorgi et al. - 2022 - The R Language An Engine for Bioinformatics and Data Science.pdf}
}

@article{goodwin2016,
  title = {Coming of Age: Ten Years of next-Generation Sequencing Technologies},
  shorttitle = {Coming of Age},
  author = {Goodwin, Sara and McPherson, John D. and McCombie, W. Richard},
  year = {2016},
  month = jun,
  journal = {Nature Reviews Genetics},
  volume = {17},
  number = {6},
  pages = {333--351},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg.2016.49},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/WS3TITGS/Goodwin et al. - 2016 - Coming of age ten years of next-generation sequencing technologies.pdf}
}

@article{greene2014,
  title = {Big {{Data Bioinformatics}}},
  author = {Greene, Casey S. and Tan, Jie and Ung, Matthew and Moore, Jason H. and Cheng, Chao},
  year = {2014},
  month = dec,
  journal = {Journal of Cellular Physiology},
  volume = {229},
  number = {12},
  pages = {1896--1900},
  issn = {0021-9541, 1097-4652},
  doi = {10.1002/jcp.24662},
  urldate = {2025-10-10},
  abstract = {Recent technological advances allow for high throughput profiling of biological systems in a cost-efficient manner. The low cost of data generation is leading us to the ``big data'' era. The availability of big data provides unprecedented opportunities but also raises new challenges for data mining and analysis. In this review, we introduce key concepts in the analysis of big data, including both ``machine learning'' algorithms as well as ``unsupervised'' and ``supervised'' examples of each. We note packages for the R programming language that are available to perform machine learning analyses. In addition to programming based solutions, we review webservers that allow users with limited or no programming background to perform these analyses on large data compendia. J. Cell. Physiol. 229: 1896--1900, 2014. {\copyright} 2014 Wiley Periodicals, Inc.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  langid = {english},
  file = {/home/santi/Zotero/storage/BACUS3MN/Greene et al. - 2014 - Big Data Bioinformatics.pdf}
}

@inproceedings{huang2018,
  title = {A {{Case Study}} of {{R Performance Analysis}} and {{Optimization}}},
  booktitle = {Proceedings of the {{Practice}} and {{Experience}} on {{Advanced Research Computing}}},
  author = {Huang, Ruizhu and Xu, Weijia and Liverani, Silvia and Hiltbrand, Dave and Stapleton, Ann E.},
  year = {2018},
  month = jul,
  pages = {1--6},
  publisher = {ACM},
  address = {Pittsburgh PA USA},
  doi = {10.1145/3219104.3219156},
  urldate = {2025-10-10},
  isbn = {978-1-4503-6446-1},
  langid = {english},
  file = {/home/santi/Zotero/storage/QNAGS6NV/Huang et al. - 2018 - A Case Study of R Performance Analysis and Optimization.pdf}
}

@article{jagadesan2025,
  title = {{{MetaDAVis}}: {{An R}} Shiny Application for Metagenomic Data Analysis and Visualization},
  shorttitle = {{{MetaDAVis}}},
  author = {Jagadesan, Sankarasubramanian and Guda, Chittibabu},
  year = {2025},
  month = apr,
  journal = {PLOS ONE},
  volume = {20},
  number = {4},
  pages = {e0319949},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0319949},
  urldate = {2025-10-14},
  abstract = {The human microbiome exerts tremendous influence on maintaining a balance between human health and disease. High-throughput sequencing has enabled the study of microbial communities at an unprecedented resolution. Generation of massive amounts of sequencing data has also presented novel challenges to analyzing and visualizing data to make biologically relevant interpretations. We have developed an interactive Metagenome Data Analysis and Visualization (MetaDAVis) tool for 16S rRNA as well as the whole genome sequencing data analysis and visualization to address these challenges using an R Shiny application. MetaDAVis can perform six different types of analyses that include: i) Taxonomic abundance distribution; ii) Alpha and beta diversity analyses; iii) Dimension reduction tasks using PCA, t-SNE, and UMAP; iv) Correlation analysis using taxa- or sample-based data; v) Heatmap generation; and vi) Differential abundance analysis. MetaDAVis creates interactive and dynamic figures and tables from multiple methods enabling users to easily understand their data using different variables. Our program is user-friendly and easily customizable allowing those without any programming background to perform comprehensive data analyses using a standalone or web-based interface.},
  langid = {english},
  keywords = {Data visualization,Graphical user interfaces,Metagenomics,Microbiome,Principal component analysis,Ribosomal RNA,Taxonomy,Web-based applications},
  file = {/home/santi/Zotero/storage/QA36ZLTB/Jagadesan and Guda - 2025 - MetaDAVis An R shiny application for metagenomic data analysis and visualization.pdf}
}

@article{jones2006,
  title = {The {{New Bioinformatics}}: {{Integrating Ecological Data}} from the {{Gene}} to the {{Biosphere}}},
  shorttitle = {The {{New Bioinformatics}}},
  author = {Jones, Matthew B. and Schildhauer, Mark P. and Reichman, O. J. and Bowers, Shawn},
  year = {2006},
  month = dec,
  journal = {Annual Review of Ecology, Evolution, and Systematics},
  volume = {37},
  number = {Volume 37, 2006},
  pages = {519--544},
  publisher = {Annual Reviews},
  issn = {1543-592X, 1545-2069},
  doi = {10.1146/annurev.ecolsys.37.091305.110031},
  urldate = {2025-09-30},
  abstract = {Abstract Bioinformatics, the application of computational tools to the management and analysis of biological data, has stimulated rapid research advances in genomics through the development of data archives such as GenBank, and similar progress is just beginning within ecology. One reason for the belated adoption of informatics approaches in ecology is the breadth of ecologically pertinent data (from genes to the biosphere) and its highly heterogeneous nature. The variety of formats, logical structures, and sampling methods in ecology create significant challenges. Cultural barriers further impede progress, especially for the creation and adoption of data standards. Here we describe informatics frameworks for ecology, from subject-specific data warehouses, to generic data collections that use detailed metadata descriptions and formal ontologies to catalog and cross-reference information. Combining these approaches with automated data integration techniques and scientific workflow systems will maximize the value of data and open new frontiers for research in ecology.},
  langid = {english},
  file = {/home/santi/Zotero/storage/UNS7XW6P/Jones et al. - 2006 - The New Bioinformatics Integrating Ecological Data from the Gene to the Biosphere.pdf;/home/santi/Zotero/storage/JL7YM9D8/annurev.ecolsys.37.091305.html}
}

@article{landau2021,
  title = {The Targets {{R}} Package: A Dynamic {{Make-like}} Function-Oriented Pipeline Toolkit for Reproducibility and High-Performance Computing},
  shorttitle = {The Targets {{R}} Package},
  author = {Landau, William Michael},
  year = {2021},
  month = jan,
  journal = {Journal of Open Source Software},
  volume = {6},
  number = {57},
  pages = {2959},
  issn = {2475-9066},
  doi = {10.21105/joss.02959},
  urldate = {2025-10-11},
  abstract = {Landau, W. M., (2021). The targets R package: a dynamic Make-like function-oriented pipeline toolkit for reproducibility and high-performance computing. Journal of Open Source Software, 6(57), 2959, https://doi.org/10.21105/joss.02959},
  langid = {english},
  file = {/home/santi/Zotero/storage/547QVIE3/Landau - 2021 - The targets R package a dynamic Make-like function-oriented pipeline toolkit for reproducibility an.pdf}
}

@article{luo2024,
  title = {Multiomics {{Research}}: {{Principles}} and {{Challenges}} in {{Integrated Analysis}}},
  shorttitle = {Multiomics {{Research}}},
  author = {Luo, Yunqing and Zhao, Chengjun and Chen, Fei},
  year = {2024},
  journal = {BioDesign Research},
  volume = {6},
  pages = {0059},
  issn = {26931257},
  doi = {10.34133/bdr.0059},
  urldate = {2025-10-10},
  langid = {english}
}

@article{marwick2018,
  title = {Packaging {{Data Analytical Work Reproducibly Using R}} (and {{Friends}})},
  author = {Marwick, Ben and Boettiger, Carl and Mullen, Lincoln},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {80--88},
  publisher = {ASA Website},
  issn = {0003-1305},
  doi = {10.1080/00031305.2017.1375986},
  urldate = {2025-10-11},
  abstract = {Computers are a central tool in the research process, enabling complex and large-scale data analysis. As computer-based research has increased in complexity, so have the challenges of ensuring that this research is reproducible. To address this challenge, we review the concept of the research compendium as a solution for providing a standard and easily recognizable way for organizing the digital materials of a research project to enable other researchers to inspect, reproduce, and extend the research. We investigate how the structure and tooling of software packages of the R programming language are being used to produce research compendia in a variety of disciplines. We also describe how software engineering tools and services are being used by researchers to streamline working with research compendia. Using real-world examples, we show how researchers can improve the reproducibility of their work using research compendia based on R packages and related tools.},
  keywords = {Computational science,Data science,Open source software,Reproducible research},
  file = {/home/santi/Zotero/storage/EGNU5VVB/Marwick et al. - 2018 - Packaging Data Analytical Work Reproducibly Using R (and Friends).pdf}
}

@article{metzker2010,
  title = {Sequencing Technologies --- the next Generation},
  author = {Metzker, Michael L.},
  year = {2010},
  month = jan,
  journal = {Nature Reviews Genetics},
  volume = {11},
  number = {1},
  pages = {31--46},
  issn = {1471-0056, 1471-0064},
  doi = {10.1038/nrg2626},
  urldate = {2025-10-10},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/home/santi/Zotero/storage/YMGPXKKP/Metzker - 2010 - Sequencing technologies â€” the next generation.pdf}
}

@article{morais2022,
  title = {{{MEDUSA}}: {{A Pipeline}} for {{Sensitive Taxonomic Classification}} and {{Flexible Functional Annotation}} of {{Metagenomic Shotgun Sequences}}},
  shorttitle = {{{MEDUSA}}},
  author = {Morais, Diego A. A. and Cavalcante, Jo{\~a}o V. F. and Monteiro, Sh{\^e}nia S. and Pasquali, Matheus A. B. and Dalmolin, Rodrigo J. S.},
  year = {2022},
  month = mar,
  journal = {Frontiers in Genetics},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-8021},
  doi = {10.3389/fgene.2022.814437},
  urldate = {2025-10-15},
  abstract = {Metagenomic studies unravel details about the taxonomic composition and the functions performed bymicrobial communities. As a complete metagenomic analysis requires different tools for differentpurposes, the selection and setup of these tools remain challenging. Furthermore, the chosen toolsetwill affect the accuracy, the formatting, and the functional identifiers reported in the results, impactingthe results interpretation and the biological answer obtained. Thus, we surveyed state-of-the-art toolsavailable in the literature, created simulated datasets, and performed benchmarks to design a sensitiveand flexible metagenomic analysis pipeline. Here we present MEDUSA, an efficient pipeline toconduct comprehensive metagenomic analyses. It performs preprocessing, assembly, alignment,taxonomic classification, and functional annotation on shotgun data, supporting user-built dictionariesto transfer annotations to any functional identifier. MEDUSA includes several tools, as fastp, Bowtie2,DIAMOND, Kaiju, MEGAHIT, and a novel tool implemented in Python to transfer annotations toBLAST/DIAMOND alignment results. These tools are installed via Conda, and the workflow ismanaged by Snakemake, easing the setup and execution. Compared with MEGAN 6 CommunityEdition, MEDUSA correctly identifies more species, especially the less abundant, and is more suitedfor functional analysis using Gene Ontology identifiers.},
  langid = {english},
  keywords = {bioinformatics,functional annotation,Metagenomics,pipeline,Shotgun sequences,taxonomic classification},
  file = {/home/santi/Zotero/storage/3VCFVMD4/Morais et al. - 2022 - MEDUSA A Pipeline for Sensitive Taxonomic Classification and Flexible Functional Annotation of Meta.pdf}
}

@article{nam2023,
  title = {Metagenomics: {{An Effective Approach}} for {{Exploring Microbial Diversity}} and {{Functions}}},
  shorttitle = {Metagenomics},
  author = {Nam, Nguyen and Do, Hoang and Loan Trinh, Kieu and Lee, Nae},
  year = {2023},
  month = may,
  journal = {Foods},
  volume = {12},
  number = {11},
  pages = {2140},
  issn = {2304-8158},
  doi = {10.3390/foods12112140},
  urldate = {2025-10-10},
  abstract = {Various fields have been identified in the ``omics'' era, such as genomics, proteomics, transcriptomics, metabolomics, phenomics, and metagenomics. Among these, metagenomics has enabled a significant increase in discoveries related to the microbial world. Newly discovered microbiomes in different ecologies provide meaningful information on the diversity and functions of microorganisms on the Earth. Therefore, the results of metagenomic studies have enabled new microbe-based applications in human health, agriculture, and the food industry, among others. This review summarizes the fundamental procedures on recent advances in bioinformatic tools. It also explores up-to-date applications of metagenomics in human health, food study, plant research, environmental sciences, and other fields. Finally, metagenomics is a powerful tool for studying the microbial world, and it still has numerous applications that are currently hidden and awaiting discovery. Therefore, this review also discusses the future perspectives of metagenomics.},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/home/santi/Zotero/storage/8UW6AIGG/Nam et al. - 2023 - Metagenomics An Effective Approach for Exploring Microbial Diversity and Functions.pdf}
}

@article{navgire2022,
  title = {Analysis and {{Interpretation}} of Metagenomics Data: An Approach},
  shorttitle = {Analysis and {{Interpretation}} of Metagenomics Data},
  author = {Navgire, Gauri S. and Goel, Neha and Sawhney, Gifty and Sharma, Mohit and Kaushik, Prashant and Mohanta, Yugal Kishore and Mohanta, Tapan Kumar and {Al-Harrasi}, Ahmed},
  year = {2022},
  month = nov,
  journal = {Biological Procedures Online},
  volume = {24},
  pages = {18},
  issn = {1480-9222},
  doi = {10.1186/s12575-022-00179-7},
  urldate = {2025-10-16},
  abstract = {Advances in next-generation sequencing technologies have accelerated the momentum of metagenomic studies, which is increasing yearly. The metagenomics field is one of the versatile applications in microbiology, where any interaction in the environment involving microorganisms can be the topic of study. Due to this versatility, the number of applications of this omics technology reached its horizons. Agriculture is a crucial sector involving crop plants and microorganisms interacting together. Hence, studying these interactions through the lenses of metagenomics would completely disclose a new meaning to crop health and development. The rhizosphere is an essential reservoir of the microbial community for agricultural soil. Hence, we focus on the R\&D of metagenomic studies on the rhizosphere of crops such as rice, wheat, legumes, chickpea, and sorghum. These recent developments are impossible without the continuous advancement seen in the next-generation sequencing platforms; thus, a brief introduction and analysis of the available sequencing platforms are presented here to have a clear picture of the workflow. Concluding the topic is the discussion about different pipelines applied to analyze data produced by sequencing techniques and have a significant role in interpreting the outcome of a particular experiment. A plethora of different software and tools are incorporated in the automated pipelines or individually available to perform manual metagenomic analysis. Here we describe 8--10 advanced, efficient pipelines used for analysis that explain their respective workflows to simplify the whole analysis process.},
  pmcid = {PMC9675974},
  pmid = {36402995},
  file = {/home/santi/Zotero/storage/NFQBPCHZ/Navgire et al. - 2022 - Analysis and Interpretation of metagenomics data an approach.pdf}
}

@article{noble2009,
  title = {A {{Quick Guide}} to {{Organizing Computational Biology Projects}}},
  author = {Noble, William Stafford},
  year = {2009},
  month = jul,
  journal = {PLOS Computational Biology},
  volume = {5},
  number = {7},
  pages = {e1000424},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000424},
  urldate = {2025-10-11},
  langid = {english},
  keywords = {Bioinformatics,Biologists,Careers in research,Computational biology,Computer software,Human learning,Software engineering,Source code},
  file = {/home/santi/Zotero/storage/CGAP7E62/Noble - 2009 - A Quick Guide to Organizing Computational Biology Projects.pdf}
}

@article{okie2020,
  title = {Genomic Adaptations in Information Processing Underpin Trophic Strategy in a Whole-Ecosystem Nutrient Enrichment Experiment},
  author = {Okie, Jordan G and {Poret-Peterson}, Amisha T and Lee, Zarraz MP and Richter, Alexander and Alcaraz, Luis D and Eguiarte, Luis E and Siefert, Janet L and Souza, Valeria and Dupont, Chris L and Elser, James J},
  editor = {Donoso, David and Weigel, Detlef},
  year = {2020},
  month = jan,
  journal = {eLife},
  volume = {9},
  pages = {e49816},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.49816},
  urldate = {2025-10-14},
  abstract = {Several universal genomic traits affect trade-offs in the capacity, cost, and efficiency of the biochemical information processing that underpins metabolism and reproduction. We analyzed the role of these traits in mediating the responses of a planktonic microbial community to nutrient enrichment in an oligotrophic, phosphorus-deficient pond in Cuatro Ci{\'e}negas, Mexico. This is one of the first whole-ecosystem experiments to involve replicated metagenomic assessment. Mean bacterial genome size, GC content, total number of tRNA genes, total number of rRNA genes, and codon usage bias in ribosomal protein sequences were all higher in the fertilized treatment, as predicted on the basis of the assumption that oligotrophy favors lower information-processing costs whereas copiotrophy favors higher processing rates. Contrasting changes in trait variances also suggested differences between traits in mediating assembly under copiotrophic versus oligotrophic conditions. Trade-offs in information-processing traits are apparently sufficiently pronounced to play a role in community assembly because the major components of metabolism---information, energy, and nutrient requirements---are fine-tuned to an organism's growth and trophic strategy.},
  keywords = {codon usage bias,GC content,genome size,metagenomics,phosphorous fertilization,rRNA operon copy number},
  file = {/home/santi/Zotero/storage/R4ZHVN2U/Okie et al. - 2020 - Genomic adaptations in information processing underpin trophic strategy in a whole-ecosystem nutrien.pdf}
}

@article{pal2020,
  title = {Big Data in Biology: {{The}} Hope and Present-Day Challenges in It},
  shorttitle = {Big Data in Biology},
  author = {Pal, Subhajit and Mondal, Sudip and Das, Gourab and Khatua, Sunirmal and Ghosh, Zhumur},
  year = {2020},
  month = dec,
  journal = {Gene Reports},
  volume = {21},
  pages = {100869},
  issn = {24520144},
  doi = {10.1016/j.genrep.2020.100869},
  urldate = {2025-10-10},
  langid = {english},
  file = {/home/santi/Zotero/storage/KEH3HJAX/Pal et al. - 2020 - Big data in biology The hope and present-day challenges in it.pdf}
}

@article{pruitt2007,
  title = {{{NCBI}} Reference Sequences ({{RefSeq}}): A Curated Non-Redundant Sequence Database of Genomes, Transcripts and Proteins},
  shorttitle = {{{NCBI}} Reference Sequences ({{RefSeq}})},
  author = {Pruitt, Kim D. and Tatusova, Tatiana and Maglott, Donna R.},
  year = {2007},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {35},
  number = {suppl\_1},
  pages = {D61-D65},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl842},
  urldate = {2025-09-30},
  abstract = {NCBI's reference sequence (RefSeq) database () is a curated non-redundant collection of sequences representing genomes, transcripts and proteins. The database includes 3774 organisms spanning prokaryotes, eukaryotes and viruses, and has records for 2 879 860 proteins (RefSeq release 19). RefSeq records integrate information from multiple sources, when additional data are available from those sources and therefore represent a current description of the sequence and its features. Annotations include coding regions, conserved domains, tRNAs, sequence tagged sites (STS), variation, references, gene and protein product names, and database cross-references. Sequence is reviewed and features are added using a combined approach of collaboration and other input from the scientific community, prediction, propagation from GenBank and curation by NCBI staff. The format of all RefSeq records is validated, and an increasing number of tests are being applied to evaluate the quality of sequence and annotation, especially in the context of complete genomic sequence.},
  file = {/home/santi/Zotero/storage/VYCT7U3T/Pruitt et al. - 2007 - NCBI reference sequences (RefSeq) a curated non-redundant sequence database of genomes, transcripts.pdf;/home/santi/Zotero/storage/KYDHF3Q8/gkl842.html}
}

@article{pruitt2007a,
  title = {{{NCBI}} Reference Sequences ({{RefSeq}}): A Curated Non-Redundant Sequence Database of Genomes, Transcripts and Proteins},
  shorttitle = {{{NCBI}} Reference Sequences ({{RefSeq}})},
  author = {Pruitt, Kim D. and Tatusova, Tatiana and Maglott, Donna R.},
  year = {2007},
  month = jan,
  journal = {Nucleic Acids Research},
  volume = {35},
  number = {suppl\_1},
  pages = {D61-D65},
  issn = {0305-1048},
  doi = {10.1093/nar/gkl842},
  urldate = {2025-10-10},
  abstract = {NCBI's reference sequence (RefSeq) database () is a curated non-redundant collection of sequences representing genomes, transcripts and proteins. The database includes 3774 organisms spanning prokaryotes, eukaryotes and viruses, and has records for 2 879 860 proteins (RefSeq release 19). RefSeq records integrate information from multiple sources, when additional data are available from those sources and therefore represent a current description of the sequence and its features. Annotations include coding regions, conserved domains, tRNAs, sequence tagged sites (STS), variation, references, gene and protein product names, and database cross-references. Sequence is reviewed and features are added using a combined approach of collaboration and other input from the scientific community, prediction, propagation from GenBank and curation by NCBI staff. The format of all RefSeq records is validated, and an increasing number of tests are being applied to evaluate the quality of sequence and annotation, especially in the context of complete genomic sequence.},
  file = {/home/santi/Zotero/storage/UI5N9VXH/Pruitt et al. - 2007 - NCBI reference sequences (RefSeq) a curated non-redundant sequence database of genomes, transcripts.pdf}
}

@article{satam2023,
  title = {Next-{{Generation Sequencing Technology}}: {{Current Trends}} and {{Advancements}}},
  shorttitle = {Next-{{Generation Sequencing Technology}}},
  author = {Satam, Heena and Joshi, Kandarp and Mangrolia, Upasana and Waghoo, Sanober and Zaidi, Gulnaz and Rawool, Shravani and Thakare, Ritesh P. and Banday, Shahid and Mishra, Alok K. and Das, Gautam and Malonia, Sunil K.},
  year = {2023},
  month = jul,
  journal = {Biology},
  volume = {12},
  number = {7},
  pages = {997},
  issn = {2079-7737},
  doi = {10.3390/biology12070997},
  urldate = {2025-10-10},
  abstract = {The advent of next-generation sequencing (NGS) has brought about a paradigm shift in genomics research, offering unparalleled capabilities for analyzing DNA and RNA molecules in a high-throughput and cost-effective manner. This transformative technology has swiftly propelled genomics advancements across diverse domains. NGS allows for the rapid sequencing of millions of DNA fragments simultaneously, providing comprehensive insights into genome structure, genetic variations, gene expression profiles, and epigenetic modifications. The versatility of NGS platforms has expanded the scope of genomics research, facilitating studies on rare genetic diseases, cancer genomics, microbiome analysis, infectious diseases, and population genetics. Moreover, NGS has enabled the development of targeted therapies, precision medicine approaches, and improved diagnostic methods. This review provides an insightful overview of the current trends and recent advancements in NGS technology, highlighting its potential impact on diverse areas of genomic research. Moreover, the review delves into the challenges encountered and future directions of NGS technology, including endeavors to enhance the accuracy and sensitivity of sequencing data, the development of novel algorithms for data analysis, and the pursuit of more efficient, scalable, and cost-effective solutions that lie ahead.},
  langid = {english},
  file = {/home/santi/Zotero/storage/GHD7TF8Z/Satam et al. - 2023 - Next-Generation Sequencing Technology Current Trends and Advancements.pdf}
}

@article{smedley2009,
  title = {{{BioMart}} -- Biological Queries Made Easy},
  author = {Smedley, Damian and Haider, Syed and Ballester, Benoit and Holland, Richard and London, Darin and Thorisson, Gudmundur and Kasprzyk, Arek},
  year = {2009},
  month = jan,
  journal = {BMC Genomics},
  volume = {10},
  number = {1},
  pages = {22},
  issn = {1471-2164},
  doi = {10.1186/1471-2164-10-22},
  urldate = {2025-10-10},
  abstract = {Biologists need to perform complex queries, often across a variety of databases. Typically, each data resource provides an advanced query interface, each of which must be learnt by the biologist before they can begin to query them. Frequently, more than one data source is required and for high-throughput analysis, cutting and pasting results between websites is certainly very time consuming. Therefore, many groups rely on local bioinformatics support to process queries by accessing the resource's programmatic interfaces if they exist. This is not an efficient solution in terms of cost and time. Instead, it would be better if the biologist only had to learn one generic interface. BioMart provides such a solution.},
  keywords = {Arrhythmogenic Right Ventricular Dysplasia,Central Portal,Complex Query,Distribute Annotation System,Ensembl Genome Browser},
  file = {/home/santi/Zotero/storage/SVL6KZSJ/Smedley et al. - 2009 - BioMart â€“ biological queries made easy.pdf;/home/santi/Zotero/storage/68T3P8RX/1471-2164-10-22.html}
}

@article{tamames2019,
  title = {{{SqueezeMeta}}, {{A Highly Portable}}, {{Fully Automatic Metagenomic Analysis Pipeline}}},
  author = {Tamames, Javier and {Puente-S{\'a}nchez}, Fernando},
  year = {2019},
  month = jan,
  journal = {Frontiers in Microbiology},
  volume = {9},
  publisher = {Frontiers},
  issn = {1664-302X},
  doi = {10.3389/fmicb.2018.03349},
  urldate = {2025-10-14},
  abstract = {The improvement of sequencing technologies has facilitated generalization of metagenomic sequencing, which has become a standard procedure for analyzing the structure and functionality of microbiomes. Bioinformatic analysis of sequencing results poses a challenge because this involves many different complex steps.SqueezeMeta is a fully automatic pipeline for metagenomics/metatranscriptomics, covering all steps of the analysis. SqueezeMeta includes multi-metagenome support that enables co-assembly of related metagenomes and retrieval of individual genomes via binning procedures. SqueezeMeta features several unique characteristics: co-assembly procedure or co-assembly of unlimited number of metagenomes via merging of individual assembled metagenomes, both with read mapping for estimation of the abundances of genes in each metagenome. It also includes binning and bin checking for retrieving individual genomes. Internal checks for the assembly and binning steps provide information about the consistency of contigs and bins. Moreover, results are stored in a mySQL database, where they can be easily exported and shared, and can be inspected anywhere using a flexible web interface that allows simple creation of complex queries.We illustrate the potential of SqueezeMeta by analyzing 32 gut metagenomes in a fully automatic way, enabling retrieval of several million genes and several hundreds of genomic bins.One of the motivations in the development of SqueezeMeta was producing a software capable of running in small desktop computers and thus amenable to all users and settings. We were also able to co-assemble two of these metagenomes and complete the full analysis in less than one day using a simple laptop computer. This reveals the capacity of SqueezeMeta to run without high-performance computing infrastructure.SqueezeMeta is a complete system covering all steps in the analysis of metagenomes and metatranscriptomes; it is capable of working even in scarce computational resources. It is therefore adequate for in-situ, real time analysis of metagenomes produced by nanopore sequencing. SqueezeMeta can be downloaded from https://github.com/jtamames/SqueezeMeta.},
  langid = {english},
  keywords = {Binning,Metagenomics,metatranscriptomics,MinION,RNAseq,Software},
  file = {/home/santi/Zotero/storage/3772Y58D/Tamames and Puente-SÃ¡nchez - 2019 - SqueezeMeta, A Highly Portable, Fully Automatic Metagenomic Analysis Pipeline.pdf}
}

@article{wen2023,
  title = {The Best Practice for Microbiome Analysis Using {{R}}},
  author = {Wen, Tao and Niu, Guoqing and Chen, Tong and Shen, Qirong and Yuan, Jun and Liu, Yong-Xin},
  year = {2023},
  month = may,
  journal = {Protein \& Cell},
  volume = {14},
  number = {10},
  pages = {713--725},
  issn = {1674-800X},
  doi = {10.1093/procel/pwad024},
  urldate = {2025-10-14},
  abstract = {With the gradual maturity of sequencing technology, many microbiome studies have published, driving the emergence and advance of related analysis tools. R language is the widely used platform for microbiome data analysis for powerful functions. However, tens of thousands of R packages and numerous similar analysis tools have brought major challenges for many researchers to explore microbiome data. How to choose suitable, efficient, convenient, and easy-to-learn tools from the numerous R packages has become a problem for many microbiome researchers. We have organized 324 common R packages for microbiome analysis and classified them according to application categories (diversity, difference, biomarker, correlation and network, functional prediction, and others), which could help researchers quickly find relevant R packages for microbiome analysis. Furthermore, we systematically sorted the integrated R packages (phyloseq, microbiome, MicrobiomeAnalystR, Animalcules, microeco, and amplicon) for microbiome analysis, and summarized the advantages and limitations, which will help researchers choose the appropriate tools. Finally, we thoroughly reviewed the R packages for microbiome analysis, summarized most of the common analysis content in the microbiome, and formed the most suitable pipeline for microbiome analysis. This paper is accompanied by hundreds of examples with 10,000 lines codes in GitHub, which can help beginners to learn, also help analysts compare and test different tools. This paper systematically sorts the application of R in microbiome, providing an important theoretical basis and practical reference for the development of better microbiome tools in the future. All the code is available at GitHub github.com/taowenmicro/EasyMicrobiomeR.,  Graphical Abstract},
  pmcid = {PMC10599642},
  pmid = {37128855}
}

@article{wen2023a,
  title = {The Best Practice for Microbiome Analysis Using {{R}}},
  author = {Wen, Tao and Niu, Guoqing and Chen, Tong and Shen, Qirong and Yuan, Jun and Liu, Yong-Xin},
  year = {2023},
  month = may,
  journal = {Protein \& Cell},
  volume = {14},
  number = {10},
  pages = {713--725},
  issn = {1674-800X},
  doi = {10.1093/procel/pwad024},
  urldate = {2025-10-16},
  abstract = {With the gradual maturity of sequencing technology, many microbiome studies have published, driving the emergence and advance of related analysis tools. R language is the widely used platform for microbiome data analysis for powerful functions. However, tens of thousands of R packages and numerous similar analysis tools have brought major challenges for many researchers to explore microbiome data. How to choose suitable, efficient, convenient, and easy-to-learn tools from the numerous R packages has become a problem for many microbiome researchers. We have organized 324 common R packages for microbiome analysis and classified them according to application categories (diversity, difference, biomarker, correlation and network, functional prediction, and others), which could help researchers quickly find relevant R packages for microbiome analysis. Furthermore, we systematically sorted the integrated R packages (phyloseq, microbiome, MicrobiomeAnalystR, Animalcules, microeco, and amplicon) for microbiome analysis, and summarized the advantages and limitations, which will help researchers choose the appropriate tools. Finally, we thoroughly reviewed the R packages for microbiome analysis, summarized most of the common analysis content in the microbiome, and formed the most suitable pipeline for microbiome analysis. This paper is accompanied by hundreds of examples with 10,000 lines codes in GitHub, which can help beginners to learn, also help analysts compare and test different tools. This paper systematically sorts the application of R in microbiome, providing an important theoretical basis and practical reference for the development of better microbiome tools in the future. All the code is available at GitHub github.com/taowenmicro/EasyMicrobiomeR.,  Graphical Abstract},
  pmcid = {PMC10599642},
  pmid = {37128855},
  file = {/home/santi/Zotero/storage/JBSST5IJ/Wen et al. - 2023 - The best practice for microbiome analysis using R.pdf}
}

@article{wilson2017,
  title = {Good Enough Practices in Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2017},
  month = jun,
  journal = {PLOS Computational Biology},
  volume = {13},
  number = {6},
  pages = {e1005510},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005510},
  urldate = {2025-10-11},
  abstract = {Author summary Computers are now essential in all branches of science, but most researchers are never taught the equivalent of basic lab skills for research computing. As a result, data can get lost, analyses can take much longer than necessary, and researchers are limited in how effectively they can work with software and data. Computing workflows need to follow the same practices as lab projects and notebooks, with organized data, documented steps, and the project structured for reproducibility, but researchers new to computing often don't know where to start. This paper presents a set of good computing practices that every researcher can adopt, regardless of their current level of computational skill. These practices, which encompass data management, programming, collaborating with colleagues, organizing projects, tracking work, and writing manuscripts, are drawn from a wide variety of published sources from our daily lives and from our work with volunteer organizations that have delivered workshops to over 11,000 people since 2010.},
  langid = {english},
  keywords = {Computer software,Control systems,Data management,Metadata,Programming languages,Reproducibility,Software tools,Source code},
  file = {/home/santi/Zotero/storage/Y9D7QYXF/Wilson et al. - 2017 - Good enough practices in scientific computing.pdf}
}

@incollection{xu2016,
  title = {Empowering {{R}} with {{High Performance Computing Resources}} for {{Big Data Analytics}}},
  booktitle = {Conquering {{Big Data}} with {{High Performance Computing}}},
  author = {Xu, Weijia and Huang, Ruizhu and Zhang, Hui and {El-Khamra}, Yaakoub and Walling, David},
  editor = {Arora, Ritu},
  year = {2016},
  pages = {191--217},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-33742-5_9},
  urldate = {2025-10-10},
  isbn = {978-3-319-33740-1 978-3-319-33742-5},
  langid = {english},
  file = {/home/santi/Zotero/storage/JH2XRYKJ/Xu et al. - 2016 - Empowering R with High Performance Computing Resources for Big Data Analytics.pdf}
}

@article{yang2021,
  title = {A Review of Computational Tools for Generating Metagenome-Assembled Genomes from Metagenomic Sequencing Data},
  author = {Yang, Chao and Chowdhury, Debajyoti and Zhang, Zhenmiao and Cheung, William K. and Lu, Aiping and Bian, Zhaoxiang and Zhang, Lu},
  year = {2021},
  month = jan,
  journal = {Computational and Structural Biotechnology Journal},
  volume = {19},
  pages = {6301--6314},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2021.11.028},
  urldate = {2025-10-14},
  abstract = {Metagenomic sequencing provides a culture-independent avenue to investigate the complex microbial communities by constructing metagenome-assembled genomes (MAGs). A MAG represents a microbial genome by a group of sequences from genome assembly with similar characteristics. It enables us to identify novel species and understand their potential functions in a dynamic ecosystem. Many computational tools have been developed to construct and annotate MAGs from metagenomic sequencing, however, there is a prominent gap to comprehensively introduce their background and practical performance. In this paper, we have thoroughly investigated the computational tools designed for both upstream and downstream analyses, including metagenome assembly, metagenome binning, gene prediction, functional annotation, taxonomic classification, and profiling. We have categorized the commonly used tools into unique groups based on their functional background and introduced the underlying core algorithms and associated information to demonstrate a comparative outlook. Furthermore, we have emphasized the computational requisition and offered guidance to the users to select the most efficient tools. Finally, we have indicated current limitations, potential solutions, and future perspectives for further improving the tools of MAG construction and annotation. We believe that our work provides a consolidated resource for the current stage of MAG studies and shed light on the future development of more effective MAG analysis tools on metagenomic sequencing.},
  keywords = {Gene functional annotation,Gene prediction,Genome assembly,Metagenome binning,Metagenome-assembled genomes,Metagenomic sequencing,Microbial abundance profiling,Taxonomic classification},
  file = {/home/santi/Zotero/storage/DF6U4FTR/Yang et al. - 2021 - A review of computational tools for generating metagenome-assembled genomes from metagenomic sequenc.pdf;/home/santi/Zotero/storage/Q8WSSPS6/S2001037021004931.html}
}

@article{ye2025,
  title = {{{MAGdb}}: A Comprehensive High Quality {{MAGs}} Repository for Exploring Microbial Metagenome-Assemble Genomes},
  shorttitle = {{{MAGdb}}},
  author = {Ye, Guo and Hong, Hao and Li, Ting and Li, Jin and Wu, Jia-Qi and Jiang, Shuai and Meng, Zhi-Tong and Yuan, He-Tian and Xue, Wen and Li, Ai-Ling and Zhou, Tao and Li, Ting-Ting and Li, Tao},
  year = {2025},
  month = sep,
  journal = {Genome Biology},
  volume = {26},
  number = {1},
  pages = {276},
  issn = {1474-760X},
  doi = {10.1186/s13059-025-03711-6},
  urldate = {2025-10-14},
  abstract = {Metagenomic analyses of microbial communities have unveiled a substantial level of interspecies and intraspecies genetic diversity by reconstructing metagenome-assembled genomes (MAGs). The MAG database (MAGdb) boasts an impressive collection of 74 representative research papers, spanning clinical, environmental, and animal categories and comprising 13,702 paired-end run accessions of metagenomic sequencing and 99,672 high quality MAGs with manually curated metadata. MAGdb provides a user-friendly interface that users can browse, search, and download MAGs and their corresponding metadata information. It represents a valuable resource for researchers in discovering potential novel microbial lineages and understanding their ecological roles. MAGdb is publicly available at https://magdb.nanhulab.ac.cn/.},
  keywords = {Database,Genome sequencing,Metagenome,Metagenome-assembled genomes,Microbiome},
  file = {/home/santi/Zotero/storage/BG47T5GQ/Ye et al. - 2025 - MAGdb a comprehensive high quality MAGs repository for exploring microbial metagenome-assemble geno.pdf;/home/santi/Zotero/storage/S69Y3PTD/s13059-025-03711-6.html}
}

@article{zhang2017,
  title = {{{BarcodingR}}: An Integrated r Package for Species Identification Using {{DNA}} Barcodes},
  shorttitle = {{{BarcodingR}}},
  author = {Zhang, Ai-bing and Hao, Meng-di and Yang, Cai-qing and Shi, Zhi-yong},
  year = {2017},
  journal = {Methods in Ecology and Evolution},
  volume = {8},
  number = {5},
  pages = {627--634},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.12682},
  urldate = {2025-09-30},
  abstract = {Species identification via DNA barcodes has recently become an important and routine task in many biodiversity projects using DNA sequence data. Here, we present BarcodingR, an integrated software package that provides a comprehensive implementation of species identification methods, including artificial intelligence, fuzzy-set, Bayesian and kmer-based methods, that are not readily available in other packages. BarcodingR additionally provides new functions for barcode evaluation, barcoding gap analysis, delimitation comparison analysis, species membership analysis and consensus identification. Comparison with other barcoding methods using 11 empirical data sets indicates that on average, FZKMER (implemented in BarcodingR) and one extant barcoding method BRONX outperform all other methods examined in this study. Two other methods, BP and FZ (both implemented in BarcodingR), present similar performance as SVM and BLOG, respectively, and all display better performance than that of Jrip. The software of BarcodingR is open source under GNU General Public License and freely available for all major operating systems.},
  copyright = {{\copyright} 2016 The Authors. Methods in Ecology and Evolution {\copyright} 2016 British Ecological Society},
  langid = {english},
  keywords = {artificial intelligence,barcodes evaluation,BP-based species identification,DNA barcoding,DNA barcoding gaps,DNA taxonomy,fuzzy-set-theory-based approach,species identification},
  file = {/home/santi/Zotero/storage/CAASIQ3X/Zhang et al. - 2017 - BarcodingR an integrated r package for species identification using DNA barcodes.pdf;/home/santi/Zotero/storage/HA8LCNLY/2041-210X.html}
}

@article{zhao2024,
  title = {Thanos: {{An R Package}} for the {{Gene-Centric Analysis}} of {{Functional Potential}} in {{Metagenomic Samples}}},
  shorttitle = {Thanos},
  author = {Zhao, Zhe and Marotta, Federico and Wu, Min},
  year = {2024},
  month = jul,
  journal = {Microorganisms},
  volume = {12},
  number = {7},
  pages = {1264},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-2607},
  doi = {10.3390/microorganisms12071264},
  urldate = {2025-10-16},
  abstract = {As the amount of metagenomic sequencing continues to increase, there is a growing need for tools that help biologists make sense of the data. Specifically, researchers are often interested in the potential of a microbial community to carry out a metabolic reaction, but this analysis requires knitting together multiple software tools into a complex pipeline. Thanos offers a user-friendly R package designed for the pathway-centric analysis and visualization of the functions encoded within metagenomic samples. It allows researchers to go beyond taxonomic profiles and find out, quantitatively, which pathways are prevalent in an environment, as well as comparing different environments in terms of their functional potential. The analysis is based on the sequencing depth of the genes of interest, either in the metagenome-assembled genomes (MAGs) or in the assembled reads (contigs), using a normalization strategy that enables comparison across samples. The package can import the data from multiple formats and offers functions for the visualization of the results as bar plots of the functional profile, box plots of compare functions across samples, and annotated pathway graphs. By streamlining the analysis of the functional potential encoded in microbial communities, Thanos can enable impactful discoveries in all the fields touched by metagenomics, from human health to the environmental sciences.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {functional profiling,KEGG,metagenomics,Rstats,sequencing depth},
  file = {/home/santi/Zotero/storage/ZDDR84MR/Zhao et al. - 2024 - Thanos An R Package for the Gene-Centric Analysis of Functional Potential in Metagenomic Samples.pdf}
}

@article{zirion-martinez2024,
  title = {A {{Data Carpentry- Style Metagenomics Workshop}}},
  author = {{Ziri{\'o}n-Mart{\'i}nez}, Claudia and {Garfias-Gallegos}, Diego and {Arellano-Fernandez}, Tania Vanessa and {Espinosa-Jaime}, Aar{\'o}n and {Bustos-D{\'i}az}, Edder D. and {Lovaco-Flores}, Jos{\'e} Abel and {Tejero-G{\'o}mez}, Luis Gerardo and {Avelar-Rivas}, J. Abraham and {S{\'e}lem-Mojica}, Nelly},
  year = {2024},
  month = feb,
  journal = {Journal of Open Source Education},
  volume = {7},
  number = {72},
  pages = {209},
  issn = {2577-3569},
  doi = {10.21105/jose.00209},
  urldate = {2025-10-14},
  abstract = {Ziri{\'o}n-Mart{\'i}nez et al., (2024). A Data Carpentry- Style Metagenomics Workshop. Journal of Open Source Education, 7(72), 209, https://doi.org/10.21105/jose.00209},
  langid = {english},
  file = {/home/santi/Zotero/storage/PAFUL9B9/ZiriÃ³n-MartÃ­nez et al. - 2024 - A Data Carpentry- Style Metagenomics Workshop.pdf}
}

@article{zirion-martinez2024a,
  title = {A {{Data Carpentry- Style Metagenomics Workshop}}},
  author = {{Ziri{\'o}n-Mart{\'i}nez}, Claudia and {Garfias-Gallegos}, Diego and {Arellano-Fernandez}, Tania Vanessa and {Espinosa-Jaime}, Aar{\'o}n and {Bustos-D{\'i}az}, Edder D and {Lovaco-Flores}, Jos{\'e} Abel and {Tejero-G{\'o}mez}, Luis Gerardo and {Avelar-Rivas}, J Abraham and {S{\'e}lem-Mojica}, Nelly},
  year = {2024},
  month = feb,
  journal = {Journal of Open Source Education},
  volume = {7},
  number = {72},
  pages = {209},
  issn = {2577-3569},
  doi = {10.21105/jose.00209},
  urldate = {2025-10-16},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  file = {/home/santi/Zotero/storage/HNHN9UJK/ZiriÃ³n-MartÃ­nez et al. - 2024 - A Data Carpentry- Style Metagenomics Workshop.pdf}
}

@article{zizka2019,
  title = {{{CoordinateCleaner}}: {{Standardized}} Cleaning of Occurrence Records from Biological Collection Databases},
  shorttitle = {{{CoordinateCleaner}}},
  author = {Zizka, Alexander and Silvestro, Daniele and Andermann, Tobias and Azevedo, Josu{\'e} and Duarte Ritter, Camila and Edler, Daniel and Farooq, Harith and Herdean, Andrei and Ariza, Mar{\'i}a and Scharn, Ruud and Svantesson, Sten and Wengstr{\"o}m, Niklas and Zizka, Vera and Antonelli, Alexandre},
  year = {2019},
  journal = {Methods in Ecology and Evolution},
  volume = {10},
  number = {5},
  pages = {744--751},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13152},
  urldate = {2025-09-30},
  abstract = {Species occurrence records from online databases are an indispensable resource in ecological, biogeographical and palaeontological research. However, issues with data quality, especially incorrect geo-referencing or dating, can diminish their usefulness. Manual cleaning is time-consuming, error prone, difficult to reproduce and limited to known geographical areas and taxonomic groups, making it impractical for datasets with thousands or millions of records. Here, we present CoordinateCleaner, an r-package to scan datasets of species occurrence records for geo-referencing and dating imprecisions and data entry errors in a standardized and reproducible way. CoordinateCleaner is tailored to problems common in biological and palaeontological databases and can handle datasets with millions of records. The software includes (a) functions to flag potentially problematic coordinate records based on geographical gazetteers, (b) a global database of 9,691 geo-referenced biodiversity institutions to identify records that are likely from horticulture or captivity, (c) novel algorithms to identify datasets with rasterized data, conversion errors and strong decimal rounding and (d) spatio-temporal tests for fossils. We describe the individual functions available in CoordinateCleaner and demonstrate them on more than 90 million occurrences of flowering plants from the Global Biodiversity Information Facility (GBIF) and 19,000 fossil occurrences from the Palaeobiology Database (PBDB). We find that in GBIF more than 3.4 million records (3.7\%) are potentially problematic and that 179 of the tested contributing datasets (18.5\%) might be biased by rasterized coordinates. In PBDB, 1205 records (6.3\%) are potentially problematic. All cleaning functions and the biodiversity institution database are open-source and available within the CoordinateCleaner r-package.},
  copyright = {{\copyright} 2019 The Authors. Methods in Ecology and Evolution published by John Wiley \& Sons Ltd on behalf of British Ecological Society.},
  langid = {english},
  keywords = {biodiversity institutions,data quality,fossils,GBIF,geo-referencing,palaeobiology database (PBDB),r package,species distribution modelling},
  file = {/home/santi/Zotero/storage/CWBPXEB8/Zizka et al. - 2019 - CoordinateCleaner Standardized cleaning of occurrence records from biological collection databases.pdf;/home/santi/Zotero/storage/YKA27GLV/2041-210X.html}
}

@misc{zotero-item-13446,
  title = {Advanced Metagenomic {{Sequence Analysis}} in {{R}}},
  journal = {Advanced metagenomic Sequence Analysis in R},
  urldate = {2025-10-16},
  abstract = {Manual for version 1, revision 2},
  howpublished = {https://askarbek-orakov.github.io/ASAR/},
  langid = {american},
  file = {/home/santi/Zotero/storage/BYFB39ZM/ASAR.html}
}
