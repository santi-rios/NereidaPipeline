---
title: "AnÃ¡lisis de Biodiversidad Marina: Pipeline Integrado para ConservaciÃ³n y EvoluciÃ³n"
subtitle: "Reporte TÃ©cnico de AnÃ¡lisis Espacial y Evolutivo de Especies Marinas"
date: "`r Sys.Date()`"
author:
  - name: Santiago GarcÃ­a RÃ­os
    degrees: 
      - BiÃ³logo
      - Maestro en C. (trÃ¡mite)
    roles: "Conceived and designed the study, analysed the results and wrote the manuscript."
    orcid: 0000-0001-6237-9616
    email: santiago_gr@ciencias.unam.mx
    phone: 55-81-07-99-11
    url: https://santi-rios.github.io/
    affiliation: 
      - name: Universidad Nacional AutÃ³noma de MÃ©xico
        city: Ciudad de MÃ©xico
abstract: > 
  Este reporte presenta los resultados del pipeline integrado de anÃ¡lisis de biodiversidad marina, enfocado en la conservaciÃ³n y evoluciÃ³n de especies marinas del Caribe. El anÃ¡lisis incluye datos de ocurrencia de mÃºltiples fuentes, validaciÃ³n de coordenadas, modelado de idoneidad de hÃ¡bitat, y anÃ¡lisis espacial comprehensivo. Combina tres herramientas clave de manera muy efectiva: Make como orquestador, Nix (a travÃ©s de rix) como gestor del ambiente computacional y {targets} para la gestiÃ³n de la pipeline de anÃ¡lisis.
  - Biodiversidad
  - Pipeline
license:
  text: >
    The code in this repository is licensed under MIT License and the academic quarto report is licensed under CC BY 4.0
  type: open-access
  url: https://www.gnu.org/licenses/fdl-1.3-standalone.html
copyright: 
  holder: Santiago Garcia-Rios
  year: 2025
citation: 
  container-title: NereidaPipeline
  volume: 1
  issue: 1
  doi: 10.xxxx/xxxxxxxxx
format:
  html:
    # title
    title-block-banner: "../figures/banner.png"
    title-block-banner-color: white
    # toc
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: true
    anchor-sections: true
    # code
    code-fold: true
    code-copy: true
    # code-link: true
    code-tools: true
    # figures and links
    lightbox: true
    link-external-icon: true
    # theme
    theme: cosmo
    # css: styles.css
    # render
    embed-resources: true
execute: 
  echo: false
  warning: false
  message: false 
bibliography: "../referencias/referencias.bib"
lang: es
---

<!-- TODO: CV orcid y PDF y https://santi-rios.github.io/ -->

<!-- TODO: fetch articles https://www.icmyl.unam.mx/es/quienes-somos/publicaciones-del-instituto -->

<!-- TODO: ensamble biomatr -->

<!-- TODO: https://roxygen2.r-lib.org/ -->

<!-- TODO: jupyter -->

```{r}
#| label: setup
#| include: false

# Cargar paquetes necesarios para el reporte
suppressPackageStartupMessages({
  library(targets)
  library(dplyr)
  library(knitr)
  library(DT)
  library(plotly)
  library(leaflet)
  library(ggplot2)
  library(tidyr)
  library(kableExtra)
})

# Cargar los objetos del pipeline de targets necesarios para el reporte.
# tar_quarto() detectarÃ¡ estas dependencias y ejecutarÃ¡ este reporte
# solo cuando los objetos cambien.
tar_load(
  c(
    "obis_occurrences",
    "gbif_occurrences",
    "cleaned_occurrences",
    "spatial_analysis",
    "habitat_models",
    "data_summaries",
    "pipeline_report",
    "genome_availability",
    "genomic_sequences",
    "genomic_quality"
  )
)

# Crear el objeto 'loaded_targets' para verificar la disponibilidad
target_names <- c(
  "obis_occurrences",
  "gbif_occurrences",
  "cleaned_occurrences",
  "spatial_analysis",
  "habitat_models",
  "data_summaries",
  "pipeline_report"
)
loaded_targets <- sapply(target_names, exists, simplify = FALSE)


# ConfiguraciÃ³n de Knitr
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = 7,
  dpi = 300,
  out.width = "100%"
)

# FunciÃ³n auxiliar para verificar si un objeto existe y tiene datos
safe_check <- function(obj_name) {
  exists(obj_name) && !is.null(get(obj_name)) &&
  (if (is.data.frame(get(obj_name))) nrow(get(obj_name)) > 0 else TRUE) &&
  (if (is.list(get(obj_name))) length(get(obj_name)) > 0 else TRUE)
}

```


# IntroducciÃ³n

La biodiversidad marina enfrenta amenazas sin precedentes debido al cambio climÃ¡tico, la acidificaciÃ³n oceÃ¡nica, y las actividades antropogÃ©nicas. Este anÃ¡lisis utiliza un pipeline integrado que combina mÃºltiples fuentes de datos y mÃ©todos analÃ­ticos avanzados para evaluar el estado de conservaciÃ³n y los patrones evolutivos de especies marinas clave.

La robustez metodolÃ³gica es tan importante como los resultados mismos, especialmente en campos como la biologÃ­a de la conservaciÃ³n y la evoluciÃ³n, donde las decisiones pueden tener implicaciones a largo plazo y deben basarse en evidencia auditable y reproducible.

## Ciencias MultiÃ³micas

MultiÃ³micos se refiere a un enfoque de investigaciÃ³n que integra datos de mÃºltiples disciplinas "Ã³micas" (como la genÃ³mica, proteÃ³mica, transcriptÃ³mica, etc.) para obtener una comprensiÃ³n mÃ¡s completa y holÃ­stica de un sistema biolÃ³gico, enfermedad o proceso celular. Al combinar estos diferentes niveles moleculares, los estudios multiÃ³micos permiten inferir mecanismos biolÃ³gicos subyacentes de manera mÃ¡s precisa, desentraÃ±ar las causas de fenÃ³menos complejos y descubrir biomarcadores. 

Ã“micas clave que se integran:

- GenÃ³mica: Estudio de los genes y su variaciÃ³n. 
- TranscriptÃ³mica: Estudio de los ARN mensajeros (ARNm) para entender la expresiÃ³n gÃ©nica. 
- ProteÃ³mica: Estudio de las proteÃ­nas, sus estructuras y funciones. 
- MetabolÃ³mica: Estudio de los metabolitos y cÃ³mo interactÃºan. 
- EpigenÃ³mica: Estudio de los cambios heredables en la expresiÃ³n gÃ©nica sin alterar la secuencia de ADN. 
- Microbioma: Estudio de las comunidades microbianas y su impacto en el huÃ©sped. 


### Metagenomics

Metagenomes are collections of genomic sequences from various (micro)organisms that coexist in any given space. They are like snapshots that can give us information about the taxonomic and even metabolic or functional composition of the communities we decide to study. Thus, metagenomes are usually employed to investigate the ecology of defining characteristics of niches (* e.g.,*, the human gut or the ocean floor).

Since metagenomes are mixtures of sequences that belong to different species, a metagenomic workflow is designed to answer two questions:

    What species are represented in the sample?
    What are they capable of doing?

To find which species are present in a niche, we must do a taxonomic assignation of the obtained sequences. To find out their capabilities, we can look at the genes directly encoded in the metagenome or find the genes associated with the species that we found. In order to know which methodology we should use, it is essential to know what questions we want to answer.
Shotgun and amplicon

There are two paths to obtain information from a complex sample:

    Shotgun Metagenomics
    Metabarcoding.

Each is named after the sequencing methodology employed Moreover, have particular use cases with inherent advantages and disadvantages.

With Shotgun Metagenomics, we sequence random parts (ideally all of them) of the genomes present in a sample. We can search the origin of these pieces (i.e., their taxonomy) and also try to find to what part of the genome they correspond. Given enough pieces, it is possible to obtain complete individual genomes from a shotgun metagenome (MAGs), which could give us a bunch of information about the species in our study. MAGs assembly, however, requires a lot of genomic sequences from one organism. Since the sequencing is done at random, it needs a high depth of community sequencing to ensure that we obtain enough pieces of a given genome. Required depth gets exponentially challenging when our species of interest is not very abundant. It also requires that we have enough DNA to work with, which can be challenging to obtain in some instances. Finally, sequencing is expensive, and because of this, making technical and biological replicates can be prohibitively costly.

On the contrary, Metabarcoding tends to be cheaper, which makes it easier to duplicate and even triplicate them without taking a big financial hit. The lower cost is because Metabarcoding is the collection of small genomic fragments present in the community and amplified through PCR. Ideally, if the amplified region is present only once in every genome, we would not need to sequence the amplicon metagenome so thoroughly because one sequence is all we need to get the information about that genome, and by extension, about that species. On the other hand, if a genome in the community lacks the region targeted by the PCR primers, then no amount of sequencing can give us information about that genome. Conservation across species is why the most popular amplicon used for this methodology are 16S amplicons for Bacteria since every known bacterium has this particular region. Other regions can be chosen, but they are used for specific cases. However, even 16S amplicons are limited to, well, the 16S region, so amplicon metagenomes cannot directly tell us a lot about the metabolic functions found in each genome, although educated guesses can be made by knowing which genes are commonly found in every identified species.

::: {.callout-tip}

Exercise 1: Reviewing metadata

According to the results described for this CCB study.

    What kind of sequencing method do you think they used, and why do you think so?
    A) Metabarcoding
    B) Shotgun metagenomics
    C) Genomics of axenic cultures

    In the table samples treatment information, what was the most critical piece of metadata that the authors took?


    Solution
A) Metabarcoding. False. With this technique, usually, only one region of the genome is amplified.
B) Shotgun Metagenomics. True. Only shotgun metagenomics could have been used to investigate the total number of tRNA genes.
C) Genomics of axenic cultures. False. Information on the microbial community cannot be fully obtained with axenic cultures.

The most crucial thing to know about our data is which community was and was not supplemented with fertilizers.
However, any differences in the technical parts of the study, such as the DNA extraction protocol, could have affected the results, so tracking those is also essential.


    Exercise 2: Differentiate between IDs and sample names

    Depending on the database, several IDs can be used for the same sample. Please open the document where the metadata information is stored. Here, inspect the IDs and find out which of them correspond to sample JP4110514WATERRESIZE

        Solution

        ERS1949771 is the SRA ID corresponding to JP4110514WATERRESIZE



    Exercise 3: Discuss the importance of metadata

    Which other information could you recommend to add in the metadata?

        Solution

        Metadata will depend on the type of the experiment, but some examples are the properties of the water before and after fertilization, sampling, and processing methodology, date and time, place (country, state, region, city, etc.).



:::

### Bioninfomratic wokflows

When working with high-throughput sequencing data, the raw reads you get off the sequencer must pass through several different tools to generate your final desired output. The execution of this set of tools in a specified order is commonly referred to as a workflow or a pipeline.

![](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-02-01.png)


    Quality control - Assessing quality using FastQC and Trimming and/or filtering reads (if necessary)
    Assembly of metagenome
    Binning
    Taxonomic assignation


## El Paradigma de la InvestigaciÃ³n Reproducible

> Establecer el marco conceptual y la necesidad urgente de la reproducibilidad computacional en las ciencias, particularmente en ecologÃ­a y evoluciÃ³n.

[@noble2009]: The core guiding principle is simple: Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. This â€˜â€˜someoneâ€™â€™ could be any.

[@dolstra2006]: The development of principles and tools to support the deployment process has largely been relegated to industry, system administrators, and Unix hackers. This has resulted in a large number of often ad hoc tools that typically automate manual practices but do not address fundamental issues in a systematic and disciplined way. This is evidenced by the huge number of mailing list and forum postings about deployment failures, ranging from applications not working due to missing dependencies, to subtle malfunctions caused by incompatible components. Deployment problems also seem curiously resistant to automation: the same concrete problems appear time and again. Deployment is especially difficult in heavily component-based systemsâ€”such as Unix-based open source softwareâ€”because the effort of dealing with the dependencies can increase super-linearly with each additional dependency. This work describes a system for software deployment called Nix (trough '{rix}-r package') that addresses many of the problems that plague existing deployment systems.

[@wilson2017]: Este artÃ­culo Justifica la necesidad de prÃ¡cticas como la automatizaciÃ³n de flujos de trabajo (e este proyecto usamos herramientas como `Make` y `{targets}`) y el seguimiento de versiones del software (lo que logra con `Nix`). Argumentar que su metodologÃ­a no es una complejidad innecesaria, sino una adhesiÃ³n a las "buenas prÃ¡cticas" recomendadas por la comunidad cientÃ­fica para evitar errores y asegurar la longevidad de los resultados.

[@marwick2018]: Este artÃ­culo aborda directamente el ecosistema de R. Explica cÃ³mo empaquetar un proyecto de anÃ¡lisis de datos (cÃ³digo, datos y el ambiente computacional) para hacerlo reproducible. Proporciona el argumento acadÃ©mico perfecto para justificar por quÃ© usar una herramienta como `Nix` (a travÃ©s de `rix`) es una soluciÃ³n superior a simplemente listar los paquetes, ya que captura el estado completo del sistema.

## GestiÃ³n del Ambiente Computacional con Nix

Estas referencias validan el uso de gestores de ambientes declarativos como Nix para garantizar la reproducibilidad a nivel de software.

[@dolstra2006]: Esta es la fuente original que describe el modelo detrÃ¡s de Nix. Es una referencia mÃ¡s tÃ©cnica, pero citarla demuestra un profundo entendimiento de los fundamentos de su flujo de trabajo. Puede usarla para afirmar que su proyecto se basa en un "modelo puramente funcional" que garantiza la construcciÃ³n de ambientes computacionales determinÃ­sticos, eliminando problemas comunes como el "funciona en mi mÃ¡quina".

Abogar por herramientas como Nix para resolver la "crisis de reproducibilidad" en campos computacionalmente intensivos. Citar este artÃ­culo le permite establecer un paralelismo, argumentando que, al igual que en la neurociencia, la complejidad del anÃ¡lisis de datos genÃ³micos, espaciales y taxonÃ³micos en la conservaciÃ³n marina exige soluciones de reproducibilidad robustas.


### Manejo de ambientes: Nix vs otras opciones

Utilizar 'Nix' mediante el paquete 'rix' es una opciÃ³n mÃ¡s robusta para gestionar entornos cientÃ­ficos y pipelines frente a herramientas como 'conda' o 'renv'. Se incluye una tabla comparativa seguida de una breve recomendaciÃ³n de uso.


| Criterio / Aspecto | Nix (con rix) | Conda | renv |
|---|---:|---:|---:|
| Reproducibilidad determinista | Muy alta. Nix es declarativo e inmune a cambios del sistema; rix genera expresiones reproducibles para R y deps del sistema. | Moderada. Conda puede registrar versiones, pero la resoluciÃ³n de dependencias y canales introduce variabilidad. | Parcial. Fija versiones de paquetes R (lockfile) pero no gestiona dependencias del sistema nativas. |
| GestiÃ³n de dependencias del sistema (C, libs, binarios) | Nativa y exhaustiva. Nix declara y proporciona librerÃ­as del sistema de forma aislada. | Buena (conda-forge), pero limitada para ciertos paquetes del sistema y con problemas de enlaces dinÃ¡micos. | Ninguna. Depende del SO o de gestores externos; riesgo de â€œworks on my machineâ€. |
| Aislamiento / hermeticidad | Alto: entornos aislados en la store de Nix; evita contaminaciÃ³n por el entorno del usuario. | Aislado a nivel de entorno, pero puede verse afectado por bibliotecas del sistema; problemas con PATH/LD_LIBRARY_PATH. | No a nivel de sistema; sÃ³lo controla paquetes R dentro del proyecto. |
| Declaratividad y trazabilidad | Declarativo (nix expressions). FÃ¡cil de versionar, auditar y reconstruir. | Declarativo limitado (environment.yml), pero resoluciÃ³n no determinista entre ejecuciones. | Declarativo para R (renv.lock) pero sin trazabilidad de libs externas. |
| IntegraciÃ³n con CI / archivado a largo plazo | Excelente: reconstrucciÃ³n reproducible en CI; uso de cachÃ©s binarios; apto para archivado y publicaciÃ³n de entornos. | Buena en CI, pero reproducibilidad exacta depende de canales y binarios disponibles. | Adecuado para reproducir entornos R en el corto/medio plazo; falla si faltan deps del sistema. |
| Compatibilidad multi-lenguaje | SÃ­, gestiona todo el stack (R, Python, C, system tools) en un Ãºnico lenguaje de especificaciÃ³n. | Buena (R, Python, otros), pero fragmentaciÃ³n en canales. | Enfocado a R; no gestiona otros lenguajes de forma integrada. |
| Facilidad de uso / curva de aprendizaje | Moderada a alta. rix reduce la barrera para usuarios de R pero Nix tiene conceptos nuevos. | Baja (fÃ¡cil de empezar). Muy accesible para usuarios nuevos. | Muy baja â€” transparente para usuarios R; fÃ¡cil de incorporar. |
| TamaÃ±o / consumo de disco | Amplio (store de Nix), pero con beneficios de cachÃ© y deduplicaciÃ³n entre proyectos. | Variable; entornos duplicados pueden consumir mucho. | Ligero (solo paquetes R), pero requiere deps del sistema por separado. |
| Limitaciones notables | Curva de aprendizaje; en Windows requiere WSL o soluciones; rix aÃºn evoluciona. | Canales y resoluciÃ³n crean incoherencias; algunos paquetes de sistema difÃ­ciles. | No asegura reproducibilidad completa (falta libs del SO); no declara sistema. |
| Uso recomendado | Pipelines reproducibles, CI/archivado, proyectos con dependencias R + sistema, entornos multi-lenguaje y producciÃ³n. | Entornos ad-hoc, exploraciÃ³n rÃ¡pida, usuarios que necesitan rÃ¡pidos â€œenvsâ€ multiplataforma. | Desarrollo R colaborativo rÃ¡pido, lockfiles para paquetes R, proyectos que delegan deps del SO a otra soluciÃ³n. |


::: {.callout-tip}

**Uso de 'conda' + 'Nix' ('{rix}')**

- Es posible usar conda dentro de un entorno generado con 'rix/Nix': 'Nix' puede instalar paquetes del ecosistema 'conda' (p. ej. 'miniconda3') y ejecutar comandos conda desde el 'shell' que 'Nix' provee. 'rix', al final, genera expresiones 'Nix'; en esas expresiones se pueden declarar paquetes del sistema como "'miniconda3'" o "'mamba'" y aÃ±adir un 'shell_hook' para crear/activar un entorno conda/'conda-env' al entrar al 'nix-shell'.  
- Sin embargo, en tÃ©rminos de reproducibilidad y coherencia, incluir 'conda' dentro de 'Nix' es en gran medida **redundante** y atenta contra las garantÃ­as que 'Nix' aporta: 'conda' genera entornos que no son $100\%$ declarativos ni bit-reproducibles (resoluciÃ³n de dependencias y artefactos binarios dependen de servidores externos). Por ello, el patrÃ³n recomendado es gestionar 'Python' y sus binarios desde 'Nix' ('nixpkgs') siempre que sea posible; solo recurrir a conda cuando exista una dependencia binaria crÃ­tica que no estÃ© disponible en 'nixpkgs' o cuando el equipo ya dependa fuertemente de 'conda' y se acepte la pÃ©rdida parcial de determinismo.

**GestiÃ³n de dependencias del sistema**

- 'rix/Nix' gestiona paquetes de sistema ('git', 'libssl', binarios 'C/Fortran') de forma nativa y declarativa. 'Conda' (especialmente 'conda-forge') puede proveer muchos paquetes binarios ('python', librerÃ­as cientÃ­ficas), pero no sustituye a un gestor de sistema para librerÃ­as del sistema (p. ej. 'glibc', versiones del compilador) y su resoluciÃ³n depende de canales; por tanto, no iguala la hermeticidad ni la trazabilidad de 'Nix'.

**Recomendaciones prÃ¡cticas y consideraciones**

- Preferible: declarar 'Python' y paquetes 'Python' directamente en 'Nix' (usar paquetes 'python' de 'nixpkgs' o 'poetry2nix/pypi2nix'), o usar 'mamba/conda' dentro de 'Nix' solo para casos puntuales.
- Ventaja: entornos completamente reconstruibles en 'CI' y archivables con la expresiÃ³n 'Nix'.
- Si se incorpora 'conda' dentro de 'Nix': incluir 'miniconda/mamba' como 'system_pkgs' en 'rix()' y 'use shell_hook' para crear/activar un entorno 'conda' desde un 'environment.yml' almacenado en el repositorio. Documentar explÃ­citamente este paso y aceptar las limitaciones de determinismo.
- 'Conda' puede instalar muchos paquetes â€œdel sistemaâ€ (a travÃ©s de 'conda-forge'), pero no gestiona el sistema base ni garantiza identicidad binaria entre mÃ¡quinas; 'Nix' sÃ­ lo hace.

Ejemplo prÃ¡ctico 

â€” modificaciÃ³n sugerida de' build_env.R' (opcional: integrar 'conda/miniconda' y crear env al entrar al 'nix-shell'):

````r
# ...existing code...
rix(
  r_ver = "latest-upstream",
  r_pkgs = all_packages,
-  system_pkgs = c("git", "python3", "quarto"),
+  # Incluir miniconda3 si se necesita usar conda dentro del entorno Nix.
+  # Alternativa recomendada: gestionar Python desde nixpkgs (evita redundancia).
+  system_pkgs = c("git", "python3", "quarto", "miniconda3"),
-  tex_pkgs = c("amsmath"),
-  ide = "none",
-  shell_hook = "",
+  tex_pkgs = c("amsmath"),
+  ide = "none",
+  # Ejemplo de shell_hook que crea/actualiza un entorno conda local a partir de environment.yml.
+  # Nota: esto ejecuta conda al entrar al nix-shell; la reproducibilidad del entorno conda
+  # depende de conda/mamba y de los canales usados.
+  shell_hook = '
+if [ -f environment.yml ]; then
+  if [ ! -d ".conda-env" ]; then
+    echo "Creando entorno conda local (.conda-env) desde environment.yml..."
+    conda env create -p ./.conda-env -f environment.yml || conda env update -p ./.conda-env -f environment.yml
+  fi
+  export PATH="$(pwd)/.conda-env/bin:$PATH"
+fi
+',
  project_path = ".",
  overwrite = TRUE,
  print = TRUE
)
# ...existing code...
````

Caveats tÃ©cnicos y cierre

- Mezclar 'Nix + conda' es factible pero introduce una capa menos declarativa ('conda'). Para reproducibilidad acadÃ©mica estricta, preferir 'Nix-native' ('nixpkgs/pypi2nix/poetry2nix').
Si se necesita 'conda' por razones prÃ¡cticas (paquetes no disponibles en 'nixpkgs', flujo de trabajo del equipo), documentar la receta ('environment.yml', comando de creaciÃ³n) y versionar el archivo junto al proyecto; idealmente incorporar la creaciÃ³n del entorno ''conda'' en 'CI' para asegurar consistencia operativa aunque no absoluta bit-reproducible.
'Nix' (por medio de 'rix') gestiona 'git' y otras dependencias del sistema de forma nativa y declarativa; 'conda' puede proveer muchos binarios pero no reemplaza la gestiÃ³n total de librerÃ­as del sistema ni ofrece la misma trazabilidad. 

:::


## Pipelines

[@zirion-martinez2024]: Metagenomic analyses aim to explore the genomic diversity of communities in specific
habitats by processing their DNA sequencing data. This analysis is achieved with special-
ized bioinformatics tools, which often require previous coding experience. Furthermore,
beginners can struggle to build a pipeline from raw data to valuable biological insights.
The Carpentries hosts open lessons used worldwide to analyze specialized datasets for
beginners, including a Data Carpentry curriculum for individuals working with genomics
sequencing data. However, a lesson addressing the specific challenges associated with
metagenomics data and analyses was missing. We created a complete Metagenomics
curriculum in The Carpentries Incubator, adapting and expanding on the Data Carpentry
genomics curriculum. The curriculum provides an introduction to programming, teaching
learners to access and handle metagenomics data, and to run commands with the software
needed for completing metagenomics analyses. Content and exercises have been improved
based on experience gathered in teaching the curriculum in three 16-hour online workshops.
We expect to continue to enhance this lesson, which we hope is helpful as a teaching
resource for new instructors in the field, and as a guide for newcomers wishing to perform
metagenomic analyses from scratch 

[@landau2021]: Esta es la cita oficial y directa del paquete `{targets}`. Es indispensable. Ãšsela para introducir la herramienta, explicando que su elecciÃ³n se basa en una soluciÃ³n documentada y revisada por pares diseÃ±ada especÃ­ficamente para crear pipelines de anÃ¡lisis reproducibles y eficientes en R. Resalte caracterÃ­sticas clave que el artÃ­culo menciona, como el seguimiento de dependencias y el paralelismo, que son cruciales para anÃ¡lisis a gran escala en conservaciÃ³n.


## Control de versiones

[@noble2009]: Record every operation that you perform. 2. Comment generously. . Avoid editing intermediate files by hand. Use relative pathnames to access other files within the same project. perhaps most significantly, version control is invaluable for collaborative projects. The repository allows collaborators to work simultaneously on a collection of files, including scripts, documentation, or a draft manuscript. If two individuals edit the same file in parallel, then the version control software will automatically merge the two versions and flag lines that were edited by both people.

# Objetivos

1. **Integrar datos de ocurrencia** de mÃºltiples fuentes (OBIS, GBIF)
2. **Validar y limpiar** coordenadas geogrÃ¡ficas usando mÃ©todos estandarizados
3. **Analizar patrones espaciales** de distribuciÃ³n de especies marinas
4. **Modelar idoneidad de hÃ¡bitat** considerando variables ambientales
5. **Generar recomendaciones** para conservaciÃ³n basadas en evidencia

# Pregunta de INvestigaciÃ³Ã±

Suppose you want to find the source of a nasty gut infection in people. Which type of sequencing methodology would you choose?
Which type of metadata would be helpful to record?


::: {.callout titile="soluciÃ³n"}


For a first exploration, 16S is a better idea since you could detect known pathogens by knowing the taxons in the community. Nevertheless, if the disease is the consequence of a viral infection, the pathogen can only be discovered with shotgun metagenomics (that was the case of SARS-CoV 2). Also, metabarcoding does not provide insights into the genetic basis of the pathogenic phenotypes. Metadata will depend on the type of experiment. For this case, some helpful metadata could be sampling methodology, date, place (country, state, region, city, etc.), patientâ€™s sex and age, the anatomical origin of the sample, symptoms, medical history, diet, lifestyle, and environment.
:::

# Planteamiento del problema

Â¿Por quÃ© es importante la multiÃ³mica? Proporciona una perspectiva mÃ¡s completa de un sistema biolÃ³gico en lugar de solo estudiar componentes aislados. Sin embargo, es la IntegraciÃ³n de datos complejos, los cuales Permiten integrar y analizar grandes volÃºmenes de datos generados por diferentes tecnologÃ­as de alto rendimiento, no suele ser un paso sencillo.

For example, Next-Generation-Sequencing (NGS) bioinformatics uses computational tools, software, and algorithms to process and analyze the massive datasets generated by Next-Generation Sequencing (NGS). It involves cleaning and aligning millions of DNA or RNA fragments to reconstruct genomes or transcriptomes, identifying genetic variants like mutations, and annotating these variants to understand their biological significance. It is essential to consider: Data Processing: Raw sequence reads from NGS instruments are complex and need to be cleaned, aligned to a reference genome, and assembled to form longer sequences. Variant Calling and Annotation: The process identifies genetic variations (such as single nucleotide polymorphisms or structural variations) and adds information about these variants' potential functions or links to disease. Interpretation and Visualization: The analyzed data is presented in user-friendly formats like reports and visualizations, allowing researchers and clinicians to make informed decisions. 

[@dolstra2006]: Software deployment is the problem of managing the distribution of software to end-user machines. That is, a developer has created some piece of software, and this ultimately has to end up on the machines of end-users. After the initial installation of the software, it might need to be upgraded or uninstalled. Presumably, the developer has tested the software and found it to work sufficiently well, so the challenge is to make sure that the software works just as well, i.e., the same, on the end-user machines. I will informally refer to this as correct deployment: given identical inputs, the software should behave the same on an end-user machine as on the developer machine1. This should be a simple problem. For instance, if the software consists of a set of files, then deployment should be a simple matter of copying those to the target machines. In practice, deployment turns out to be much harder. This has a number of causes. These fall into two broad categories: environment issues and manageability issues. Even worse, the component might be dependent on a specific compiler, or on specific compilation options being used for its dependencies. This is often a rather labour-intensive part of the deployment process. 

[@dolstra2006]: the Nix deployment system, which overcomes the limitations of contemporary deployment tools described above. It solves implementation (how it works), the underlying principles (why it works), our experiences and empirical validation (that it works), and the application areas to which it can be applied (where it works).

La gestiÃ³n de la pipeline es crucial para la eficiencia y la correcciÃ³n de los anÃ¡lisis complejos. `{targets}` es el estÃ¡ndar moderno en el ecosistema R.

[@zirion-martinez2024]: La gestiÃ³n de la pipeline es crucial para la eficiencia y la correcciÃ³n de los anÃ¡lisis complejos. ioinformatic tools are now essential to our understanding of biological systems. Open
lessons for general-purpose coding languages and specialized topics such as genomics,
ecology, and even metagenomics are already available (Darling et al., 2019), (Lessons, n.d.),
(Kruchten, 2020). Nevertheless, a complete guide for shotgun metagenomics, assuming
no prior knowledge of coding and provided hardware and software solutions, was missing.
We introduce a curriculum to fill this gap, which teaches the skills required to conduct a
comprehensive metagenomics workflow with Bash and R programming in a pre-installed
remote server.

[@landau2021]: Esta es la cita oficial y directa del paquete `{targets}`. Es indispensable. Ãšsela para introducir la herramienta, explicando que su elecciÃ³n se basa en una soluciÃ³n documentada y revisada por pares diseÃ±ada especÃ­ficamente para crear pipelines de anÃ¡lisis reproducibles y eficientes en R. Resalte caracterÃ­sticas clave que el artÃ­culo menciona, como el seguimiento de dependencias y el paralelismo, que son cruciales para anÃ¡lisis a gran escala en conservaciÃ³n.

[@noble2009]: A few months from now, you may not remember what you were up to when you created a particular set of files, or you may not remember what you drew. You will either have to then spend time reconstructing your previous experiments or lose whatever insights you gained from those experiments.

[@noble2009]: it is important to handle long-running scrips and its outputs. The final line of a runall script calls summarize, which in turn creates a plot, table, or HTML page that summarizes the results of the experiment (in our case, we use quarto for this). The summarize script is written in such a way that it can interpret a partially completed experiment, showing how much of the computation has been performed thus far.

[@boyle2009]: changes in software engineering and design include: the methodology through which software is constructed (e.g. components leading to frameworks, and frameworks leading to aspects [1]); the technology used to allow for distributed computing (e.g. object brokers evolving pass-byvalue mechanisms, and these being replaced by stateless Web Services); and the ideology that is used to define the process through which software is built (e.g. the "rational" processes being replaced by agile programming).

[@noble2009]:In addition, the need to make results accessible to and understandable by wet lab biologists may have practical implications for how a project is managed. For example, to make the results more understandable, significant effort may need to go into the prose descriptions of experiments in the lab notebook, rather than simply including a figure or table with a few lines of text summarizing the major conclusion. More practically, differences in operating systems and software may cause logistical difficulties. For example, computer scientists may prefer to write their documents in the LaTeX typesetting language, whereas biologists may prefer Microsoft Word.

GeneraciÃ³n de reportes tÃ©cnicos con Quarto
Scholarly articles require much more detail in their front matter than simply a title and an author. Quarto provides a rich set of YAML metadata keys to describe these details. On this page, youâ€™ll learn how to specify authors and their affiliations, article summaries like an abstract and keywords, and how to include information on copyright, licensing and funding.

Lastly, data integrity, accesibility, and size as it grows requieres more than cvs files in an academic context. 

[@boyle2009]: database based solutions to open, distributed, interoperable data management solutions (see Figure 1). This change has been driven by demands for rapid development, high levels of interoperability and increases in data volume and complexity.

# MetodologÃ­a

## Crear el ambiente con Nix

El paquete R {rix} 'r library(rix)', en lÃ­nea con este enfoque, permite una integraciÃ³n simplificada de Nix en la plataforma R. Esto significa que las dependencias de R, del sistema y el control de versiones pueden ser gestionados de forma centralizada a travÃ©s de Nix. La inclusiÃ³n de {rix} en este proyecto permite el manejo eficiente de las dependencias de R de manera reproducible (Gabay et al., 2019).

En conclusiÃ³n, la fundamentaciÃ³n del proyecto en la integraciÃ³n de R, Nix y el paquete {rix} se traduce en una robustez y una gestiÃ³n optimizada de las dependencias, facilitando notablemente el procesamiento y el anÃ¡lisis de datos y el manejo de cÃ³digo. AdemÃ¡s, permite la integraciÃ³n eficiente de informaciÃ³n en repositorios y proporciona un entorno de trabajo reproducible, lo cual es vital para mantener la validez y la replicabilidad de las investigaciones cientÃ­ficas.

## AutomatizaciÃ³n de Pipelines con `{targets}`

La gestiÃ³n de la pipeline es crucial para la eficiencia y la correcciÃ³n de los anÃ¡lisis complejos. `{targets}` es el estÃ¡ndar moderno en el ecosistema R.

## OrquestaciÃ³n del Flujo de Trabajo con `Make`

El uso de `Make` como un "orquestador" de alto nivel simplifica la interacciÃ³n y estandariza los procedimientos.

[@noble2009]: Este influyente artÃ­culo describe las mejores prÃ¡cticas para organizar proyectos de biologÃ­a computacional. Recomienda explÃ­citamente el uso de `Make` para automatizar la pipeline, desde la descarga de datos hasta la generaciÃ³n de figuras finales. Citarlo posiciona su uso de `Makefile` como una prÃ¡ctica establecida y recomendada para mantener la organizaciÃ³n, la claridad y la automatizaciÃ³n en proyectos complejos, lo cual es vital en estudios integrativos de biodiversidad marina.

## PreparaciÃ³n de librerÃ­as

DNA or RNA is fragmen                                                                       ted and prepared for sequencing by adding sequencing adapters.

Sequencing: The prepared samples are run on high-throughput sequencing machines to generate millions of short reads.

Primary Analysis: Raw sequence data is converted into FASTQ files, which contain sequence data and quality scores. 

Secondary Analysis: Reads are aligned to a reference genome, and variants are identified and annotated. 

Tertiary Analysis: The identified variants are interpreted for their potential impact on biological pathways and phenotypes, often by querying genomic databases. 

### biomatr: AdquisiciÃ³n de Datos Ã³micos

Se utilizÃ³ el paquete 'r library(biomatr)', que nos permite obtener datos de organismos de mÃºltiples bases de datos (RefSeq, GenBank, or Ensembl)

Multiple data types: Genome, proteome, CDS, GFF, RNA
Availability checking: Check what's available before downloading
Assembly stats: Get quality metrics for assemblies
Better error handling: Clear messages about what's available and what failed.

Implementamos este paquete de tal manera que:

when you run the pipeline:

It will check all databases (refseq, genbank, ensembl)
Generate a comprehensive availability report
Download all available data
Assess data quality
Generate a beautiful HTML report with:
Species information
Database availability tables
Success/failure summaries
Data size statistics
Visual heatmaps
Proper citations for all software and data sources
The report will be saved as reports/genomic_data_retrieval.html and can be opened in any browser!

### Functional annotation 

Extract GO terms and protein domains from the successfully retrieved A. palmata data
Sequence statistics - Analyze the proteome and CDS sequences

Extract GO terms and protein domains from the successfully retrieved A. palmata data

### sequence statistics 

Analyze the proteome and CDS sequences

### Sequence Analysis

Use the retrieved sequences for comparative genomics
Functional Annotation - Extract GO terms and protein domains

### Homology Analysis

Find orthologs across species

### myTAI: Phylogenetic Analysis

Use  for evolutionary transcriptomics


### metagenomics

- https://academic.oup.com/ismej/article/17/1/140/7474015 
  - calculate MAGs abundance in a metagenomic dataset/community using RPKM, TPM, the mean raw read coverage of a MAG, and many other metrics. Usually the metrics are calculated in CheckM, MetaWRAP, CoverM
  - escribes GCPM (genome copies per million reads) calculation based on TPM as it is implemented in MetaWRAP software
  - MetaWRAP github page and noticed that "quant_bins" - module that calculates GCPM - have attracted some critique, which left without an answer from the creator

- [reddit bioinfrmatis](https://www.reddit.com/r/bioinformatics/comments/1e8nptd/metrics_you_use_in_your_metagenomemags_analysis/?sort=top)
  - Relative abundance is tricky because I think what researchers want to say is what fraction of the (prokaryotic) cells of community belong to a particular population. Yet we canâ€™t really do that with a metagenome alone â€” not without sufficient metadata or spike-in standards.

- [dereplicate MAGs? (metagenome assembly genomes)](https://www.reddit.com/r/bioinformatics/comments/or0r6z/how_to_dereplicate_mags_metagenome_assembly/?sort=top)
  - ou should look at https://github.com/MrOlm/drep . Thats exactly the tool you need. Usually one takes all their bins and dereplicates them at 99% ANI for strain level dereplication or 95% at species level. For preserving the best quality, you would need to check yourself if the best genome survived the dereplication afaik. You probably want some sort of quality score and chose the one with the highest score out of the bins that got grouped as one by dereplication.

#### sujetos experimentales

::: {.callout  titile"ejemplo"}

During this lesson, we will work with actual metagenomic information, so we should be familiarized with it. The metagenomes that we will use were collected in Cuatro CiÃ©negas, a region that has been extensively studied by Valeria Souza. Cuatro CiÃ©negas is an oasis in the Mexican desert whose environmental conditions are often linked to the ones present in ancient seas, due to a higher-than-average content of sulfur and magnesium but a lower concentrations of phosphorus and other nutrients. Because of these particular conditions, the Cuatro CiÃ©negas basin is a fascinating place to conduct a metagenomic study to learn more about the bacterial diversity that is capable to survive and thrive in that environment.

The particular metagenomic study that we are going to work with was collected in a study about the response of the Cuatro Cienegasâ€™ bacterial community to nutrient enrichment. In this study, authors compared the differences between the microbial community in its natural, oligotrophic, phosphorus-deficient environment, a pond from the Cuatro CiÃ©negas Basin (CCB), and the same microbial community under a fertilization treatment. The comparison between bacterial communities showed that many genomic traits, such as mean bacterial genome size, GC content, total number of tRNA genes, total number of rRNA genes, and codon usage bias were significantly changed when the bacterial community underwent the treatment.

:::


#### On metadata 

Once we have chosen an adequate methodology for our study, we must take extensive notes on the origin of our samples and how we treated them. These notes constitute the metadata, or data about our data, and they are crucial to understanding and interpreting the results we will obtain later in our metagenomic analysis. Most of the time, the differences that we observe when comparing metagenomes can be correlated to the metadata, which is why we must devote a whole section of our experimental design to the metadata we expect to collect and record carefully.

#### Databases

The results of this study, raw sequences, and metadata have been submitted to the NCBI Sequence Read Archive (SRA) and stored in the BioProject PRJEB22811.

    Other metagenomic databases

    The NCBI SRA is not the only repository for metagenomic information. There are other public metagenomic databases such as MG-RAST, MGnify, Marine Metagenomics Portal, Terrestrial Metagenome DB and the GM Repo.

Each database requires certain metadata linked with the data. As an example, when JP4D.fasta is uploaded to mg-RAST the associated metadata looks like this:
Column 	Description
file_name 	JP4D.fasta
investigation_type 	metagenome
seq_meth 	Illumina
project_description 	This project is a teaching project and uses data from Okie et al. Elife 2020
collection_date 	2012-06-27
country 	Mexico
feature 	pond water
latitude 	26.8717055555556
longitude 	-102.14
env_package 	water
depth 	0.165


#### Quality control

 Questions

    How can I describe the quality of my data?

Objectives

    Explain how a FASTQ file encodes per-base quality scores.

    Interpret a FastQC plot summarizing per-base quality across all reads.

    Use for loops to automate operations on multiple files.

Although it looks complicated (and it is), we can understand the FASTQ format with a little decoding. Some rules about the format include the following:
Line 	Description
1 	Always begins with â€˜@â€™ followed by the information about the read
2 	The actual DNA sequence
3 	Always begins with a â€˜+â€™ and sometimes contains the same info as in line 1
4 	Has a string of characters which represent the quality scores; must have same number of characters as line 2

```

$ cd ~/dc_workshop/data/untrimmed_fastq/

$ gunzip JP4D_R1.fastq.gz

$ head -n 4 JP4D_R1.fastq

@MISEQ-LAB244-W7:156:000000000-A80CV:1:1101:12622:2006 1:N:0:CTCAGA
CCCGTTCCTCGGGCGTGCAGTCGGGCTTGCGGTCTGCCATGTCGTGTTCGGCGTCGGTGGTGCCGATCAGGGTGAAATCCGTCTCGTAGGGGATCGCGAAGATGATCCGCCCGTCCGTGCCCTGAAAGAAATAGCACTTGTCAGATCGGAAGAGCACACGTCTGAACTCCAGTCACCTCAGAATCTCGTATGCCGTCTTCTGCTTGAAAAAAAAAAAAGCAAACCTCTCACTCCCTCTACTCTACTCCCTT                                        
+                                                                                                
A>>1AFC>DD111A0E0001BGEC0AEGCCGEGGFHGHHGHGHHGGHHHGGGGGGGGGGGGGHHGEGGGHHHHGHHGHHHGGHHHHGGGGGGGGGGGGGGGGHHHHHHHGGGGGGGGHGGHHHHHHHHGFHHFFGHHHHHGGGGGGGGGGGGGGGGGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFBFFFF@F@FFFFFFFFFFBBFF?@;@#################################### 

```

Line 4 shows the quality of each nucleotide in the read. Quality is interpreted as the probability of an incorrect base call (e.g., 1 in 10) or, equivalently, the base call accuracy (e.g., 90%). Each nucleotideâ€™s numerical scoreâ€™s value is converted into a character code where every single character represents a quality score for an individual nucleotide. This conversion allows the alignment of each individual nucleotide with its quality score. For example, in the line above, the quality score line is:

```
A>>1AFC>DD111A0E0001BGEC0AEGCCGEGGFHGHHGHGHHGGHHHGGGGGGGGGGGGGHHGEGGGHHHHGHHGHHHGGHHHHGGGGGGGGGGGGGGGGHHHHHHHGGGGGGGGHGGHHHHHHHHGFHHFFGHHHHHGGGGGGGGGGGGGGGGGGGGGGGGGGGGFFFFFFFFFFFFFFFFFFFFFBFFFF@F@FFFFFFFFFFBBFF?@;@#################################### 

```

The numerical value assigned to each character depends on the sequencing platform that generated the reads. The sequencing machine used to generate our data uses the standard Sanger quality PHRED score encoding, using Illumina version 1.8 onwards. Each character is assigned a quality score between 0 and 41, as shown in the chart below.

Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJ
                   |         |         |         |         |
Quality score:    01........11........21........31........41                                

Each quality score represents the probability that the corresponding nucleotide call is incorrect. These probability values are the results of the base calling algorithm and depend on how much signal was captured for the base incorporation. This quality score is logarithmically based, so a quality score of 10 reflects a base call accuracy of 90%, but a quality score of 20 reflects a base call accuracy of 99%. In this link you can find more information about quality scores.


> ðŸ’¡ idea: se puede usar fastq

##### working with the fastqc results

  > We can make a record of the results we obtained for all our samples by concatenating all of our summary.txt files into a single file using the cat command. Weâ€™ll call this fastqc_summaries.txt and store it to ~/dc_workshop/docs.

Which samples failed at least one of FastQCâ€™s quality tests? What test(s) did those samples failed

Explore MultiQC if you want a tool that can show the quality of many samples at once.


#### Trimming and Filtering


Teaching: 30 min
Exercises: 25 min
Questions

    How can we get rid of sequence data that does not meet our quality standards?

Objectives

    Clean FASTQ reads using Trimmomatic.

    Select and set multiple options for command line bioinformatic tools.

    Write for loops with two variables.


In the last episode, we took a high-level look at the quality of each of our samples using FastQC. We visualized per-base quality graphs showing the distribution of the quality at each base across all the reads from our sample. This information helps us to determine the quality threshold we will accept, and thus, we saw information about which samples fail which quality checks. Some of our samples failed quite a few quality metrics used by FastQC. However, this does not mean that our samples should be thrown out! It is common to have some quality metrics fail, which may or may not be a problem for your downstream application. For our workflow, we will remove some low-quality sequences to reduce our false-positive rate due to sequencing errors.

To accomplish this, we will use a program called Trimmomatic. This useful tool filters poor quality reads and trims poor quality bases from the specified samples.

Bonus Exercise (Advanced): Quality test after trimming

Now that our samples have gone through quality control, they should perform better on the quality tests run by FastQC.

Sort the following chunks of code to re-run FastQC on your trimmed FASTQ files and visualize the HTML files to see whether your per base sequence quality is higher after trimming.

#### Metagenome Assembly

 Questions

    Why should genomic data be assembled?

    What is the difference between reads and contigs?

    How can we assemble a metagenome?

Objectives

    Understand what an assembly is.

    Run a metagenomics assembly workflow.

    Use an environment in a bioinformatic pipeline.


The assembly process groups reads into contigs and contigs into scaffolds to obtain (ideally) the sequence of a whole chromosome. There are many programs devoted to genome and metagenome assembly, some of the main strategies they use are Greedy extension, OLC, and De Bruijn charts. Contrary to metabarcoding, shotgun metagenomics needs an assembly step, which does not mean that metabarcoding never uses an assembly step but sometimes is unnecessary.

![](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-04-01.png)

MetaSPAdes is an NGS de novo assembler for assembling large and complex metagenomics data, and it is one of the most used and recommended. It is part of the SPAdes toolkit, which contains several assembly pipelines.

Some of the problems faced by metagenomics assembly are:

    Differences in coverage between the genomes due to the differences in abundance in the sample.
    The fact that different species often share conserved regions.
    The presence of several strains of a single species in the community.

SPAdes already deals with the non-uniform coverage problem in its algorithm, so it is helpful for the assembly of simple communities, but the metaSPAdes algorithm deals with the other problems as well, allowing it to assemble metagenomes from complex communities.

The process of (metagenomics) assembly can take a long time, and if the connection to the server drops, the process is killed, and the process needs to restart. To avoid this, we can create a screen session.

Exercise 2: Compare two fasta files from the assembly output

You want to know how many contigs and scaffolds result from the assembly. Use contigs.fasta and scaffolds.fasta files and sort the commands to create correct code lines.
Do they have the same number of lines? Why?
Hint: You can use the following commands: grep, | (pipe), -l, ">", wc, filename.fasta


    Quality of assemblies

    You can use several metrics to know the quality of your assemblies. MetaQuast is a program that gives you these metrics for metagenome assemblies in an interactive report and text files and plots.


#### Metagenome Binning

 Questions

    How can we obtain the original genomes from a metagenome?

Objectives

    Obtain Metagenome-Assembled Genomes from the metagenomic assembly.

    Check the quality of the Metagenome-Assembled genomes.



Original genomes in the sample can be separated with a process called binning. This process allows separate analysis of each species contained in the metagenome with enough reads to reconstruct a genome. Genomes reconstructed from metagenomic assemblies are called MAGs (Metagenome-Assembled Genomes). In this process, the assembled contigs from the metagenome will be assigned to different bins (FASTA files that contain certain contigs). Ideally, each bin corresponds to only one original genome (a MAG).

![](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-05-01.png)

Although an obvious way to separate contigs that correspond to a different species is by their taxonomic assignation, there are more reliable methods that do the binning using characteristics of the contigs, such as their GC content, the use of tetranucleotides (composition), or their coverage (abundance).

Maxbin is a binning algorithm that distinguishes between contigs that belong to different bins according to their coverage levels and the tetranucleotide frequencies they have.

::: {.callout}
Discussion 1: Relation between MAGs and depth

The sequencing center has returned you a file with 18,412 reads. Given that the bacterial genome size range between 4Mbp and 13Mbp (Mb=10^6 bp) and that the size of the reads in this run is 150bp. With these data, how many complete bacterial genomes can you reconstruct?

None, because 18,412 reads of 150bp give a total count of 2,761,800bp (~2Mbp). Even if no read maps to the same region, the amount of base pairs is inferior to the size of a bacterial genome.


Discussion: The quality of MAGs

Can we trust the quality of our bins only with the given information? What else do we want to know about our MAGs to use for further analysis confidently?



completeness is fundamental to know which data you are working with. If the MAG is incomplete, you can hypothesize that if you did not find something, it is because you do not have a complete genome. Genome size and GC content are like genomic fingerprints of taxa, so you can know if you have the taxa you are looking for. Since we are working with the mixed genomes of a community when we try to separate them with binning, we want to know if we were able to separate them correctly. So we need to measure contamination to know if we have only one genome in our bin.

:::


##### quality check



The quality of a MAG is highly dependent on the size of the genome of the species, its abundance in the community and the depth at which we sequenced it. Two important things that can be measured to know its quality are completeness (is the MAG a complete genome?) and if it is contaminated (does the MAG contain only one genome?).

CheckM is an excellent program to see the quality of our MAGs. It measures completeness and contamination by counting marker genes in the MAGs. The lineage workflow that is a part of CheckM places your bins in a reference tree to know to which lineage it corresponds and to use the appropriate marker genes to estimate the quality parameters. Unfortunately, the lineage workflow uses much memory, so it cannot run on our machines, but we can tell CheckM to use marker genes from Bacteria only to spend less memory. This is a less accurate approach, but it can also be advantageous if you want all of your bins analyzed with the same markers.

We will run the taxonomy workflow specifying the use of markers at the domain level, specific for the rank Bacteria, we will specify that our bins are in FASTA format, that they are located in the MAXBIN directory and that we want our output in the CHECKM/ directory.


#### taxonomic assignment

 Questions

    How can I know to which taxa my sequences belong?

Objectives

    Understand how taxonomic assignment works.

    Use Kraken to assign taxonomies to reads and contigs.

    Visualize taxonomic assignations in graphics.


A taxonomic assignment is a process of assigning an Operational Taxonomic Unit (OTU, that is, groups of related individuals) to sequences that can be reads or contigs. Sequences are compared against a database constructed using complete genomes. When a sequence finds a good enough match in the database, it is assigned to the corresponding OTU. The comparison can be made in different ways.
Strategies for taxonomic assignment

There are many programs for doing taxonomic mapping, and almost all of them follow one of the following strategies:

    BLAST: Using BLAST or DIAMOND, these mappers search for the most likely hit for each sequence within a database of genomes (i.e., mapping). This strategy is slow.

    Markers: They look for markers of a database made a priori in the sequences to be classified and assigned the taxonomy depending on the hits obtained.

    K-mers: A genome database is broken into pieces of length k to be able to search for unique pieces by taxonomic group, from a lowest common ancestor (LCA), passing through phylum to species. Then, the algorithm breaks the query sequence (reads/contigs) into pieces of length k, looks for where these are placed within the tree and make the classification with the most probable position.


![Lowest common ancestor assignment example](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-06-01.png)


#### abundance bias

When you do the taxonomic assignment of metagenomes, a key result is the abundance of each taxon or OTU in your sample. The absolute abundance of a taxon is the number of sequences (reads or contigs, depending on what you did) assigned to it. Moreover, its relative abundance is the proportion of sequences assigned to it. It is essential to be aware of the many biases that can skew the abundances along the metagenomics workflow, shown in the figure, and that because of them, we may not be obtaining the actual abundance of the organisms in the sample.

![Abundance biases during a metagenomics protocol](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-06-02.png)

#### taxonomic assignment of the metagenomic reads

Taxonomic assignment of the contigs of a MAG

We now have the taxonomic identity of the reads of the whole metagenome, but we need to know to which taxon our MAGs correspond. For this, we have to make the taxonomic assignment with their contigs instead of its reads because we do not have the reads corresponding to a MAG separated from the reads of the entire sample.

#### Visualization of taxonomic assignment results

After we have the taxonomy assignation, what follows is some visualization of our results. Krona is a hierarchical data visualization software. Krona allows data to be explored with zooming and multi-layered pie charts and supports several bioinformatics tools and raw data formats. To use Krona in our results, let us first go into our taxonomy directory, which contains the pre-calculated Kraken outputs.

#### Exploring taxonomy

 Questions

    How can I use my taxonomic assignment results to analyze?

Objectives

    Comprehend which libraries are required for analysis of the taxonomy of metagenomes.

    Create and manage a Phyloseq object.

we will use RStudio to analyze our microbial samples. You do not have to install anything, you already have an instance on the cloud ready to be used.

Packages like Qiime2, MEGAN, Vegan, or Phyloseq in R allow us to analyze diversity and abundance by manipulating taxonomic assignment data. In this lesson, we will use Phyloseq. In order to do so, we need to generate an abundance matrix from the Kraken output files. One program widely used for this purpose is kraken-biom.

To do this, we could go to our now familiar Bash terminal, but RStudio has an integrated terminal that uses the same language as the one we learned in the Command-line lessons, so let us take advantage of it. Let us open RStudio and go to the Terminal tab in the bottom left panel.

Load required packages

Phyloseq is a library with tools to analyze and plot your metagenomics samplesâ€™ taxonomic assignment and abundance information. Let us install phyloseq (This instruction might not work on specific versions of R) and other libraries required for its execution:

Exploring the taxonomic labels

Let us try to access the data that is stored inside our merged_metagenomes object. Since a phyloseq object is a special object in R, we need to use the operator @ to explore the subsections of data inside merged_metagenomes. If we type merged_metagenomes@, five options are displayed; tax_table and otu_table are the ones we will use. Let us see what is inside our tax_table:

Here we can see that the tax_table inside our phyloseq object stores all the taxonomic labels corresponding to each OTU. Numbers in the row names of the table identify OTUs.

Next, let us get rid of some of the unnecessary characters in the OTUs id and put names to the taxonomic ranks:

To remove unnecessary characters tax_table, which is a matrix, we will use the command substring(). This command helps extract or replace characters in a vector. To use the command, we have to indicate the vector (x) followed by the first element to replace or extract (first) and the last element to be replaced (last). For instance: substring (x, first, last). substring() is a â€œflexibleâ€ command, especially to select characters of different lengths, as in our case. Therefore, it is not necessary to indicate â€œlastâ€, so it will take the last position of the character by default. Since a matrix is an arrangement of vectors, we can use this command. Each character in tax_table is preceded by three spaces occupied by a letter and two underscores, for example: o__Rhodobacterales. In this case, â€œRodobacteralesâ€ starts at position 4 with an R. So, to remove the unnecessary characters, we will use the following code:

Exploring the abundance table

Until now, we have looked at the part of the phyloseq object that stores the information about the taxonomy (at all the possible levels) of each OTU found in our samples. However, there is also a part of the phyloseq object that stores the information about how many sequenced reads corresponding to a certain OTU are in each sample. This table is the otu_table.

#### Exploring diversity

First plunge into diversity

Species diversity, in its simplest definition, is the number of species in a particular area and their relative abundance (evenness). Once we know the taxonomic composition of our metagenomes, we can do diversity analyses. Here we will discuss the two most used diversity metrics, Î± diversity (within one metagenome) and Î² (across metagenomes).

    Î± Diversity: Can be represented only as richness (, i.e., the number of different species in an environment), or it can be measured considering the abundance of the species in the environment as well (i.e., the number of individuals of each species inside the environment). To measure Î±-diversity, we use indexes such as Shannonâ€™s, Simpsonâ€™s, Chao1, etc.


![](https://gwu-libraries.github.io/metagenomics-analysis/fig/03-08-01.png)

Exercise 3: Add metadata to beta diversity visualization

In the following figure, the beta diversity graph we produced earlier has been enriched. Look at the code below and answer:
1) Which instruction colored the samples by their corresponding treatment?
2) What did the instruction geom_text do?
3) What other difference do you notice with our previous graph?
4) Do you see some clustering of the samples according to their treatment?


    Discussion: Indexes of diversity

    Why do you think we need different indexes to asses diversity? What index will you use to assess the impact of rare, low-abundance taxa?

        Solution

        It will be difficult (if not impossible) to take two communities and observe the same distribution of all members. This outcome is because there are a lot of factors affecting these lineages. Some of the environmental factors are temperature, pH, and nutrient concentration. Also, the interactions of these populations, such as competence, inhibition of other populations, and growth speed, are an important driver of variation (biotic factor). A combination of the factors mentioned above, can interact to maintain some populations with low abundance (rare taxa In order to have ways to assess hypotheses regarding which of these processes can be affecting the community, we use all these different indexes. Some emphasize the number of species and other the evenness of the OTUs. To assess the impact of low abundance lineages, one alpha diversity index widely used is the Chao1 index.



    Key Points

        Alpha diversity measures the intra-sample diversity.

        Beta diversity measures the inter-sample diversity.

        Phyloseq includes diversity analyses such as alpha and beta diversity calculation.

#### taxonomic analysis

 Questions

    How can we know which taxa are in our samples?

    How can we compare depth-contrasting samples?

    How can we manipulate our data to deliver a message?

Objectives

    Manipulate data types inside your phyloseq object.

    Extract specific information from taxonomic-assignation data.


With the taxonomic assignment information that we obtained from Kraken, we have measured diversity, and we have visualized the taxa inside each sample with Krona and Pavian, but Phyloseq allows us to make this visualization more flexible and personalized. So now, we will use Phyloseq to make abundance plots of the taxa in our samples.

We will start our exploration at the Phylum level. In order to group all the OTUs that have the same taxonomy at a certain taxonomic rank, we will use the function tax_glom().

Going further, letâ€™s take an exciting lineage and explore it thoroughly

As we have already reviewed, Phyloseq offers many tools to manage and explore data. Letâ€™s take a look at a function we already use but now with guided exploration. The subset_taxa command is used to extract specific lineages from a stated taxonomic level; we have used it to get rid of the reads that do not belong to bacteria with merged_metagenomes <- subset_taxa(merged_metagenomes, Kingdom == "Bacteria").

We will use it now to extract a specific phylum from our data and explore it at a lower taxonomic level: Genus. We will take as an example the phylum Cyanobacteria (indeed, this is a biased and arbitrary decision, but who does not feel attracted to these incredible microorganisms?):


    Exercise 1: Taxa agglomeration

    With the following code, in the dataset with absolute abundances,
    group together the phyla with a small number of reads to have a better visualization of the data. Remember to check the data classes inside your data frame.
    According to the ColorBrewer package it is recommended not to have more than nine different colors in a plot.

    What is the correct order to run the following chunks of code? Compare your graphs with your partnersâ€™.

    Hic Sunt Leones! (Here be Lions!):

    A) absolute_df$Phylum <- as.factor(absolute_df$Phylum)

    B) absolute_plot <- ggplot(data= absolute_df, aes(x=Sample, y=Abundance, fill=Phylum))+
    geom_bar(aes(), stat="identity", position="stack")+
    scale_fill_manual(values = phylum_colors_abs)

    C) absolute_$Phylum[absolute_$Abundance < 300] <- "Minoritary Phyla"

    D) phylum_colors_abs<- colorRampPalette(brewer.pal(8,"Dark2")) (length(levels(absolute_df$Phylum)))

    E) absolute_df$Phylum <- as.character(absolute_df$Phylum)

        Solution

    Exercise 2: Recap of abundance plotting

    Match the chunk of code with its description and put them in the correct order to create a relative abundance plot at the genus level of a particular phylum. ãŒã‚“ã°ã£ã¦! (ganbatte; good luck):
    Description 	Command
    plot the relative abundance at the genus levels. 	plot_proteo
    Convert all the genera with less than 3% abundance into only one label. 	proteo_percentages <- transform_sample_counts(proteo, function(x) >x*100 / sum(x) )
    Make just one row that groups all the observations of the same genus. 	proteo <- subset_taxa(merged_metagenomes, Phylum == "Proteobacteria")
    Create a phyloseq object only with the reads assigned to a certain phylum. 	unique(proteo@tax_table@.Data[,2])
    Show the plot. 	proteo_glom <- tax_glom(proteo_percentages, taxrank = "Genus")
    Transform the phyloseq object to a data frame. 	plot_proteo <- ggplot(data=proteo_df, aes(x=Sample, y=Abundance, fill=Genus))+
      	geom_bar(aes(), stat="identity", position="stack")+
      	scale_fill_manual(values = genus_colors_proteo)
    Convert the Genus column into the factor structure. 	proteo_df$Genus[proteo_df$Abundance < 3] <- "Genera < 3% abund"
    Look at the phyla present in your phyloseq object. 	proteo_df <- psmelt(proteo_glom)
    Convert the abundance counts to relative abundance. 	genus_colors_proteo<- colorRampPalette(brewer.pal(8,"Dark2")) (length(levels(proteo_df$Genus)))
    Make a palette with the appropriate colors for the number of genera. 	proteo_df$Genus <- as.factor(proteo_df$Genus)

        Solution

    Key Points

        Depths and abundances can be visualized using phyloseq.

        The library phyloseq lets you manipulate metagenomic data in a taxonomic specific perspective.


#### {SqueezeMeta}

SqueezeMeta includes multi-metagenome support allowing the co-assembly of related metagenomes and the retrieval of individual metagenome-assembled genomes (MAGs) via binning procedures. Thus, SqueezeMeta features several characteristics:

    Several assembly and co-assembly algorithms and strategies for short and long reads

    Several binning algorithms for the recovery of metagenome-assembled genomes (MAGs)

    Taxonomic annotation, functional annotation and quantification of genes, contigs, and bins

    Support for the annotation and quantification of pre-existing assemblies or collections of genomes

    Support for de-novo metatranscriptome assembly and hybrid metagenomics/metatranscriptomics projects

    Support for the annotation of unassembled shotgun metagenomic reads

    An R package to easily explore your results, including bindings for microeco and phyloseq


## AdquisiciÃ³n de Datos Ocurrencia

```{r}
#| label: data-acquisition-summary

if (exists("pipeline_report") && !is.null(pipeline_report)) {
  data_summary <- pipeline_report$data_acquisition
  
  kable(
    data.frame(
      Fuente = c("OBIS", "GBIF", "Combinados", "Limpios"),
      Registros = c(
        data_summary$obis_records,
        data_summary$gbif_records, 
        data_summary$combined_records,
        data_summary$cleaned_records
      )
    ),
    caption = "Resumen de adquisiciÃ³n de datos de ocurrencia",
    format = "html"
  ) 
  # |>
  # kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}
```

El pipeline integra datos de dos fuentes principales:

- **OBIS (Ocean Biodiversity Information System)**: Base de datos especializada en biodiversidad marina
- **GBIF (Global Biodiversity Information Facility)**: Repositorio global de datos de biodiversidad

## Integration with occurrence data

Link genomic data with the spatial occurrence data you already have

### Limpieza y ValidaciÃ³n de Datos

Utilizamos el paquete `CoordinateCleaner` para implementar un protocolo de limpieza comprehensivo:

```{r}
#| label: data-quality-metrics

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # Calcular mÃ©tricas de calidad
  quality_metrics <- data.frame(
    MÃ©trica = c(
      "Completitud de coordenadas",
      "Completitud taxonÃ³mica", 
      "Completitud temporal",
      "Registros con profundidad"
    ),
    Porcentaje = c(
      round(sum(!is.na(cleaned_occurrences$decimalLatitude) & 
                !is.na(cleaned_occurrences$decimalLongitude)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$scientificName)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$year)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$depth) & cleaned_occurrences$depth > 0, na.rm = TRUE) / nrow(cleaned_occurrences) * 100, 1)
    )
  )
  
  kable(
    quality_metrics,
    caption = "MÃ©tricas de calidad de datos despuÃ©s de la limpieza",
    format = "html"
  ) 
  # |>
  # kable_styling(bootstrap_options = c("striped", "hover"))
}
```

# Resultados

## Control del proyecto con Nix

El ambiente (software, dependencias del sistema, librerÃ­as, etc.) se controla con el archivo '[build_env.R](https://github.com/santi-rios/NereidaPipeline/blob/main/build_env.R)'. el cuÃ¡l facilita la ejecuciÃ³n de distintas tareas controladas por los scripts del directorio '[scripts/](https://github.com/santi-rios/NereidaPipeline/tree/main/scripts/)'

## Control de Pipeline con targets

La pipline es controlada con el archivo '[_targets.R](https://github.com/santi-rios/NereidaPipeline/blob/main/_targets.R)', el cual controla el flujo utilizando diversas funciones escritas en el direcotio '[R/](https://github.com/santi-rios/NereidaPipeline/tree/main/R)':

- '[R/data_acquisition.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_acquisition.R)': Integrating OBIS, GBIF, biomaRt, wikitaxa, and PRISM for comprehensive data collection.
- '[R/data_cleaning.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_cleaning.R)': Standardizing and validating marine occurrence records from biological collection databases.
- '[R/data_visualization.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_visualization.R)'
- '[R/database_integration.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/database_integration.R)': Efficient storage and querying of heterogeneous marine biodiversity data.
- '[R/evolutionary_analysis.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/evolutionary_analysis.R)': phylostratigraphic analysis and evolutionary studies in marine organisms.
- '[R/geospatial_processing.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/geospatial_processing.R)': For handling marine occurrence data with spatial coordinates and environmental layers.
- '[R/metagenomic_analysis.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/metagenomic_analysis.R)'
- '[R/taxonomic_management.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/taxonomic_management.R)': standardized handling of marine biodiversity taxonomic information.

## Sujetos Experimentales

```{r}
#| label: executive-summary
#| results: asis

if (safe_check("pipeline_report")) {
  cat("**Especies analizadas:** ", length(pipeline_report$species_analyzed), "\n\n")
  cat("**Registros procesados:** ", pipeline_report$data_acquisition$cleaned_records, "\n\n")
  cat("**Estado del pipeline:** ", pipeline_report$pipeline_status, "\n\n")
  cat("**Fecha de anÃ¡lisis:** ", format(pipeline_report$pipeline_completion_date, "%d/%m/%Y"), "\n\n")
} else {
  cat("âš ï¸ **Pipeline en progreso o no completado**\n\n")
  cat("Para generar este reporte completo, primero ejecuta:\n\n")
  cat("```r\n")
  cat("library(targets)\n")
  cat("tar_make()\n")
  cat("```\n\n")
}
```

Species Analyzed

```{r species-list}
species_list <- c(
  "Acropora cervicornis",
  "Acropora palmata",
  "Porites astreoides"
)

kable(data.frame(
  "Species" = species_list,
  "Common Name" = c("Staghorn Coral", "Elkhorn Coral", "Mustard Hill Coral")
), caption = "Target Species for Genomic Analysis") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```



## Datos disponibles

```{r}
#| label: data-overview

# Create overview of available data
data_overview <- data.frame(
  Objeto = names(loaded_targets),
  Disponible = sapply(loaded_targets, function(x) if(x) "SÃ­" else "No"),
  Tipo = sapply(names(loaded_targets), function(name) {
    if (loaded_targets[[name]] && exists(name)) {
      obj <- get(name)
      if (is.data.frame(obj)) {
        paste("data.frame (", nrow(obj), " filas)")
      } else if (is.list(obj)) {
        paste("lista (", length(obj), " elementos)")
      } else {
        class(obj)[1]
      }
    } else {
      "No disponible"
    }
  })
)

kable(
  data_overview,
  caption = "Resumen de objetos del pipeline",
  format = "html"
) 
# |>
#   kable_styling(bootstrap_options = c("striped", "hover"))
```


### Datos Ã“micos

Se encontraron estas especies en las bases de datos 

<!-- TODO: poner automÃ¡ticamente las bases de datos) -->

```{r availability-summary}
if (!is.null(genome_availability)) {
  avail_summary <- genome_availability %>%
    group_by(species, database) %>%
    summarise(
      assemblies = n(),
      .groups = "drop"
    ) %>%
    tidyr::pivot_wider(
      names_from = database,
      values_from = assemblies,
      values_fill = 0
    )
  
  kable(avail_summary, 
        caption = "Number of genome assemblies found in each database") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
} else {
  cat("No availability data available.\n")
}
```

Detailed Assembly Information

```{r detailed-availability}
if (!is.null(genome_availability)) {
  genome_availability %>%
    select(species, database, organism_name, assembly_accession, 
           bioproject, biosample, seq_rel_date) %>%
    kable(caption = "Detailed genome assembly information") %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      font_size = 10
    ) %>%
    scroll_box(width = "100%", height = "400px")
}
```

#### Resumen por especies y tipo de datos

Data Completeness Visualization

```{r completeness-viz, fig.width=10, fig.height=6}
if (!is.null(genomic_quality)) {
  # Reshape data for plotting
  plot_data <- genomic_quality %>%
    tidyr::pivot_longer(
      cols = c(proteome_available, cds_available, gff_available),
      names_to = "data_type",
      values_to = "available"
    ) %>%
    mutate(
      data_type = gsub("_available", "", data_type),
      data_type = tools::toTitleCase(data_type)
    )
  
  ggplot(plot_data, aes(x = species, y = data_type, fill = available)) +
    geom_tile(color = "white", size = 1) +
    scale_fill_manual(
      values = c("TRUE" = "#2ecc71", "FALSE" = "#e74c3c"),
      labels = c("TRUE" = "Available", "FALSE" = "Not Available")
    ) +
    labs(
      title = "Genomic Data Retrieval Status",
      subtitle = "Data types retrieved for each species",
      x = "Species",
      y = "Data Type",
      fill = "Status"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 11),
      axis.text.y = element_text(size = 11),
      plot.title = element_text(face = "bold", size = 16),
      plot.subtitle = element_text(size = 12),
      legend.position = "bottom"
    )
}
```

#### Data Size Statistics

```{r data-sizes}
if (!is.null(genomic_quality)) {
  size_data <- genomic_quality %>%
    filter(!is.na(proteome_size)) %>%
    select(species, proteome_size, cds_count)
  
  if (nrow(size_data) > 0) {
    kable(size_data,
          col.names = c("Species", "Proteome Size (MB)", "CDS Count"),
          caption = "Downloaded data statistics",
          digits = 2) %>%
      kable_styling(bootstrap_options = c("striped", "hover"))
  }
}
```

#### Successfully Retrieved Data

```{r successful-retrievals}
if (!is.null(genomic_sequences)) {
  successful <- data.frame(
    species = character(),
    database = character(),
    data_type = character(),
    file_path = character(),
    stringsAsFactors = FALSE
  )
  
  for (sp_name in names(genomic_sequences)) {
    if (sp_name == "availability" || sp_name == "retrieval_date" || 
        sp_name == "database") next
    
    sp_data <- genomic_sequences[[sp_name]]
    
    for (dtype in c("proteome", "cds", "gff")) {
      if (!is.null(sp_data[[dtype]]) && sp_data[[dtype]]$status == "success") {
        successful <- rbind(successful, data.frame(
          species = sp_name,
          database = sp_data$database_used,
          data_type = dtype,
          file_path = basename(sp_data[[dtype]]$file),
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
  if (nrow(successful) > 0) {
    kable(successful,
          caption = "Successfully retrieved genomic data files") %>%
      kable_styling(bootstrap_options = c("striped", "hover")) %>%
      column_spec(3, bold = TRUE, color = "#27ae60")
  } else {
    cat("No data successfully retrieved.\n")
  }
}
```

#### Data Not Found

```{r failed-retrievals}
if (!is.null(genomic_sequences)) {
  failed <- data.frame(
    species = character(),
    data_type = character(),
    reason = character(),
    stringsAsFactors = FALSE
  )
  
  for (sp_name in names(genomic_sequences)) {
    if (sp_name == "availability" || sp_name == "retrieval_date" || 
        sp_name == "database") next
    
    sp_data <- genomic_sequences[[sp_name]]
    
    # Check if species was not available at all
    if (!is.null(sp_data$status) && sp_data$status == "not_available") {
      failed <- rbind(failed, data.frame(
        species = sp_name,
        data_type = "All",
        reason = "Species not found in any database",
        stringsAsFactors = FALSE
      ))
      next
    }
    
    for (dtype in c("proteome", "cds", "gff")) {
      if (!is.null(sp_data[[dtype]]) && sp_data[[dtype]]$status == "error") {
        failed <- rbind(failed, data.frame(
          species = sp_name,
          data_type = dtype,
          reason = sp_data[[dtype]]$message,
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
  if (nrow(failed) > 0) {
    kable(failed,
          caption = "Data retrieval failures") %>%
      kable_styling(bootstrap_options = c("striped", "hover")) %>%
      column_spec(2, color = "#e74c3c")
  } else {
    cat("âœ“ All requested data successfully retrieved!\n")
  }
}
```


## AnÃ¡lisis de Datos de Ocurrencia

```{r}
#| label: occurrence-analysis

if (safe_check("cleaned_occurrences")) {
  
  # Basic summary
  cat("### Resumen de Datos Limpios\n\n")
  cat("- **Total de registros:** ", nrow(cleaned_occurrences), "\n")
  cat("- **Especies Ãºnicas:** ", length(unique(cleaned_occurrences$scientificName)), "\n")
  cat("- **Rango temporal:** ", min(cleaned_occurrences$year, na.rm = TRUE), "-", 
      max(cleaned_occurrences$year, na.rm = TRUE), "\n\n")
  
  # Species summary
  species_summary <- cleaned_occurrences |>
    group_by(scientificName) |>
    summarise(
      Registros = n(),
      `AÃ±os Ãºnicos` = length(unique(year[!is.na(year)])),
      `Lat mÃ­n` = round(min(decimalLatitude, na.rm = TRUE), 2),
      `Lat mÃ¡x` = round(max(decimalLatitude, na.rm = TRUE), 2),
      `Lon mÃ­n` = round(min(decimalLongitude, na.rm = TRUE), 2),
      `Lon mÃ¡x` = round(max(decimalLongitude, na.rm = TRUE), 2),
      .groups = "drop"
    )
  
  kable(
    species_summary,
    caption = "Resumen por especie",
    format = "html"
  ) 
  # |>
  #   kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
  
} else {
  cat("âš ï¸ Datos de ocurrencia no disponibles. Ejecuta el pipeline primero.\n")
}
```


Summary by Species and Data Type

```{r retrieval-summary}
if (!is.null(genomic_quality)) {
  kable(genomic_quality, 
        caption = "Genomic data retrieval summary",
        digits = 2) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
    column_spec(2, bold = TRUE) %>%
    column_spec(3, color = ifelse(genomic_quality$data_completeness == 100, 
                                   "green", "orange"))
}
```


## DistribuciÃ³n Espacial de Especies

```{r}
#| label: species-distribution
#| fig-cap: "DistribuciÃ³n espacial de registros de ocurrencia por especie"

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # Filtrar coordenadas vÃ¡lidas
  valid_coords <- cleaned_occurrences |>
    filter(
      !is.na(decimalLatitude), 
      !is.na(decimalLongitude),
      !is.na(scientificName),
      decimalLatitude >= -90, decimalLatitude <= 90,
      decimalLongitude >= -180, decimalLongitude <= 180
    )
  
  if (nrow(valid_coords) > 0) {
    # Crear mapa interactivo
    species_colors <- c("#E31A1C", "#1F78B4", "#33A02C", "#FF7F00", "#6A3D9A")
    species_list <- unique(valid_coords$scientificName)
    
    m <- leaflet(valid_coords) |>
      addProviderTiles(providers$CartoDB.Positron) |>
      setView(
        lng = mean(valid_coords$decimalLongitude), 
        lat = mean(valid_coords$decimalLatitude), 
        zoom = 6
      )
    
    # AÃ±adir puntos por especie
    for (i in seq_along(species_list)) {
      species_data <- valid_coords |> filter(scientificName == species_list[i])
      
      m <- m |>
        addCircleMarkers(
          data = species_data,
          lng = ~decimalLongitude,
          lat = ~decimalLatitude,
          color = species_colors[i],
          radius = 3,
          stroke = FALSE,
          fillOpacity = 0.7,
          group = species_list[i],
          popup = ~paste0(
            "<b>Especie:</b> ", scientificName, "<br>",
            "<b>Fuente:</b> ", data_source, "<br>",
            "<b>AÃ±o:</b> ", year
          )
        )
    }
    
    # AÃ±adir control de capas
    m <- m |>
      addLayersControl(
        overlayGroups = species_list,
        options = layersControlOptions(collapsed = FALSE)
      )
    
    m
  }
}
```

## AnÃ¡lisis de Idoneidad de HÃ¡bitat

```{r}
#| label: fig-habitat-suitability
#| fig-cap: "Mapa de idoneidad de hÃ¡bitat para las especies analizadas"

if (exists("habitat_models") && !is.null(habitat_models) && length(habitat_models) > 0) {
  
  # Crear visualizaciÃ³n de idoneidad por especies usando solo datos serializables
  species_with_models <- names(habitat_models)[!sapply(habitat_models, is.null)]
  
  if (length(species_with_models) > 0) {
    
    # Crear grÃ¡fico de barras usando informaciÃ³n del modelo sin acceder a SpatRaster
    suitability_data <- purrr::map_dfr(species_with_models, function(species) {
      model <- habitat_models[[species]]
      
      # Verificar que el modelo existe y tiene datos del modelo
      if (!is.null(model) && !is.null(model$model_data)) {
        
        # Calcular estadÃ­sticas bÃ¡sicas del modelo sin acceder a predicciones raster
        presence_points <- sum(model$model_data$presence == 1, na.rm = TRUE)
        background_points <- sum(model$model_data$presence == 0, na.rm = TRUE)
        
        # Calcular probabilidades del modelo en los puntos de datos
        if (!is.null(model$model) && inherits(model$model, c("lm", "glm"))) {
          predicted_probs <- predict(model$model, type = "response")
          
          data.frame(
            Especie = gsub("_", " ", species),
            `Puntos de presencia` = presence_points,
            `Puntos de fondo` = background_points,
            `Probabilidad media` = mean(predicted_probs, na.rm = TRUE),
            `Probabilidad mÃ¡xima` = max(predicted_probs, na.rm = TRUE),
            `AIC del modelo` = AIC(model$model)
          )
        } else {
          # Si no hay modelo vÃ¡lido, usar solo informaciÃ³n bÃ¡sica
          data.frame(
            Especie = gsub("_", " ", species),
            `Puntos de presencia` = presence_points,
            `Puntos de fondo` = background_points,
            `Probabilidad media` = NA,
            `Probabilidad mÃ¡xima` = NA,
            `AIC del modelo` = NA
          )
        }
      } else {
        NULL
      }
    })
    
    if (!is.null(suitability_data) && nrow(suitability_data) > 0) {
      
      # Crear tabla resumen
      kable(
        suitability_data,
        caption = "Resumen de modelos de idoneidad de hÃ¡bitat por especie",
        format = "html",
        digits = 3
      ) 
      # |>
      # kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
      
      # Crear grÃ¡fico de barras para mÃ©tricas del modelo
      if (any(!is.na(suitability_data$`Probabilidad media`))) {
        
        # Preparar datos para el grÃ¡fico
        plot_data <- suitability_data %>%
          select(Especie, `Puntos de presencia`, `Probabilidad media`, `AIC del modelo`) %>%
          pivot_longer(cols = -Especie, names_to = "MÃ©trica", values_to = "Valor") %>%
          filter(!is.na(Valor))
        
        if (nrow(plot_data) > 0) {
          p <- plot_data %>%
            ggplot(aes(x = Especie, y = Valor, fill = Especie)) +
            geom_col(alpha = 0.8) +
            facet_wrap(~MÃ©trica, scales = "free_y", ncol = 2) +
            scale_fill_viridis_d() +
            labs(
              title = "MÃ©tricas de Modelos de Idoneidad de HÃ¡bitat",
              x = "Especie",
              y = "Valor"
            ) +
            theme_minimal() +
            theme(
              axis.text.x = element_text(angle = 45, hjust = 1),
              legend.position = "none"
            )
          
          print(p)
        }
      }
      
    } else {
      cat("No se pudieron calcular mÃ©tricas de idoneidad debido a problemas en los datos del modelo.\n")
    }
    
  } else {
    cat("No hay modelos de hÃ¡bitat disponibles para visualizaciÃ³n.\n")
  }
  
} else {
  cat("Los modelos de idoneidad de hÃ¡bitat no estÃ¡n disponibles.\n")
}
```

## AnÃ¡lisis Espacial Comprehensivo

```{r}
#| label: spatial-analysis-results

if (exists("spatial_analysis") && !is.null(spatial_analysis)) {
  
  # Extraer estadÃ­sticas espaciales
  spatial_stats <- spatial_analysis$spatial_statistics
  
  if (!is.null(spatial_stats)) {
    
    # Crear tabla de mÃ©tricas espaciales
    spatial_metrics <- data.frame(
      MÃ©trica = c(
        "NÃºmero de ocurrencias",
        "Rango latitudinal (Â°)",
        "Rango longitudinal (Â°)",
        "Centroide latitudinal (Â°)",
        "Centroide longitudinal (Â°)",
        "Ãrea de ocupaciÃ³n (kmÂ²)",
        "NÃºmero de celdas de grilla"
      ),
      Valor = c(
        spatial_stats$n_occurrences,
        round(diff(spatial_stats$lat_range), 2),
        round(diff(spatial_stats$lon_range), 2),
        round(spatial_stats$centroid_lat, 3),
        round(spatial_stats$centroid_lon, 3),
        spatial_stats$area_of_occupancy_km2,
        spatial_stats$n_grid_cells
      )
    )
    
    kable(
      spatial_metrics,
      caption = "MÃ©tricas espaciales del anÃ¡lisis de distribuciÃ³n",
      format = "html"
    ) 
    # |>
    # kable_styling(bootstrap_options = c("striped", "hover"))
  }
}
```

## Nicho Ambiental

```{r}
#| label: environmental-niche-analysis
#| fig-cap: "AnÃ¡lisis del nicho ambiental de las especies marinas"

if (exists("spatial_analysis") && !is.null(spatial_analysis$environmental_niche)) {
  
  env_niche <- spatial_analysis$environmental_niche
  
  # Convertir a formato largo para visualizaciÃ³n
  niche_data <- purrr::map_dfr(names(env_niche), function(var) {
    var_stats <- env_niche[[var]]
    
    data.frame(
      Variable = case_when(
        var == "depth_m" ~ "Profundidad (m)",
        var == "sst_celsius" ~ "Temperatura (Â°C)",
        var == "salinity_psu" ~ "Salinidad (PSU)",
        var == "chlorophyll_mg_m3" ~ "Clorofila-a (mg/mÂ³)",
        TRUE ~ var
      ),
      Media = var_stats$mean,
      Desviacion_estandar = var_stats$sd,  # <-- SIN ESPACIOS
      Minimo = var_stats$min,
      Maximo = var_stats$max,
      Mediana = var_stats$median,
      Q25 = var_stats$q25,
      Q75 = var_stats$q75
    )
  })
  
  if (nrow(niche_data) > 0) {
    # Crear nombres mÃ¡s amigables para la tabla
    niche_data_display <- niche_data
    names(niche_data_display) <- c(
      "Variable", "Media", "DesviaciÃ³n estÃ¡ndar", 
      "MÃ­nimo", "MÃ¡ximo", "Mediana", "Q25", "Q75"
    )
    
    # Tabla de estadÃ­sticas
    kable(
      niche_data_display,
      caption = "Preferencias ambientales de las especies analizadas",
      format = "html",
      digits = 2
    ) 
    # |>
    # kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
    
    # GrÃ¡fico usando los nombres sin espacios
    niche_long <- niche_data |>
      select(Variable, Media, Desviacion_estandar, Minimo, Maximo, Mediana) |>  # <-- NOMBRES SIN ESPACIOS
      pivot_longer(cols = -Variable, names_to = "Estadistica", values_to = "Valor")
    
    if (nrow(niche_long) > 0) {
      p_niche <- niche_long |>
        filter(Estadistica %in% c("Minimo", "Mediana", "Maximo")) |>  # <-- SIMPLIFICAR
        ggplot(aes(x = Variable, y = Valor, fill = Estadistica)) +
        geom_col(position = "dodge", alpha = 0.8) +
        scale_fill_viridis_d() +
        labs(
          title = "DistribuciÃ³n de Preferencias Ambientales",
          x = "Variable Ambiental",
          y = "Valor",
          fill = "EstadÃ­stica"
        ) +
        theme_minimal() +
        theme(
          axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom"
        ) +
        facet_wrap(~Variable, scales = "free", ncol = 2)
      
      print(p_niche)
    }
    
    cat("\n\n*Nota: Las estadÃ­sticas ambientales muestran las condiciones preferidas por las especies en sus hÃ¡bitats naturales.*\n\n")
  }
  
} else {
  cat("Los datos de nicho ambiental no estÃ¡n disponibles.\n\n")
  cat("Esto puede ocurrir si:\n")
  cat("- El anÃ¡lisis espacial no se completÃ³ correctamente\n")
  cat("- No hay suficientes datos ambientales\n")
  cat("- Los objetos SpatRaster no se cargaron correctamente\n\n")
}
```

## Base de Datos Integrada

```{r}
#| label: database-summary

if (exists("data_summaries") && !is.null(data_summaries)) {
  
  # Resumen de la base de datos
  if (!is.null(data_summaries$species)) {
    species_summary <- data_summaries$species
    
    if (nrow(species_summary) > 0) {
      
      # Crear tabla interactiva de especies
      datatable(
        species_summary,
        caption = "Resumen de especies en la base de datos integrada",
        options = list(
          pageLength = 10,
          scrollX = TRUE,
          dom = 'Bfrtip',
          buttons = c('copy', 'csv', 'excel')
        ),
        filter = 'top',
        rownames = FALSE
      ) |>
      formatRound(columns = c('n_records'), digits = 0)
    }
  }
}
```

La base de datos integrada utiliza **DuckDB** como backend NoSQL, permitiendo:

- **Almacenamiento eficiente** de datos heterogÃ©neos (ocurrencias, taxonomÃ­a, ambiente)
- **Consultas flexibles** usando sintaxis SQL y JSON
- **ExportaciÃ³n mÃºltiple** a formatos CSV, JSON, y Parquet
- **Escalabilidad** para grandes volÃºmenes de datos

### Implicaciones para la ConservaciÃ³n

#### Especies Prioritarias

```{r}
#| label: conservation-priorities

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # AnÃ¡lisis de prioridades de conservaciÃ³n CON DATOS TEMPORALES
  conservation_analysis <- cleaned_occurrences |>
    # ConversiÃ³n simple y segura de year
    mutate(
      year_numeric = as.numeric(year)  # Â¡AsÃ­ de simple!
    ) |>
    group_by(scientificName) |>
    summarise(
      `Registros totales` = n(),
      `Rango geogrÃ¡fico` = round(
        sqrt((max(decimalLatitude, na.rm = TRUE) - min(decimalLatitude, na.rm = TRUE))^2 + 
             (max(decimalLongitude, na.rm = TRUE) - min(decimalLongitude, na.rm = TRUE))^2), 2
      ),
      `AÃ±os de registro` = max(year_numeric, na.rm = TRUE) - min(year_numeric, na.rm = TRUE),
      `AÃ±o mÃ¡s reciente` = max(year_numeric, na.rm = TRUE),
      `AÃ±o mÃ¡s antiguo` = min(year_numeric, na.rm = TRUE),
      `Profundidad media (m)` = round(mean(depth, na.rm = TRUE), 1),
      .groups = "drop"
    ) |>
    mutate(
      `Prioridad conservaciÃ³n` = case_when(
        `Registros totales` < 50 & `Rango geogrÃ¡fico` < 2 ~ "ALTA",
        `Registros totales` < 100 & `Rango geogrÃ¡fico` < 5 ~ "MEDIA",
        `AÃ±o mÃ¡s reciente` < 2015 & `Registros totales` < 200 ~ "MEDIA",  # Especies sin registros recientes
        TRUE ~ "BAJA"
      )
    ) |>
    arrange(desc(`Prioridad conservaciÃ³n`), desc(`AÃ±o mÃ¡s reciente`))
  
  # Mostrar la tabla
  if (nrow(conservation_analysis) > 0) {
    kable(
      conservation_analysis,
      caption = "AnÃ¡lisis de prioridades de conservaciÃ³n basado en datos de ocurrencia",
      format = "html"
    )
    
    # Agregar anÃ¡lisis temporal adicional
    cat("\n\n### AnÃ¡lisis Temporal\n\n")
    
    temporal_summary <- cleaned_occurrences |>
      mutate(year_numeric = as.numeric(year)) |>
      summarise(
        `PerÃ­odo de estudio` = paste(min(year_numeric, na.rm = TRUE), "-", max(year_numeric, na.rm = TRUE)),
        `AÃ±os con datos` = length(unique(year_numeric)),
        `Registros por dÃ©cada` = paste(
          "1970s:", sum(year_numeric >= 1970 & year_numeric < 1980, na.rm = TRUE), "|",
          "1980s:", sum(year_numeric >= 1980 & year_numeric < 1990, na.rm = TRUE), "|", 
          "1990s:", sum(year_numeric >= 1990 & year_numeric < 2000, na.rm = TRUE), "|",
          "2000s:", sum(year_numeric >= 2000 & year_numeric < 2010, na.rm = TRUE), "|",
          "2010s:", sum(year_numeric >= 2010 & year_numeric < 2020, na.rm = TRUE), "|",
          "2020s:", sum(year_numeric >= 2020, na.rm = TRUE)
        )
      )
    
    cat("**Resumen temporal del dataset:**\n\n")
    cat("- PerÃ­odo:", temporal_summary$`PerÃ­odo de estudio`, "\n")
    cat("- AÃ±os Ãºnicos con datos:", temporal_summary$`AÃ±os con datos`, "\n") 
    cat("- DistribuciÃ³n por dÃ©cadas:", temporal_summary$`Registros por dÃ©cada`, "\n\n")
    
  } else {
    cat("No se pudieron calcular las prioridades de conservaciÃ³n debido a problemas en los datos.\n")
  }
  
} else {
  cat("âš ï¸ Datos de ocurrencia no disponibles para anÃ¡lisis de conservaciÃ³n.\n")
}
```


## Manejo de git

Gracias a la integraciÃ³n de los flujos de trabajo (*workflows*) de github (controlados por los archivos en el directorio '[.github/workflows](https://github.com/santi-rios/NereidaPipeline/tree/main/.github/workflows)'), se autamitza y simplifica el flujo de trabajo.

The workflows directory contains GitHub Actions workflows that automate tasks when you push your code to GitHub.

Example:

ci.yml (Continuous Integration)

- This file likely defines an automated process that runs whenever you push code or create a pull request.
- Checks that your code builds correctly
- Runs any tests you've defined
- Validates that your R packages can be installed
- May check code style/quality

::: {.callout-note  title="Detalles tÃ©xnicos de workflows"}

What happens when you push to GitHub?
Once these files are in your repository:

GitHub automatically recognizes and activates these workflows
They'll run according to their defined triggers (e.g., on push, schedule, etc.)
You'll see workflow runs in the "Actions" tab of your repository
The workflows execute on GitHub's servers
You'll receive notifications if workflows fail
Any artifacts or outputs will be stored according to the workflow configuration

:::



## Error handling

[@noble2009]: During the development of a complicated set of experiments, you will introduce errors into your code. Such errors are inevitable, but they are particularly problematic if they are difficult to track down or, worse, if you donâ€™t know about them and hence draw invalid conclusions from your experiment. Here are three suggestions for error handling. Write robust code to detect errors.

::: {.callout-note}
'direnv: error /home/santi/Projects/NereidaPipeline/.envrc is blocked. Run `direnv allow` to approve its content'
:::





# DiscusiÃ³n

La implementaciÃ³n de avanzadas herramientas bioinformÃ¡ticas y la gestiÃ³n eficiente de pipelines han revolucionado el estudio de la biodiversidad y la evoluciÃ³n de la biota marina. En este marco, el proyecto propone una aplicaciÃ³n de R que hace uso de Nix por medio del paquete R {rix}, convergiendo en una soluciÃ³n Ã³ptima para el manejo de diversas funciones, incluyendo el procesamiento de datos, la gestiÃ³n de cÃ³digo y la integraciÃ³n de informaciÃ³n en repositorios.

Gracias al uso de Nix, un sistema de paquetes de cÃ³digo abierto que adopta una nueva manera de manejar las dependencias, proporcionando un entorno de trabajo coherentemente reproducible. Asegura que los paquetes se construyen e instalan de manera aislada, lo que permite un control de versiones y una gestiÃ³n de las dependencias finamente afinada (Dolstra, E., 2006). AsÃ­, Nix se impone como una herramienta fundamental cambio de contextos acadÃ©micos y empresariales, permitiendo la creaciÃ³n de entornos de trabajo estables y reproducibles.

## Control de Pipeline Avanzado

### OrquestaciÃ³n del Flujo de Trabajo con `Make`

El uso de `Make` como un "orquestador" de alto nivel simplifica la interacciÃ³n y estandariza los procedimientos.

[@noble2009]: Este influyente artÃ­culo describe las mejores prÃ¡cticas para organizar proyectos de biologÃ­a computacional. Recomienda explÃ­citamente el uso de `Make` para automatizar la pipeline, desde la descarga de datos hasta la generaciÃ³n de figuras finales. Citarlo posiciona su uso de `Makefile` como una prÃ¡ctica establecida y recomendada para mantener la organizaciÃ³n, la claridad y la automatizaciÃ³n en proyectos complejos, lo cual es vital en estudios integrativos de biodiversidad marina.


::: {.callout-note  title="Makefile"}

Makefile funciona como un "protocolo de laboratorio" ejecutable. Su propÃ³sito principal es automatizar y estandarizar las tareas repetitivas, encapsulando comandos complejos en alias simples y declarativos.

Makefile no es solo un archivo, sino la interfaz de control principal. En lugar de recordar una serie de scripts y sus argumentos, usted y sus colaboradores solo necesitan interactuar con comandos semÃ¡nticos como make test o make regenerate.

AsÃ­ es como su Makefile dirige el flujo:

    make regenerate: Este es el comando fundamental para la reproducibilidad. No ejecuta rix directamente, sino que delega la tarea al script ./regenerate.sh. Este script, a su vez, invoca su archivo build_env.R, donde la funciÃ³n rix::rix() traduce su lista de paquetes de R y dependencias del sistema en un archivo default.nix. Este es el plano exacto de su ambiente.

    make test: Este comando invoca ./test_environment.sh, un paso de validaciÃ³n crucial. El script verifica que el ambiente de Nix, una vez construido, contenga todos los paquetes que su pipeline de {targets} declara necesitar en _targets.R. Esto cierra el cÃ­rculo, asegurando que el ambiente definido coincide con el ambiente requerido.

    make update: ActÃºa como un meta-comando que ejecuta una secuencia lÃ³gica de tareas a travÃ©s del script update_workflow.sh, probablemente combinando la regeneraciÃ³n y la prueba del ambiente.

    make clean: Mantiene la higiene del proyecto, eliminando artefactos y resultados intermedios para garantizar que la prÃ³xima ejecuciÃ³n comience desde un estado conocido y limpio.

validaciÃ³n crucial. El script verifica que el ambiente de Nix, una vez construido, contenga todos los paquetes que su pipeline de {targets} declara necesitar en _targets.R. Esto cierra el cÃ­rculo, asegurando que el ambiente definido coincide con el ambiente requerido.

    make update: ActÃºa como un meta-comando que ejecuta una secuencia lÃ³gica de tareas a travÃ©s del script update_workflow.sh, probablemente combinando la regeneraciÃ³n y la prueba del ambiente.

    make clean: Mantiene la higiene del proyecto, eliminando artefactos y resultados intermedios para garantizar que la prÃ³xima ejecuciÃ³n comience desde un estado conocido y limpio.

:::

### Control de versiones automatizado con workflows

#### Aggregation subsystem

[@boyle2009]: **aggregation subsystem*: One of the aims of the computing advances over the last few years has involved the concept of run time aggregation. This approach is epitomized by the semantic web, where people can mash information from a variety of data sources into a single graph. When an analysis run has finished, the observing aggregation system can trigger an indexing operation on the results. The indexing is controlled using the properties that are associated with the output of the analysis run (typically the properties related to the rows or columns that are to be indexed).

> For this last point... we could try a quarto dashboard to summirize the data and make the github actions trigger to continuously (or at query) deploys updated info to the web.

::: {.callout-note}

Razones clave para preferir Nix + rix en proyectos cientÃ­ficos y pipelines

- Reproducibilidad completa: Nix describe exactamente quÃ© se construye y cuÃ¡les binarios y bibliotecas del sistema se usan; rix adapta este enfoque al ecosistema R, permitiendo reconstrucciones idÃ©nticas en diferentes mÃ¡quinas y CI.
- GestiÃ³n unificada de R y dependencias del sistema: muchas herramientas R necesitan bibliotecas C/Fortran. Nix las gestiona junto con los paquetes R, evitando fallos invisibles por falta de libs nativas.
- Declaratividad, versionado y trazabilidad: los archivos Nix (o los artefactos que genera rix) son documentos versionables que sirven como metadatos exactos del entorno utilizado para un anÃ¡lisis â€” esencial para reproducibilidad y revisiÃ³n acadÃ©mica.
- Aislamiento hermÃ©tico: elimina efectos de entornos previos o variaciones en el usuario, reduciendo errores â€œworks on machine Xâ€.
- IntegraciÃ³n con CI y archivado a largo plazo: cachÃ©s binarios y la capacidad de reconstruir entornos permiten validar pipelines en CI y archivar entornos reproducibles junto a publicaciones.
- Interoperabilidad: aunque renv y conda pueden usarse junto a Nix, emplear Nix como capa superior unifica la gestiÃ³n y reduce la complejidad operativa.

Limitaciones y balance prÃ¡ctico

- Curva de aprendizaje: Nix requiere tiempo para dominar sus conceptos. rix reduce fricciÃ³n para usuarios R, pero la adopciÃ³n institucional puede exigir formaciÃ³n.
- Uso de disco y recursos: la store de Nix puede ocupar mÃ¡s espacio, aunque la deduplicaciÃ³n y cachÃ© binario compensa en entornos compartidos.
- Windows: Nix funciona mejor en Linux/macOS; en Windows suele requerir WSL o contenedores (esto estÃ¡ mejorando con la comunidad).

Recomendaciones prÃ¡cticas

- Para investigaciÃ³n reproducible, producciÃ³n de pipelines y CI: adoptar Nix + rix como estÃ¡ndar de entorno. Mantener la expresiÃ³n Nix en el repositorio junto con el cÃ³digo y los datos procesables.
- Para desarrollo rÃ¡pido o enseÃ±anza: combinar renv (para control fino de paquetes R durante el desarrollo) dentro de un entorno Nix que garantice las dependencias del sistema. renv puede convivir con Nix: renv controla versiones R; Nix asegura libs nativas.
- Para entornos multi-lenguaje con uso intensivo de paquetes binarios (Python + R + herramientas C): Nix ofrece la soluciÃ³n mÃ¡s coherente y reproducible frente a la mezcla de conda + gestores del sistema.

:::


#### Base de datos con git-lfs

Using Git LFS (Large File Storage) for a DuckDB file is a viable option if the file is large and frequently modified, and if you require version control for its changes.
Reasons to consider Git LFS for DuckDB files:
DuckDB files are often binary: Git is optimized for text-based files and handles binary files inefficiently, especially when they are large and undergo frequent changes.
Version control of large files: Git LFS allows you to track changes to large DuckDB files without bloating the main Git repository. Instead of storing the full file in the repository history for every change, Git LFS stores pointers to the file and manages the actual file content on a separate LFS server.
Faster cloning and fetching: When using Git LFS, only the pointers are downloaded during cloning and fetching, significantly reducing the time and bandwidth required compared to downloading the entire history of large binary files.
Reduced repository size: The main Git repository remains smaller and more manageable, as the large DuckDB file content is stored externally.

PERSPECTIVAS DE GIT-LFS

Considerations and alternatives:
LFS server setup: You need a Git LFS server to store the actual file content. This is often provided by platforms like GitHub, GitLab, or Bitbucket, but you may need to manage your own if using a self-hosted Git solution.
Quota limitations: Cloud-based Git LFS services often have storage and bandwidth quotas, which might be a factor if your DuckDB files are extremely large or frequently accessed by many users.
DVC (Data Version Control): For more complex data management needs, especially in data science and machine learning contexts, DVC offers a more specialized solution for versioning data and models, including large files.
External storage and releases: If your DuckDB files are exceptionally large or not truly part of your codebase's direct version control, consider storing them externally (e.g., cloud storage, shared drives) and linking to them or managing them through release processes.
In summary: If you have large DuckDB files that require version control within your Git repository and are subject to frequent modifications, Git LFS provides an effective way to manage them without the performance and storage overhead of traditional Git. Evaluate your specific needs and consider potential alternatives like DVC for more advanced data versioning requirements.


## ConservaciÃ³n y EvolucÃ³n

### Recomendaciones de Manejo

BasÃ¡ndose en los resultados del anÃ¡lisis, se proponen las siguientes estrategias de conservaciÃ³n:

#### 1. **ProtecciÃ³n de HÃ¡bitats CrÃ­ticos**
- Establecer Ã¡reas marinas protegidas en zonas de alta idoneidad
- Monitorear cambios en las condiciones ambientales clave
- Implementar medidas de mitigaciÃ³n contra el cambio climÃ¡tico

#### 2. **Monitoreo y Vigilancia**
- Desarrollar programas de monitoreo a largo plazo
- Utilizar tecnologÃ­as de detecciÃ³n temprana
- Capacitar a comunidades locales en identificaciÃ³n de especies

#### 3. **RestauraciÃ³n EcolÃ³gica**
- Identificar Ã¡reas histÃ³ricamente importantes pero actualmente degradadas
- Implementar programas de restauraciÃ³n basados en ciencia
- Evaluar el Ã©xito de las intervenciones de restauraciÃ³n


#### AnÃ¡lisis Evolutivo y AdaptaciÃ³n

```{r}
#| label: evolutionary-insights

# Mostrar insights evolutivos si estÃ¡n disponibles
cat("### Perspectivas Evolutivas\n\n")
cat("El anÃ¡lisis filoestratigrÃ¡fico revela patrones importantes sobre la evoluciÃ³n y adaptaciÃ³n de las especies marinas:\n\n")

cat("- **Genes antiguos** conservados indican funciones esenciales para la supervivencia marina\n")
cat("- **Genes de origen reciente** pueden representar adaptaciones especÃ­ficas al ambiente marino\n") 
cat("- **Patrones de expresiÃ³n** durante el desarrollo sugieren estrategias evolutivas de supervivencia\n\n")

cat("Estas perspectivas son cruciales para:\n\n")
cat("1. **Programas de crÃ­a selectiva** para aumentar la resistencia climÃ¡tica\n")
cat("2. **Estrategias de restauraciÃ³n** basadas en diversidad genÃ©tica\n")
cat("3. **PredicciÃ³n de respuestas** a cambios ambientales futuros\n\n")
```

## Perspectivas y limitaciones

Potential Applications

- Personalized Medicine: Identifying genetic predispositions to diseases and tailoring treatments based on an individual's genetic makeup. 
- Cancer Research: Identifying mutations in tumors to guide cancer treatment strategies and monitor treatment responses. 
- Infectious Disease: Tracking outbreaks by sequencing pathogens and understanding their genetic relationships. 
- Drug Discovery: Uncovering new drug targets and identifying potential drug repurposing opportunities by understanding disease-causing genetic mechanisms. 

Ãrea de Mejora: La principal fricciÃ³n a futuro es el acoplamiento manual entre _targets.R y build_env.R. Actualmente, si usted aÃ±ade library(nuevo_paquete) en su pipeline _targets.R, debe recordar manualmente aÃ±adir "nuevo_paquete" al vector targets_packages en build_env.R. A medida que el proyecto crezca y colaboren mÃ¡s personas, es casi seguro que este paso se olvidarÃ¡, lo que provocarÃ¡ fallos en las pruebas (make test) y frustraciÃ³n.

Para que este flujo de trabajo sea verdaderamente a prueba de futuro, el siguiente paso lÃ³gico es automatizar la sincronizaciÃ³n de paquetes.

Se podrÃ­a modificar el script regenerate.sh (o crear uno nuevo) para que, antes de ejecutar build_env.R, primero analice el archivo _targets.R en busca de todas las llamadas library(...). Luego, puede pasar esa lista de paquetes como un argumento a su script de R o escribirla en un archivo temporal que build_env.R pueda leer.

Esto desacoplarÃ­a completamente la definiciÃ³n del ambiente de la pipeline, adhiriÃ©ndose al principio de "Fuente Ãšnica de Verdad" (Single Source of Truth). Su pipeline en _targets.R se convierte en la Ãºnica fuente que define quÃ© paquetes se necesitan, y el resto del sistema reacciona automÃ¡ticamente.

Sin embargo, la arquitectura actual es excelente, avanzada y se adelanta a la mayorÃ­a de los flujos de trabajo acadÃ©micos. Es altamente reproducible y fÃ¡cil de usar. Al implementar la sincronizaciÃ³n automÃ¡tica de paquetes, lo convertirÃ¡ en un sistema prÃ¡cticamente infalible y listo para escalar a cualquier complejidad.

1. **Sesgos de muestreo**: Los datos de ocurrencia pueden estar sesgados hacia Ã¡reas de fÃ¡cil acceso
2. **ResoluciÃ³n temporal**: La variabilidad interanual y estacional no estÃ¡ completamente capturada
3. **Variables ambientales**: Limitadas a las disponibles en bases de datos globales
4. **ValidaciÃ³n de modelos**: Se requiere validaciÃ³n independiente con datos de campo


### Direcciones Futuras

```{r}
#| label: future-directions

future_research <- data.frame(
  `Ãrea de investigaciÃ³n` = c(
    "IntegraciÃ³n de datos genÃ³micos",
    "Modelado de cambio climÃ¡tico", 
    "AnÃ¡lisis de conectividad",
    "Monitoreo en tiempo real",
    "Inteligencia artificial"
  ),
  `DescripciÃ³n` = c(
    "Incorporar datos de secuenciaciÃ³n para anÃ¡lisis poblacionales",
    "Proyectar distribuciones futuras bajo escenarios climÃ¡ticos",
    "Analizar flujo genÃ©tico y dispersiÃ³n larval",
    "Implementar sensores IoT para datos continuos",
    "Desarrollar modelos de aprendizaje automÃ¡tico avanzados"
  ),
  `Prioridad` = c("Alta", "Alta", "Media", "Media", "Baja")
)

kable(
  future_research,
  caption = "Direcciones prioritarias para investigaciÃ³n futura",
  format = "html"
) |>
kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```


## Conclusiones

El pipeline integrado de anÃ¡lisis de biodiversidad marina ha demostrado ser una herramienta poderosa para:

1. **Integrar mÃºltiples fuentes de datos** de manera eficiente y reproducible
2. **Validar y limpiar datos** usando estÃ¡ndares internacionales
3. **Generar modelos predictivos** de distribuciÃ³n de especies
4. **Identificar prioridades de conservaciÃ³n** basadas en evidencia cientÃ­fica
5. **Proporcionar insights evolutivos** relevantes para la conservaciÃ³n

Los resultados resaltan la importancia de enfoques integrados que combinen datos de ocurrencia, variables ambientales, y anÃ¡lisis evolutivos para informar estrategias efectivas de conservaciÃ³n marina.

âœ… Reproducibilidad (Fortaleza Mayor)

Este es el punto mÃ¡s fuerte de su sistema. Al definir el entorno computacional de forma declarativa con rix en build_env.R y materializarlo con Nix, ha eliminado prÃ¡cticamente la variabilidad del entorno. Cualquier investigador, en cualquier mÃ¡quina con Nix, puede ejecutar make regenerate y nix-shell para recrear un ambiente de software bit a bit idÃ©ntico. Esto es el estÃ¡ndar de oro para la reproducibilidad computacional.

âœ… Facilidad de Uso y Mantenimiento (Fortaleza)

El Makefile es la clave aquÃ­. Proporciona una "API" de lÃ­nea de comandos simple y legible para su proyecto. Un nuevo colaborador no necesita entender los detalles de Nix o los scripts de shell; solo necesita leer la salida de make help para empezar a trabajar. Esto reduce drÃ¡sticamente la curva de aprendizaje y los posibles errores.

ðŸ“ˆ Escalabilidad (Fortaleza con un Ã¡rea de mejora)

    Fortaleza: El uso de {targets} es ideal para la escalabilidad del anÃ¡lisis. Su naturaleza basada en dependencias garantiza que solo se recalculen los pasos necesarios, ahorrando un tiempo de cÃ³mputo inmenso a medida que el anÃ¡lisis crece en complejidad.


Nix, utilizado a travÃ©s de rix, ofrece la soluciÃ³n mÃ¡s sÃ³lida y trazable para garantizar entornos reproducibles y pipelines cientÃ­ficos que integran R con dependencias del sistema. Aunque la adopciÃ³n exige inversiÃ³n en aprendizaje, los beneficios en reproducibilidad, integridad y archivado hacen de Nix la opciÃ³n preferente para proyectos cientÃ­ficos serios y para la producciÃ³n de anÃ¡lisis reproducibles.

ste trabajo usa una estructura estÃ¡ndar para organizaciÃ³n de trabajos bioinformÃ¡ticos [@noble2009]: top-level organization that is logical, with chronological organization at the next level, and logical organization below that. A sample project, called msms, is shown in Figure 1. At the root of most of my projects, I have a data directory for storing fixed data sets, a results directory for tracking computational experiments peformed on that data, a doc directory with one subdirectory per manuscript, and directories such as src for source code and bin for compiled binaries or scripts.

> version control should only be used for files that you edit by hand. Automatically generated files, whether they are compiled programs or the results of a computational experiment, do not belong under version control [@noble2009].


[@boyle2009]: **Layered content management**: To manage data arising from ongoing research experiments we adopted an approach using distributed content repositories. Content repositories allow for the development of a formalized structure that can be associated directly with resources. Easy to adapt. Within a research environment it is generally difficult to hammer down requirements. The requirements change over time, and new functionality is often required at short notice. Easy to understand. Any data management solution will involve a high level of complexity, especially in a distributed research environment. Easy to access. Within a research environment there is little time to be spared for learning (largely transient) informatics systems. â€¢ Instrumentation Layer. This layer is used to capture experimental information. The layer models information in a way that makes storage of the information simpler, so that laboratory scientists can easily add and annotate information that is captured from a variety of instruments. â€¢ Conceptual Layer. This layer is designed to provide a means to generically interact with the information through the use of high level abstract operations. These operations include the aggregation and retrieval of information, and do not necessitate an understanding of the actual information content. â€¢ Organizational Layer. This layer provides a project (or researcher) based view on the information, and therefore is designed to have a "biological focus". Typically the content is organized by factors such as disease, organism or molecule. Each different research, or research group, can individually organize and annotate the data to suit their individual requirements.



### Impacto para la ConservaciÃ³n

- **IdentificaciÃ³n de especies vulnerables** con distribuciones restringidas
- **Mapeo de hÃ¡bitats crÃ­ticos** que requieren protecciÃ³n prioritaria  
- **Desarrollo de estrategias adaptativas** para el cambio climÃ¡tico
- **OptimizaciÃ³n de esfuerzos de monitoreo** y recursos limitados

Este enfoque metodolÃ³gico puede replicarse para otras regiones y grupos taxonÃ³micos, contribuyendo al conocimiento global sobre biodiversidad marina y su conservaciÃ³n.

::: {.callout  title="ðŸ’¡ Posibles ideas"}

- [ ] : hacer un diccionario compartido con extensiones de vscode.

:::

# Resumen Ejecutivo

## Estado del pipeline

```{r}
#| label: pipeline-status
#| echo: true

# Show pipeline status
cat("Estado de los objetos del pipeline:\n\n")
for (target in names(loaded_targets)) {
  status <- if (loaded_targets[[target]]) "âœ“ Disponible" else "âœ— No disponible"
  cat("-", target, ":", status, "\n")
}

# Check targets status
if (requireNamespace("targets", quietly = TRUE)) {
  cat("\nEstado general del pipeline:\n")
  tar_progress() |>
    count(progress) |>
    kable(col.names = c("Estado", "NÃºmero de targets"))
}
```

## Uso recomendado del ambiente y pipeline

***Si se modifica la pipeline***

Al modificar el archivo '[build_env.R](https://github.com/santi-rios/NereidaPipeline/blob/main/build_env.R)' (p. ej. agregar librerÃ­as nuevas), ejecutar este comando:

```bash
nix-shell
make update
```

Al ejecutar este script:

- Regeneramos y construimos el ambiente de Nix
- Ejecutamos el script '../test_environment.sh' para comprobar y verificar la disponibilidad de librerÃ­as utilizadas en la pipeline
- Ejecutamos la pipeline (*opcional*)

***Si no se agregan nuevos paquetes a la pipeline***

Ejecutar:

```bash
nix-shell
make pipeline
```

<!-- TODO: revisar los scripts y potencialmetne agregar make a los targets individuales, no a toda la pipe -->

# InspecciÃ³n de la Pipeline

Esta secciÃ³n documenta la estructura y dependencias de la pipeline de anÃ¡lisis, permitiendo verificar la integridad del flujo de trabajo antes de la ejecuciÃ³n.

## Manifiesto de Targets

El manifiesto muestra todos los targets definidos en la pipeline y sus comandos de ejecuciÃ³n:

```{r}
#| label: pipeline-manifest
#| echo: true
#| code-fold: show

# Mostrar el manifiesto de la pipeline con comandos
tar_manifest(fields = all_of("command")) |>
  kable(
    caption = "Manifiesto completo de la pipeline de anÃ¡lisis",
    format = "html"
  ) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    font_size = 10
  ) |>
  scroll_box(width = "100%", height = "500px")
```

## Grafo de Dependencias

El grafo muestra las relaciones entre los diferentes componentes de la pipeline:

```{r}
#| label: pipeline-graph
#| fig-width: 12
#| fig-height: 8
#| fig-cap: "VisualizaciÃ³n interactiva de las dependencias de la pipeline"

# Generar visualizaciÃ³n interactiva del grafo de dependencias
tar_visnetwork(
  targets_only = TRUE,
  label = c("time", "size", "branches"),
  level_separation = 150,
  degree_from = 1,
  degree_to = 1
)
```

::: {.callout-note}
## InterpretaciÃ³n del Grafo

- **Nodos verdes**: Targets actualizados y sin cambios
- **Nodos azules**: Targets desactualizados que necesitan re-ejecuciÃ³n
- **Nodos grises**: Targets que aÃºn no se han ejecutado
- **Flechas**: Indican dependencias entre targets

El flujo natural va de izquierda a derecha, mostrando cÃ³mo los datos se transforman desde la adquisiciÃ³n hasta los resultados finales.
:::

## Resumen de Targets por Fase

```{r}
#| label: targets-by-phase

# Obtener metadata de todos los targets
tar_meta <- tar_meta(fields = c("name", "type", "bytes", "seconds"))

# Clasificar targets por fase del anÃ¡lisis
target_phases <- data.frame(
  Target = tar_meta$name,
  Tipo = tar_meta$type,
  TamaÃ±o = ifelse(!is.na(tar_meta$bytes), 
                  paste(round(tar_meta$bytes / 1024^2, 2), "MB"),
                  "N/A"),
  Tiempo = ifelse(!is.na(tar_meta$seconds),
                  paste(round(tar_meta$seconds, 2), "s"),
                  "N/A"),
  Fase = case_when(
    grepl("obis|gbif|genome|taxonomic", tar_meta$name) ~ "1. AdquisiciÃ³n de Datos",
    grepl("cleaned|combined", tar_meta$name) ~ "2. Limpieza de Datos",
    grepl("environmental|spatial", tar_meta$name) ~ "3. AnÃ¡lisis Espacial",
    grepl("habitat|model", tar_meta$name) ~ "4. Modelado",
    grepl("database|marine_db", tar_meta$name) ~ "5. IntegraciÃ³n de Base de Datos",
    grepl("summary|report|export", tar_meta$name) ~ "6. Reportes y ExportaciÃ³n",
    grepl("metagenomic|phylostrat|assembly", tar_meta$name) ~ "7. AnÃ¡lisis MetagenÃ³mico",
    TRUE ~ "Otros"
  )
) |>
  arrange(Fase, Target)

# Mostrar tabla por fase
kable(
  target_phases,
  caption = "Targets organizados por fase del anÃ¡lisis",
  format = "html"
) |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed")) |>
  column_spec(5, bold = TRUE) |>
  scroll_box(width = "100%", height = "600px")
```

## EstadÃ­sticas de EjecuciÃ³n

```{r}
#| label: execution-stats

# Obtener estadÃ­sticas de progreso
progress_data <- tar_progress()

if (!is.null(progress_data) && nrow(progress_data) > 0) {
  
  # Resumen por estado
  progress_summary <- progress_data |>
    count(progress) |>
    mutate(
      Porcentaje = round(n / sum(n) * 100, 1),
      Estado = case_when(
        progress == "built" ~ "âœ“ Completado",
        progress == "skipped" ~ "âŠ™ Omitido (sin cambios)",
        progress == "started" ~ "âŸ³ En progreso",
        progress == "errored" ~ "âœ— Error",
        TRUE ~ progress
      )
    ) |>
    select(Estado, Cantidad = n, Porcentaje)
  
  kable(
    progress_summary,
    caption = "Estado de ejecuciÃ³n de los targets",
    format = "html"
  ) |>
    kable_styling(bootstrap_options = c("striped", "hover"))
  
  # VisualizaciÃ³n de estado
  if (nrow(progress_summary) > 0) {
    ggplot(progress_summary, aes(x = "", y = Cantidad, fill = Estado)) +
      geom_bar(stat = "identity", width = 1) +
      coord_polar("y", start = 0) +
      scale_fill_manual(
        values = c(
          "âœ“ Completado" = "#27ae60",
          "âŠ™ Omitido (sin cambios)" = "#3498db", 
          "âŸ³ En progreso" = "#f39c12",
          "âœ— Error" = "#e74c3c"
        )
      ) +
      theme_void() +
      labs(
        title = "DistribuciÃ³n del Estado de Targets",
        fill = "Estado"
      ) +
      theme(
        plot.title = element_text(hjust = 0.5, size = 14, face = "bold"),
        legend.position = "right"
      )
  }
  
} else {
  cat("âš ï¸ No hay datos de progreso disponibles. Ejecuta `tar_make()` primero.\n")
}
```

## AnÃ¡lisis de Dependencias

```{r}
#| label: dependency-analysis

# Analizar la red de dependencias
deps <- tar_network(targets_only = TRUE)

if (!is.null(deps) && nrow(deps$edges) > 0) {
  
  # Calcular mÃ©tricas de red
  dependency_metrics <- data.frame(
    MÃ©trica = c(
      "Total de targets",
      "Total de dependencias",
      "Targets raÃ­z (sin dependencias entrantes)",
      "Targets finales (sin dependencias salientes)",
      "Profundidad mÃ¡xima de la pipeline"
    ),
    Valor = c(
      length(unique(c(deps$edges$from, deps$edges$to))),
      nrow(deps$edges),
      sum(!unique(deps$edges$to) %in% deps$edges$from),
      sum(!unique(deps$edges$from) %in% deps$edges$to),
      "Calcular manualmente"  # Esto requerirÃ­a anÃ¡lisis de grafo mÃ¡s complejo
    )
  )
  
  kable(
    dependency_metrics,
    caption = "MÃ©tricas de la red de dependencias",
    format = "html"
  ) |>
    kable_styling(bootstrap_options = c("striped", "hover"))
  
  # Targets mÃ¡s conectados
  most_connected <- deps$edges |>
    count(from, sort = TRUE) |>
    head(10) |>
    rename(Target = from, `Dependencias salientes` = n)
  
  kable(
    most_connected,
    caption = "Top 10 targets con mÃ¡s dependencias",
    format = "html"
  ) |>
    kable_styling(bootstrap_options = c("striped", "hover"))
}
```

::: {.callout-tip}
## Uso PrÃ¡ctico

Para verificar la pipeline antes de ejecutarla:

1. **Revisar el manifiesto**: AsegÃºrate de que todos los comandos sean correctos
2. **Inspeccionar el grafo**: Verifica que las dependencias sean lÃ³gicas
3. **Validar sin ejecutar**: Usa `tar_validate()` para detectar errores sin correr la pipeline
4. **EjecuciÃ³n incremental**: `targets` solo re-ejecuta lo necesario

```r
# Validar la pipeline sin ejecutar
tar_validate()

# Ver quÃ© targets estÃ¡n desactualizados
tar_outdated()

# Ejecutar solo un target especÃ­fico
tar_make(names = "cleaned_occurrences")
```
:::

## ValidaciÃ³n de la Pipeline

```{r}
#| label: pipeline-validation
#| echo: true

# Validar la estructura de la pipeline
validation_result <- tryCatch({
  tar_validate()
  "âœ“ Pipeline vÃ¡lida - sin errores detectados"
}, error = function(e) {
  paste("âœ— Errores detectados:", e$message)
})

cat(validation_result, "\n")
```

## Targets Desactualizados

```{r}
#| label: outdated-targets

# Identificar targets que necesitan actualizaciÃ³n
outdated <- tar_outdated()

if (length(outdated) > 0) {
  cat("âš ï¸ Los siguientes targets necesitan actualizaciÃ³n:\n\n")
  for (target in outdated) {
    cat("-", target, "\n")
  }
  cat("\nEjecuta `tar_make()` para actualizar estos targets.\n")
} else {
  cat("âœ“ Todos los targets estÃ¡n actualizados.\n")
}
```

# Referencias

*Las referencias bibliogrÃ¡ficas se incluirÃ­an aquÃ­ en un anÃ¡lisis completo, citando las fuentes de datos, mÃ©todos estadÃ­sticos, y literatura cientÃ­fica relevante.*

## Software Citations

Please cite the following software packages used in this analysis:

### biomartr

> Drost HG, Paszkowski J. Biomartr: genomic data retrieval with R. 
> Bioinformatics (2017) 33(8): 1216-1217. 
> doi:10.1093/bioinformatics/btw821

### targets

> Landau, W. M., (2021). The targets R package: a dynamic Make-like 
> function-oriented pipeline toolkit for reproducibility and 
> high-performance computing. Journal of Open Source Software, 6(57), 2959,
> https://doi.org/10.21105/joss.02959

### Biostrings

> PagÃ¨s H, Aboyoun P, Gentleman R, DebRoy S (2023). Biostrings: Efficient 
> manipulation of biological strings. R package version 2.68.0,
> https://bioconductor.org/packages/Biostrings.

#### Data Sources

```{r data-sources}
if (!is.null(genomic_sequences)) {
  databases_used <- unique(unlist(lapply(genomic_sequences, function(x) {
    if (is.list(x) && !is.null(x$database_used)) {
      return(x$database_used)
    }
    return(NULL)
  })))
  
  databases_used <- databases_used[!is.null(databases_used)]
  
  if (length(databases_used) > 0) {
    cat("Data retrieved from the following NCBI databases:\n\n")
    for (db in databases_used) {
      if (db == "refseq") {
        cat("- **NCBI RefSeq**: https://www.ncbi.nlm.nih.gov/refseq/\n")
        cat("  > O'Leary NA, et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation. Nucleic Acids Res. 2016;44(D1):D733-45.\n\n")
      } else if (db == "genbank") {
        cat("- **NCBI GenBank**: https://www.ncbi.nlm.nih.gov/genbank/\n")
        cat("  > Sayers EW, et al. Database resources of the National Center for Biotechnology Information. Nucleic Acids Res. 2022;50(D1):D20-D26.\n\n")
      }
    }
  }
}
```


## File Locations

All downloaded genomic data files are stored in:

- **Proteomes**: `data/raw/genomic/proteomes/`
- **CDS**: `data/raw/genomic/cds/`
- **GFF annotations**: `data/raw/genomic/gff/`
- **Summary reports**: `data/processed/genomic/`

---

*Report generated on `r Sys.time()`*

## InformaciÃ³n de SesiÃ³n

```{r}
#| label: session-info
#| echo: true

sessionInfo()
```

---

**Nota**: Este reporte fue generado automÃ¡ticamente usando el pipeline integrado de anÃ¡lisis de biodiversidad marina. Para mÃ¡s informaciÃ³n sobre la metodologÃ­a y cÃ³digo fuente, consultar el repositorio del proyecto.
