---
title: "An√°lisis de Biodiversidad Marina: Pipeline Integrado para Conservaci√≥n y Evoluci√≥n"
subtitle: "Reporte T√©cnico de An√°lisis Espacial y Evolutivo de Especies Marinas"
date: "`r Sys.Date()`"
author:
  - name: Santiago Garc√≠a R√≠os
    degrees: 
      - Bi√≥logo
      - Maestro en C. (tr√°mite)
    roles: "Conceived and designed the study, analysed the results and wrote the manuscript."
    orcid: 0000-0001-6237-9616
    email: santiago_gr@ciencias.unam.mx
    phone: 55-81-07-99-11
    url: https://santi-rios.github.io/
    affiliation: 
      - name: Universidad Nacional Aut√≥noma de M√©xico
        city: Ciudad de M√©xico
abstract: > 
  Este reporte presenta los resultados del pipeline integrado de an√°lisis de biodiversidad marina, enfocado en la conservaci√≥n y evoluci√≥n de especies marinas del Caribe. El an√°lisis incluye datos de ocurrencia de m√∫ltiples fuentes, validaci√≥n de coordenadas, modelado de idoneidad de h√°bitat, y an√°lisis espacial comprehensivo. Combina tres herramientas clave de manera muy efectiva: Make como orquestador, Nix (a trav√©s de rix) como gestor del ambiente computacional y {targets} para la gesti√≥n de la pipeline de an√°lisis.
  - Biodiversidad
  - Pipeline
license:
  text: >
    The code in this repository is licensed under MIT License and the academic quarto report is licensed under CC BY 4.0
  type: open-access
  url: https://www.gnu.org/licenses/fdl-1.3-standalone.html
copyright: 
  holder: Josiah Carberry
  year: 2008
citation: 
  container-title: Journal of Psychoceramics
  volume: 1
  issue: 1
  doi: 10.5555/12345678
funding: "The author received no specific funding for this work."
format:
  html:
    # title
    title-block-banner: "../figures/banner.png"
    title-block-banner-color: white
    # toc
    toc: true
    toc-depth: 3
    toc-location: body
    number-sections: true
    anchor-sections: true
    # code
    code-fold: true
    code-copy: true
    # code-link: true
    code-tools: true
    # figures and links
    lightbox: true
    link-external-icon: true
    # theme
    theme: cosmo
    # css: styles.css
    # render
    embed-resources: true
execute: 
  echo: false
  warning: false
  message: false
bibliography: "../referencias/referencias.bib"
lang: es
---

<!-- TODO: CV orcid y PDF y https://santi-rios.github.io/ -->

<!-- TODO: fetch articles https://www.icmyl.unam.mx/es/quienes-somos/publicaciones-del-instituto -->

<!-- TODO: ensamble biomatr -->

<!-- TODO: https://roxygen2.r-lib.org/ -->

<!-- TODO: jupyter -->

```{r}
#| label: setup
#| include: false

# Cargar paquetes necesarios para el reporte
suppressPackageStartupMessages({
  library(targets)
  library(dplyr)
  library(knitr)
  library(DT)
  library(plotly)
  library(leaflet)
  library(ggplot2)
  library(tidyr)
  library(kableExtra)
})

# Cargar los objetos del pipeline de targets necesarios para el reporte.
# tar_quarto() detectar√° estas dependencias y ejecutar√° este reporte
# solo cuando los objetos cambien.
tar_load(
  c(
    "obis_occurrences",
    "gbif_occurrences",
    "cleaned_occurrences",
    "spatial_analysis",
    "habitat_models",
    "data_summaries",
    "pipeline_report",
    "genome_availability",
    "genomic_sequences",
    "genomic_quality"
  )
)

# Crear el objeto 'loaded_targets' para verificar la disponibilidad
target_names <- c(
  "obis_occurrences",
  "gbif_occurrences",
  "cleaned_occurrences",
  "spatial_analysis",
  "habitat_models",
  "data_summaries",
  "pipeline_report"
)
loaded_targets <- sapply(target_names, exists, simplify = FALSE)


# Configuraci√≥n de Knitr
knitr::opts_chunk$set(
  fig.width = 10,
  fig.height = 7,
  dpi = 300,
  out.width = "100%"
)

# Funci√≥n auxiliar para verificar si un objeto existe y tiene datos
safe_check <- function(obj_name) {
  exists(obj_name) && !is.null(get(obj_name)) &&
  (if (is.data.frame(get(obj_name))) nrow(get(obj_name)) > 0 else TRUE) &&
  (if (is.list(get(obj_name))) length(get(obj_name)) > 0 else TRUE)
}

```


## Introducci√≥n

La biodiversidad marina enfrenta amenazas sin precedentes debido al cambio clim√°tico, la acidificaci√≥n oce√°nica, y las actividades antropog√©nicas. Este an√°lisis utiliza un pipeline integrado que combina m√∫ltiples fuentes de datos y m√©todos anal√≠ticos avanzados para evaluar el estado de conservaci√≥n y los patrones evolutivos de especies marinas clave.

La robustez metodol√≥gica es tan importante como los resultados mismos, especialmente en campos como la biolog√≠a de la conservaci√≥n y la evoluci√≥n, donde las decisiones pueden tener implicaciones a largo plazo y deben basarse en evidencia auditable y reproducible.

### Ciencias Multi√≥micas

Multi√≥micos se refiere a un enfoque de investigaci√≥n que integra datos de m√∫ltiples disciplinas "√≥micas" (como la gen√≥mica, prote√≥mica, transcript√≥mica, etc.) para obtener una comprensi√≥n m√°s completa y hol√≠stica de un sistema biol√≥gico, enfermedad o proceso celular. Al combinar estos diferentes niveles moleculares, los estudios multi√≥micos permiten inferir mecanismos biol√≥gicos subyacentes de manera m√°s precisa, desentra√±ar las causas de fen√≥menos complejos y descubrir biomarcadores. 

√ìmicas clave que se integran:

- Gen√≥mica: Estudio de los genes y su variaci√≥n. 
- Transcript√≥mica: Estudio de los ARN mensajeros (ARNm) para entender la expresi√≥n g√©nica. 
- Prote√≥mica: Estudio de las prote√≠nas, sus estructuras y funciones. 
- Metabol√≥mica: Estudio de los metabolitos y c√≥mo interact√∫an. 
- Epigen√≥mica: Estudio de los cambios heredables en la expresi√≥n g√©nica sin alterar la secuencia de ADN. 
- Microbioma: Estudio de las comunidades microbianas y su impacto en el hu√©sped. 


### El Paradigma de la Investigaci√≥n Reproducible

> Establecer el marco conceptual y la necesidad urgente de la reproducibilidad computacional en las ciencias, particularmente en ecolog√≠a y evoluci√≥n.

[@noble2009]: The core guiding principle is simple: Someone unfamiliar with your project should be able to look at your computer files and understand in detail what you did and why. This ‚Äò‚Äòsomeone‚Äô‚Äô could be any.

[@dolstra2006]: The development of principles and tools to support the deployment process has largely been relegated to industry, system administrators, and Unix hackers. This has resulted in a large number of often ad hoc tools that typically automate manual practices but do not address fundamental issues in a systematic and disciplined way. This is evidenced by the huge number of mailing list and forum postings about deployment failures, ranging from applications not working due to missing dependencies, to subtle malfunctions caused by incompatible components. Deployment problems also seem curiously resistant to automation: the same concrete problems appear time and again. Deployment is especially difficult in heavily component-based systems‚Äîsuch as Unix-based open source software‚Äîbecause the effort of dealing with the dependencies can increase super-linearly with each additional dependency. This work describes a system for software deployment called Nix (trough '{rix}-r package') that addresses many of the problems that plague existing deployment systems.

[@wilson2017]: Este art√≠culo Justifica la necesidad de pr√°cticas como la automatizaci√≥n de flujos de trabajo (e este proyecto usamos herramientas como `Make` y `{targets}`) y el seguimiento de versiones del software (lo que logra con `Nix`). Argumentar que su metodolog√≠a no es una complejidad innecesaria, sino una adhesi√≥n a las "buenas pr√°cticas" recomendadas por la comunidad cient√≠fica para evitar errores y asegurar la longevidad de los resultados.

[@marwick2018]: Este art√≠culo aborda directamente el ecosistema de R. Explica c√≥mo empaquetar un proyecto de an√°lisis de datos (c√≥digo, datos y el ambiente computacional) para hacerlo reproducible. Proporciona el argumento acad√©mico perfecto para justificar por qu√© usar una herramienta como `Nix` (a trav√©s de `rix`) es una soluci√≥n superior a simplemente listar los paquetes, ya que captura el estado completo del sistema.

### Gesti√≥n del Ambiente Computacional con Nix

Estas referencias validan el uso de gestores de ambientes declarativos como Nix para garantizar la reproducibilidad a nivel de software.

[@dolstra2006]: Esta es la fuente original que describe el modelo detr√°s de Nix. Es una referencia m√°s t√©cnica, pero citarla demuestra un profundo entendimiento de los fundamentos de su flujo de trabajo. Puede usarla para afirmar que su proyecto se basa en un "modelo puramente funcional" que garantiza la construcci√≥n de ambientes computacionales determin√≠sticos, eliminando problemas comunes como el "funciona en mi m√°quina".

Abogar por herramientas como Nix para resolver la "crisis de reproducibilidad" en campos computacionalmente intensivos. Citar este art√≠culo le permite establecer un paralelismo, argumentando que, al igual que en la neurociencia, la complejidad del an√°lisis de datos gen√≥micos, espaciales y taxon√≥micos en la conservaci√≥n marina exige soluciones de reproducibilidad robustas.

### Automatizaci√≥n de Pipelines con `{targets}`

La gesti√≥n de la pipeline es crucial para la eficiencia y la correcci√≥n de los an√°lisis complejos. `{targets}` es el est√°ndar moderno en el ecosistema R.

[@landau2021]: Esta es la cita oficial y directa del paquete `{targets}`. Es indispensable. √ösela para introducir la herramienta, explicando que su elecci√≥n se basa en una soluci√≥n documentada y revisada por pares dise√±ada espec√≠ficamente para crear pipelines de an√°lisis reproducibles y eficientes en R. Resalte caracter√≠sticas clave que el art√≠culo menciona, como el seguimiento de dependencias y el paralelismo, que son cruciales para an√°lisis a gran escala en conservaci√≥n.

[@noble2009]: A few months from now, you may not remember what you were up to when you created a particular set of files, or you may not remember what you drew. You will either have to then spend time reconstructing your previous experiments or lose whatever insights you gained from those experiments.

[@noble2009]: it is important to handle long-running scrips and its outputs. The final line of a runall script calls summarize, which in turn creates a plot, table, or HTML page that summarizes the results of the experiment (in our case, we use quarto for this). The summarize script is written in such a way that it can interpret a partially completed experiment, showing how much of the computation has been performed thus far.

### Orquestaci√≥n del Flujo de Trabajo con `Make`

El uso de `Make` como un "orquestador" de alto nivel simplifica la interacci√≥n y estandariza los procedimientos.

[@noble2009]: Este influyente art√≠culo describe las mejores pr√°cticas para organizar proyectos de biolog√≠a computacional. Recomienda expl√≠citamente el uso de `Make` para automatizar la pipeline, desde la descarga de datos hasta la generaci√≥n de figuras finales. Citarlo posiciona su uso de `Makefile` como una pr√°ctica establecida y recomendada para mantener la organizaci√≥n, la claridad y la automatizaci√≥n en proyectos complejos, lo cual es vital en estudios integrativos de biodiversidad marina.

::: {.callout-note  title="Makefile"}

Makefile funciona como un "protocolo de laboratorio" ejecutable. Su prop√≥sito principal es automatizar y estandarizar las tareas repetitivas, encapsulando comandos complejos en alias simples y declarativos.

Makefile no es solo un archivo, sino la interfaz de control principal. En lugar de recordar una serie de scripts y sus argumentos, usted y sus colaboradores solo necesitan interactuar con comandos sem√°nticos como make test o make regenerate.

As√≠ es como su Makefile dirige el flujo:

    make regenerate: Este es el comando fundamental para la reproducibilidad. No ejecuta rix directamente, sino que delega la tarea al script ./regenerate.sh. Este script, a su vez, invoca su archivo build_env.R, donde la funci√≥n rix::rix() traduce su lista de paquetes de R y dependencias del sistema en un archivo default.nix. Este es el plano exacto de su ambiente.

    make test: Este comando invoca ./test_environment.sh, un paso de validaci√≥n crucial. El script verifica que el ambiente de Nix, una vez construido, contenga todos los paquetes que su pipeline de {targets} declara necesitar en _targets.R. Esto cierra el c√≠rculo, asegurando que el ambiente definido coincide con el ambiente requerido.

    make update: Act√∫a como un meta-comando que ejecuta una secuencia l√≥gica de tareas a trav√©s del script update_workflow.sh, probablemente combinando la regeneraci√≥n y la prueba del ambiente.

    make clean: Mantiene la higiene del proyecto, eliminando artefactos y resultados intermedios para garantizar que la pr√≥xima ejecuci√≥n comience desde un estado conocido y limpio.

:::

### Control de versiones

Este trabajo usa una estructura est√°ndar para organizaci√≥n de trabajos bioinform√°ticos [@noble2009]: top-level organization that is logical, with chronological organization at the next level, and logical organization below that. A sample project, called msms, is shown in Figure 1. At the root of most of my projects, I have a data directory for storing fixed data sets, a results directory for tracking computational experiments peformed on that data, a doc directory with one subdirectory per manuscript, and directories such as src for source code and bin for compiled binaries or scripts.

[@noble2009]: Record every operation that you perform. 2. Comment generously. . Avoid editing intermediate files by hand. Use relative pathnames to access other files within the same project. perhaps most significantly, version control is invaluable for collaborative projects. The repository allows collaborators to work simultaneously on a collection of files, including scripts, documentation, or a draft manuscript. If two individuals edit the same file in parallel, then the version control software will automatically merge the two versions and flag lines that were edited by both people.

> version control should only be used for files that you edit by hand. Automatically generated files, whether they are compiled programs or the results of a computational experiment, do not belong under version control [@noble2009].

[@boyle2009]: changes in software engineering and design include: the methodology through which software is constructed (e.g. components leading to frameworks, and frameworks leading to aspects [1]); the technology used to allow for distributed computing (e.g. object brokers evolving pass-byvalue mechanisms, and these being replaced by stateless Web Services); and the ideology that is used to define the process through which software is built (e.g. the "rational" processes being replaced by agile programming).

[@boyle2009]: **Layered content management**: To manage data arising from ongoing research experiments we adopted an approach using distributed content repositories. Content repositories allow for the development of a formalized structure that can be associated directly with resources. Easy to adapt. Within a research environment it is generally difficult to hammer down requirements. The requirements change over time, and new functionality is often required at short notice. Easy to understand. Any data management solution will involve a high level of complexity, especially in a distributed research environment. Easy to access. Within a research environment there is little time to be spared for learning (largely transient) informatics systems. ‚Ä¢ Instrumentation Layer. This layer is used to capture experimental information. The layer models information in a way that makes storage of the information simpler, so that laboratory scientists can easily add and annotate information that is captured from a variety of instruments. ‚Ä¢ Conceptual Layer. This layer is designed to provide a means to generically interact with the information through the use of high level abstract operations. These operations include the aggregation and retrieval of information, and do not necessitate an understanding of the actual information content. ‚Ä¢ Organizational Layer. This layer provides a project (or researcher) based view on the information, and therefore is designed to have a "biological focus". Typically the content is organized by factors such as disease, organism or molecule. Each different research, or research group, can individually organize and annotate the data to suit their individual requirements.

### Generaci√≥n de reportes t√©cnicos con Quarto

[@noble2009]:In addition, the need to make results accessible to and understandable by wet lab biologists may have practical implications for how a project is managed. For example, to make the results more understandable, significant effort may need to go into the prose descriptions of experiments in the lab notebook, rather than simply including a figure or table with a few lines of text summarizing the major conclusion. More practically, differences in operating systems and software may cause logistical difficulties. For example, computer scientists may prefer to write their documents in the LaTeX typesetting language, whereas biologists may prefer Microsoft Word.

### Uso de bases de datos

[@boyle2009]: database based solutions to open, distributed, interoperable data management solutions (see Figure 1). This change has been driven by demands for rapid development, high levels of interoperability and increases in data volume and complexity.

[@boyle2009]: **aggregation subsystem*: One of the aims of the computing advances over the last few years has involved the concept of run time aggregation. This approach is epitomized by the semantic web, where people can mash information from a variety of data sources into a single graph. When an analysis run has finished, the observing aggregation system can trigger an indexing operation on the results. The indexing is controlled using the properties that are associated with the output of the analysis run (typically the properties related to the rows or columns that are to be indexed).

> For this last point... we could try a quarto dashboard to summirize the data and make the github actions trigger to continuously (or at query) deploys updated info to the web.

### Error handling

[@noble2009]: During the development of a complicated set of experiments, you will introduce errors into your code. Such errors are inevitable, but they are particularly problematic if they are difficult to track down or, worse, if you don‚Äôt know about them and hence draw invalid conclusions from your experiment. Here are three suggestions for error handling. Write robust code to detect errors.

::: {.callout-note}
'direnv: error /home/santi/Projects/NereidaPipeline/.envrc is blocked. Run `direnv allow` to approve its content'
:::


## Objetivos

1. **Integrar datos de ocurrencia** de m√∫ltiples fuentes (OBIS, GBIF)
2. **Validar y limpiar** coordenadas geogr√°ficas usando m√©todos estandarizados
3. **Analizar patrones espaciales** de distribuci√≥n de especies marinas
4. **Modelar idoneidad de h√°bitat** considerando variables ambientales
5. **Generar recomendaciones** para conservaci√≥n basadas en evidencia

## Planteamiento del problema

¬øPor qu√© es importante la multi√≥mica? Proporciona una perspectiva m√°s completa de un sistema biol√≥gico en lugar de solo estudiar componentes aislados. Sin embargo, es la Integraci√≥n de datos complejos, los cuales Permiten integrar y analizar grandes vol√∫menes de datos generados por diferentes tecnolog√≠as de alto rendimiento, no suele ser un paso sencillo.

For example, Next-Generation-Sequencing (NGS) bioinformatics uses computational tools, software, and algorithms to process and analyze the massive datasets generated by Next-Generation Sequencing (NGS). It involves cleaning and aligning millions of DNA or RNA fragments to reconstruct genomes or transcriptomes, identifying genetic variants like mutations, and annotating these variants to understand their biological significance. It is essential to consider: Data Processing: Raw sequence reads from NGS instruments are complex and need to be cleaned, aligned to a reference genome, and assembled to form longer sequences. Variant Calling and Annotation: The process identifies genetic variations (such as single nucleotide polymorphisms or structural variations) and adds information about these variants' potential functions or links to disease. Interpretation and Visualization: The analyzed data is presented in user-friendly formats like reports and visualizations, allowing researchers and clinicians to make informed decisions. 

[@dolstra2006]: Software deployment is the problem of managing the distribution of software to end-user machines. That is, a developer has created some piece of software, and this ultimately has to end up on the machines of end-users. After the initial installation of the software, it might need to be upgraded or uninstalled. Presumably, the developer has tested the software and found it to work sufficiently well, so the challenge is to make sure that the software works just as well, i.e., the same, on the end-user machines. I will informally refer to this as correct deployment: given identical inputs, the software should behave the same on an end-user machine as on the developer machine1. This should be a simple problem. For instance, if the software consists of a set of files, then deployment should be a simple matter of copying those to the target machines. In practice, deployment turns out to be much harder. This has a number of causes. These fall into two broad categories: environment issues and manageability issues. Even worse, the component might be dependent on a specific compiler, or on specific compilation options being used for its dependencies. This is often a rather labour-intensive part of the deployment process. 

[@dolstra2006]: the Nix deployment system, which overcomes the limitations of contemporary deployment tools described above. It solves implementation (how it works), the underlying principles (why it works), our experiences and empirical validation (that it works), and the application areas to which it can be applied (where it works).




## Metodolog√≠a

### Preparaci√≥n de librer√≠as

DNA or RNA is fragmented and prepared for sequencing by adding sequencing adapters.

Sequencing: The prepared samples are run on high-throughput sequencing machines to generate millions of short reads.

Primary Analysis: Raw sequence data is converted into FASTQ files, which contain sequence data and quality scores. 

Secondary Analysis: Reads are aligned to a reference genome, and variants are identified and annotated. 

Tertiary Analysis: The identified variants are interpreted for their potential impact on biological pathways and phenotypes, often by querying genomic databases. 

### Crear el ambiente con Nix

El paquete R {rix} 'r library(rix)', en l√≠nea con este enfoque, permite una integraci√≥n simplificada de Nix en la plataforma R. Esto significa que las dependencias de R, del sistema y el control de versiones pueden ser gestionados de forma centralizada a trav√©s de Nix. La inclusi√≥n de {rix} en este proyecto permite el manejo eficiente de las dependencias de R de manera reproducible (Gabay et al., 2019).

En conclusi√≥n, la fundamentaci√≥n del proyecto en la integraci√≥n de R, Nix y el paquete {rix} se traduce en una robustez y una gesti√≥n optimizada de las dependencias, facilitando notablemente el procesamiento y el an√°lisis de datos y el manejo de c√≥digo. Adem√°s, permite la integraci√≥n eficiente de informaci√≥n en repositorios y proporciona un entorno de trabajo reproducible, lo cual es vital para mantener la validez y la replicabilidad de las investigaciones cient√≠ficas.

#### Scripts de Ayuda

El archivo 'regenerate.sh' ejecuta 'build_env.R' utilizando 'rix' de 'nixpkgs'.



::: {.callout-tip title="Flujo de Trabajo Recomendado"}

***Si se modifica la pipeline***

Al modificar el archivo './build_env.R'(p. ej. agregar librer√≠as nuevas), ejecutar este comando:

```bash
nix-shell
./update_workflow.sh
```

Al ejecutar este script:

- Regeneramos y construimos el ambiente de Nix
- Ejecutamos el script '../test_environment.sh' para comprobar y verificar la disponibilidad de librer√≠as utilizadas en la pipeline
- Ejecutamos la pipeline (*opcional*)

***Si no se agregan nuevos paquetes a la pipeline***

Ejecutar:

```bash
nix-shell
R
# dentro de R:
targets::tar_make()
```

:::

#### Manejo de ambientes: Nix vs otras opciones

Utilizar 'Nix' mediante el paquete 'rix' es una opci√≥n m√°s robusta para gestionar entornos cient√≠ficos y pipelines frente a herramientas como 'conda' o 'renv'. Se incluye una tabla comparativa seguida de una breve recomendaci√≥n de uso.


| Criterio / Aspecto | Nix (con rix) | Conda | renv |
|---|---:|---:|---:|
| Reproducibilidad determinista | Muy alta. Nix es declarativo e inmune a cambios del sistema; rix genera expresiones reproducibles para R y deps del sistema. | Moderada. Conda puede registrar versiones, pero la resoluci√≥n de dependencias y canales introduce variabilidad. | Parcial. Fija versiones de paquetes R (lockfile) pero no gestiona dependencias del sistema nativas. |
| Gesti√≥n de dependencias del sistema (C, libs, binarios) | Nativa y exhaustiva. Nix declara y proporciona librer√≠as del sistema de forma aislada. | Buena (conda-forge), pero limitada para ciertos paquetes del sistema y con problemas de enlaces din√°micos. | Ninguna. Depende del SO o de gestores externos; riesgo de ‚Äúworks on my machine‚Äù. |
| Aislamiento / hermeticidad | Alto: entornos aislados en la store de Nix; evita contaminaci√≥n por el entorno del usuario. | Aislado a nivel de entorno, pero puede verse afectado por bibliotecas del sistema; problemas con PATH/LD_LIBRARY_PATH. | No a nivel de sistema; s√≥lo controla paquetes R dentro del proyecto. |
| Declaratividad y trazabilidad | Declarativo (nix expressions). F√°cil de versionar, auditar y reconstruir. | Declarativo limitado (environment.yml), pero resoluci√≥n no determinista entre ejecuciones. | Declarativo para R (renv.lock) pero sin trazabilidad de libs externas. |
| Integraci√≥n con CI / archivado a largo plazo | Excelente: reconstrucci√≥n reproducible en CI; uso de cach√©s binarios; apto para archivado y publicaci√≥n de entornos. | Buena en CI, pero reproducibilidad exacta depende de canales y binarios disponibles. | Adecuado para reproducir entornos R en el corto/medio plazo; falla si faltan deps del sistema. |
| Compatibilidad multi-lenguaje | S√≠, gestiona todo el stack (R, Python, C, system tools) en un √∫nico lenguaje de especificaci√≥n. | Buena (R, Python, otros), pero fragmentaci√≥n en canales. | Enfocado a R; no gestiona otros lenguajes de forma integrada. |
| Facilidad de uso / curva de aprendizaje | Moderada a alta. rix reduce la barrera para usuarios de R pero Nix tiene conceptos nuevos. | Baja (f√°cil de empezar). Muy accesible para usuarios nuevos. | Muy baja ‚Äî transparente para usuarios R; f√°cil de incorporar. |
| Tama√±o / consumo de disco | Amplio (store de Nix), pero con beneficios de cach√© y deduplicaci√≥n entre proyectos. | Variable; entornos duplicados pueden consumir mucho. | Ligero (solo paquetes R), pero requiere deps del sistema por separado. |
| Limitaciones notables | Curva de aprendizaje; en Windows requiere WSL o soluciones; rix a√∫n evoluciona. | Canales y resoluci√≥n crean incoherencias; algunos paquetes de sistema dif√≠ciles. | No asegura reproducibilidad completa (falta libs del SO); no declara sistema. |
| Uso recomendado | Pipelines reproducibles, CI/archivado, proyectos con dependencias R + sistema, entornos multi-lenguaje y producci√≥n. | Entornos ad-hoc, exploraci√≥n r√°pida, usuarios que necesitan r√°pidos ‚Äúenvs‚Äù multiplataforma. | Desarrollo R colaborativo r√°pido, lockfiles para paquetes R, proyectos que delegan deps del SO a otra soluci√≥n. |


::: {.callout-note}

Razones clave para preferir Nix + rix en proyectos cient√≠ficos y pipelines

- Reproducibilidad completa: Nix describe exactamente qu√© se construye y cu√°les binarios y bibliotecas del sistema se usan; rix adapta este enfoque al ecosistema R, permitiendo reconstrucciones id√©nticas en diferentes m√°quinas y CI.
- Gesti√≥n unificada de R y dependencias del sistema: muchas herramientas R necesitan bibliotecas C/Fortran. Nix las gestiona junto con los paquetes R, evitando fallos invisibles por falta de libs nativas.
- Declaratividad, versionado y trazabilidad: los archivos Nix (o los artefactos que genera rix) son documentos versionables que sirven como metadatos exactos del entorno utilizado para un an√°lisis ‚Äî esencial para reproducibilidad y revisi√≥n acad√©mica.
- Aislamiento herm√©tico: elimina efectos de entornos previos o variaciones en el usuario, reduciendo errores ‚Äúworks on machine X‚Äù.
- Integraci√≥n con CI y archivado a largo plazo: cach√©s binarios y la capacidad de reconstruir entornos permiten validar pipelines en CI y archivar entornos reproducibles junto a publicaciones.
- Interoperabilidad: aunque renv y conda pueden usarse junto a Nix, emplear Nix como capa superior unifica la gesti√≥n y reduce la complejidad operativa.

Limitaciones y balance pr√°ctico

- Curva de aprendizaje: Nix requiere tiempo para dominar sus conceptos. rix reduce fricci√≥n para usuarios R, pero la adopci√≥n institucional puede exigir formaci√≥n.
- Uso de disco y recursos: la store de Nix puede ocupar m√°s espacio, aunque la deduplicaci√≥n y cach√© binario compensa en entornos compartidos.
- Windows: Nix funciona mejor en Linux/macOS; en Windows suele requerir WSL o contenedores (esto est√° mejorando con la comunidad).

Recomendaciones pr√°cticas

- Para investigaci√≥n reproducible, producci√≥n de pipelines y CI: adoptar Nix + rix como est√°ndar de entorno. Mantener la expresi√≥n Nix en el repositorio junto con el c√≥digo y los datos procesables.
- Para desarrollo r√°pido o ense√±anza: combinar renv (para control fino de paquetes R durante el desarrollo) dentro de un entorno Nix que garantice las dependencias del sistema. renv puede convivir con Nix: renv controla versiones R; Nix asegura libs nativas.
- Para entornos multi-lenguaje con uso intensivo de paquetes binarios (Python + R + herramientas C): Nix ofrece la soluci√≥n m√°s coherente y reproducible frente a la mezcla de conda + gestores del sistema.

:::

Nix, utilizado a trav√©s de rix, ofrece la soluci√≥n m√°s s√≥lida y trazable para garantizar entornos reproducibles y pipelines cient√≠ficos que integran R con dependencias del sistema. Aunque la adopci√≥n exige inversi√≥n en aprendizaje, los beneficios en reproducibilidad, integridad y archivado hacen de Nix la opci√≥n preferente para proyectos cient√≠ficos serios y para la producci√≥n de an√°lisis reproducibles.

::: {.callout-tip}

**Uso de 'conda' + 'Nix' ('{rix}')**

- Es posible usar conda dentro de un entorno generado con 'rix/Nix': 'Nix' puede instalar paquetes del ecosistema 'conda' (p. ej. 'miniconda3') y ejecutar comandos conda desde el 'shell' que 'Nix' provee. 'rix', al final, genera expresiones 'Nix'; en esas expresiones se pueden declarar paquetes del sistema como "'miniconda3'" o "'mamba'" y a√±adir un 'shell_hook' para crear/activar un entorno conda/'conda-env' al entrar al 'nix-shell'.  
- Sin embargo, en t√©rminos de reproducibilidad y coherencia, incluir 'conda' dentro de 'Nix' es en gran medida **redundante** y atenta contra las garant√≠as que 'Nix' aporta: 'conda' genera entornos que no son $100\%$ declarativos ni bit-reproducibles (resoluci√≥n de dependencias y artefactos binarios dependen de servidores externos). Por ello, el patr√≥n recomendado es gestionar 'Python' y sus binarios desde 'Nix' ('nixpkgs') siempre que sea posible; solo recurrir a conda cuando exista una dependencia binaria cr√≠tica que no est√© disponible en 'nixpkgs' o cuando el equipo ya dependa fuertemente de 'conda' y se acepte la p√©rdida parcial de determinismo.

**Gesti√≥n de dependencias del sistema**

- 'rix/Nix' gestiona paquetes de sistema ('git', 'libssl', binarios 'C/Fortran') de forma nativa y declarativa. 'Conda' (especialmente 'conda-forge') puede proveer muchos paquetes binarios ('python', librer√≠as cient√≠ficas), pero no sustituye a un gestor de sistema para librer√≠as del sistema (p. ej. 'glibc', versiones del compilador) y su resoluci√≥n depende de canales; por tanto, no iguala la hermeticidad ni la trazabilidad de 'Nix'.

**Recomendaciones pr√°cticas y consideraciones**

- Preferible: declarar 'Python' y paquetes 'Python' directamente en 'Nix' (usar paquetes 'python' de 'nixpkgs' o 'poetry2nix/pypi2nix'), o usar 'mamba/conda' dentro de 'Nix' solo para casos puntuales.
- Ventaja: entornos completamente reconstruibles en 'CI' y archivables con la expresi√≥n 'Nix'.
- Si se incorpora 'conda' dentro de 'Nix': incluir 'miniconda/mamba' como 'system_pkgs' en 'rix()' y 'use shell_hook' para crear/activar un entorno 'conda' desde un 'environment.yml' almacenado en el repositorio. Documentar expl√≠citamente este paso y aceptar las limitaciones de determinismo.
- 'Conda' puede instalar muchos paquetes ‚Äúdel sistema‚Äù (a trav√©s de 'conda-forge'), pero no gestiona el sistema base ni garantiza identicidad binaria entre m√°quinas; 'Nix' s√≠ lo hace.

Ejemplo pr√°ctico 

‚Äî modificaci√≥n sugerida de' build_env.R' (opcional: integrar 'conda/miniconda' y crear env al entrar al 'nix-shell'):

````r
# ...existing code...
rix(
  r_ver = "latest-upstream",
  r_pkgs = all_packages,
-  system_pkgs = c("git", "python3", "quarto"),
+  # Incluir miniconda3 si se necesita usar conda dentro del entorno Nix.
+  # Alternativa recomendada: gestionar Python desde nixpkgs (evita redundancia).
+  system_pkgs = c("git", "python3", "quarto", "miniconda3"),
-  tex_pkgs = c("amsmath"),
-  ide = "none",
-  shell_hook = "",
+  tex_pkgs = c("amsmath"),
+  ide = "none",
+  # Ejemplo de shell_hook que crea/actualiza un entorno conda local a partir de environment.yml.
+  # Nota: esto ejecuta conda al entrar al nix-shell; la reproducibilidad del entorno conda
+  # depende de conda/mamba y de los canales usados.
+  shell_hook = '
+if [ -f environment.yml ]; then
+  if [ ! -d ".conda-env" ]; then
+    echo "Creando entorno conda local (.conda-env) desde environment.yml..."
+    conda env create -p ./.conda-env -f environment.yml || conda env update -p ./.conda-env -f environment.yml
+  fi
+  export PATH="$(pwd)/.conda-env/bin:$PATH"
+fi
+',
  project_path = ".",
  overwrite = TRUE,
  print = TRUE
)
# ...existing code...
````

Caveats t√©cnicos y cierre

- Mezclar 'Nix + conda' es factible pero introduce una capa menos declarativa ('conda'). Para reproducibilidad acad√©mica estricta, preferir 'Nix-native' ('nixpkgs/pypi2nix/poetry2nix').
Si se necesita 'conda' por razones pr√°cticas (paquetes no disponibles en 'nixpkgs', flujo de trabajo del equipo), documentar la receta ('environment.yml', comando de creaci√≥n) y versionar el archivo junto al proyecto; idealmente incorporar la creaci√≥n del entorno ''conda'' en 'CI' para asegurar consistencia operativa aunque no absoluta bit-reproducible.
'Nix' (por medio de 'rix') gestiona 'git' y otras dependencias del sistema de forma nativa y declarativa; 'conda' puede proveer muchos binarios pero no reemplaza la gesti√≥n total de librer√≠as del sistema ni ofrece la misma trazabilidad. 

:::


##### Ejemplo de Github problema

## Repository Cleanup and GitHub Push Issues

You're facing several related issues with large files and Git history cleaning. Let's solve this systematically:

## Root Issues

1. **Large files in Git history**: 
   - `data/raw/obis/orites_astreoides_obis_data.csv` (54.69 MB)
   - `data/raw/acropora_data.csv` (117.00 MB)

2. **Tool problems**:
   - BFG requires a newer Java version
   - Changes to build_env.R get reverted
   - Git cleaning scripts aren't targeting the specific CSV files

## Complete Solution

Let's create an enhanced version of your quick_fix_git.sh script that specifically targets the problematic files:

````bash
#!/bin/bash
# Complete repository cleanup script

echo "üßπ Repository Deep Cleanup"
echo "=========================="
echo ""
echo "This script will:"
echo "  1. Create a backup of your repository"
echo "  2. Remove large CSV files from Git history"
echo "  3. Create proper .gitignore entries"
echo "  4. Set up a clean repository ready for GitHub"
echo ""

read -p "Continue? (y/n): " -n 1 -r
echo ""
if [[ ! $REPLY =~ ^[Yy]$ ]]; then
    echo "Operation cancelled."
    exit 0
fi

# Step 1: Create backup
BACKUP_DIR="../biologia-marina-backup-$(date +%Y%m%d-%H%M%S)"
echo "üì¶ Creating backup in $BACKUP_DIR..."
cp -r . "$BACKUP_DIR"
echo "‚úì Backup created"
echo ""

# Step 2: Remove specific large files using git filter-repo
echo "üîß Removing large files from history..."

# Ensure git-filter-repo is available
if ! command -v git-filter-repo &> /dev/null; then
    echo "‚ö†Ô∏è git-filter-repo not found. Installing..."
    pip install --user git-filter-repo
    
    if ! command -v git-filter-repo &> /dev/null; then
        echo "‚ùå Failed to install git-filter-repo. Try adding it manually:"
        echo "    pip install --user git-filter-repo"
        exit 1
    fi
fi

# Add explicit patterns for CSV files
echo "Removing large CSV files..."
git filter-repo --path 'data/raw/obis/orites_astreoides_obis_data.csv' --invert-paths --force
git filter-repo --path 'data/raw/acropora_data.csv' --invert-paths --force

# Clean additional data files
echo "Removing other large data files..."
git filter-repo --path-glob 'data/raw/*.csv' --invert-paths --force
git filter-repo --path-glob '*.rds' --invert-paths --force
git filter-repo --path '_targets/objects' --invert-paths --force
git filter-repo --path-glob '*.RData' --invert-paths --force

# Step 3: Update .gitignore
echo "üìÑ Creating proper .gitignore..."
cat > .gitignore << 'GITIGNORE'
# R and RStudio files
.Rproj.user/
.Rhistory
.RData
.Ruserdata
*.Rproj

# targets
_targets/
!_targets.R

# Large data files
*.rds
*.RData
data/raw/*.csv
data/raw/obis/*.csv
data/processed/*.csv
data/raw/environmental/*.tif
data/raw/environmental/*.asc

# Nix
result
result-*
*.backup

# System
.DS_Store
*.swp
*~
GITIGNORE
git add .gitignore
git commit -m "Update .gitignore to exclude large data files"

# Step 4: Update build_env.R to include git-filter-repo
echo "üìù Updating build_env.R..."
sed -i 's/"git",/"git", "git-filter-repo",/' build_env.R
git add build_env.R
git commit -m "Add git-filter-repo to system packages"

# Step 5: Clean and optimize repo
echo "üßπ Optimizing repository..."
git gc --aggressive --prune=now

# Step 6: Instructions
echo ""
echo "‚úÖ Repository cleaned successfully!"
echo ""
echo "Repository size: $(du -sh .git | cut -f1)"
echo ""
echo "To push to GitHub:"
echo "  1. Create a new empty repository on GitHub (don't initialize it)"
echo "  2. Run these commands:"
echo ""
echo "     git remote add origin https://github.com/YOUR-USERNAME/biologia-marina-reproducible.git"
echo "     git push -u --force origin main"
echo ""
echo "If issues persist, consider a fresh start with a new repository:"
echo "  1. Create a new empty directory"
echo "  2. Copy all your current files (except .git folder)"
echo "  3. Initialize a new Git repository there"
echo ""
````

## Using the solution

1. **Create and run the fix script**:
   ```bash
   chmod +x fix_repository.sh
   ./fix_repository.sh
   ```

2. **If the script doesn't fully resolve the issue**, go with the clean-start approach:

   ```bash
   # Create new directory
   mkdir ~/biologia-marina-fresh
   
   # Copy files (excluding .git directory)
   rsync -av --exclude='.git/' ~/Projects/biologia-marina-reproducible/ ~/biologia-marina-fresh/
   
   # Initialize new repository
   cd ~/biologia-marina-fresh
   git init
   git add .
   git commit -m "Initial commit with clean history"
   
   # Add GitHub remote and push
   git remote add origin https://github.com/santi-rios/biologia-marina-reproducible.git
   git push -u origin main --force
   ```

## Updating the build_env.R file

To ensure `git-filter-repo` stays in your build_env.R:

````r
system_pkgs = c(
    "git",
    "git-filter-repo",  # For cleaning Git history
    "python3",
    "quarto",
    "bfg-repo-cleaner" # alternative to git-filter-branch
  ),
````

Remember that the BFG tool is having compatibility issues with your Java version, which is why adding `git-filter-repo` is important as a more reliable alternative.

## Quarto

Scholarly articles require much more detail in their front matter than simply a title and an author. Quarto provides a rich set of YAML metadata keys to describe these details. On this page, you‚Äôll learn how to specify authors and their affiliations, article summaries like an abstract and keywords, and how to include information on copyright, licensing and funding.

## github

The workflows directory contains GitHub Actions workflows that automate tasks when you push your code to GitHub. You have two workflow files:

ci.yml (Continuous Integration)
This file likely defines an automated process that runs whenever you push code or create a pull request. Without seeing the exact contents, it probably:

Checks that your code builds correctly
Runs any tests you've defined
Validates that your R packages can be installed
May check code style/quality

What happens when you push to GitHub?
Once these files are in your repository:

GitHub automatically recognizes and activates these workflows
They'll run according to their defined triggers (e.g., on push, schedule, etc.)
You'll see workflow runs in the "Actions" tab of your repository
The workflows execute on GitHub's servers
You'll receive notifications if workflows fail
Any artifacts or outputs will be stored according to the workflow configuration
Displaying Your Quarto Document on GitHub
Yes, you can display your Quarto document on your GitHub repository page! Here are three options:

Option 1: GitHub Pages (Recommended)
This approach creates a website from your repository:

 recommend Option 1 (GitHub Pages) because:

It provides a professional-looking website for your analysis
The full interactive HTML report will be available
You can link to it from your README.md
GitHub Actions can automatically update it when you push changes

Additional Recommendations
Create a README.md with:

Project description
Link to the rendered Quarto document (https://santi-rios.github.io/NereidaPipeline/analisis_biodiversidad_marina.html)
Setup instructions
Customize the GitHub Pages Theme:

Create a _config.yml file in your repository root with theme settings


### Adquisici√≥n de Datos

```{r}
#| label: data-acquisition-summary

if (exists("pipeline_report") && !is.null(pipeline_report)) {
  data_summary <- pipeline_report$data_acquisition
  
  kable(
    data.frame(
      Fuente = c("OBIS", "GBIF", "Combinados", "Limpios"),
      Registros = c(
        data_summary$obis_records,
        data_summary$gbif_records, 
        data_summary$combined_records,
        data_summary$cleaned_records
      )
    ),
    caption = "Resumen de adquisici√≥n de datos de ocurrencia",
    format = "html"
  ) 
  # |>
  # kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
}
```

El pipeline integra datos de dos fuentes principales:

- **OBIS (Ocean Biodiversity Information System)**: Base de datos especializada en biodiversidad marina
- **GBIF (Global Biodiversity Information Facility)**: Repositorio global de datos de biodiversidad

### Limpieza y Validaci√≥n de Datos

Utilizamos el paquete `CoordinateCleaner` para implementar un protocolo de limpieza comprehensivo:

```{r}
#| label: data-quality-metrics

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # Calcular m√©tricas de calidad
  quality_metrics <- data.frame(
    M√©trica = c(
      "Completitud de coordenadas",
      "Completitud taxon√≥mica", 
      "Completitud temporal",
      "Registros con profundidad"
    ),
    Porcentaje = c(
      round(sum(!is.na(cleaned_occurrences$decimalLatitude) & 
                !is.na(cleaned_occurrences$decimalLongitude)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$scientificName)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$year)) / nrow(cleaned_occurrences) * 100, 1),
      round(sum(!is.na(cleaned_occurrences$depth) & cleaned_occurrences$depth > 0, na.rm = TRUE) / nrow(cleaned_occurrences) * 100, 1)
    )
  )
  
  kable(
    quality_metrics,
    caption = "M√©tricas de calidad de datos despu√©s de la limpieza",
    format = "html"
  ) 
  # |>
  # kable_styling(bootstrap_options = c("striped", "hover"))
}
```

## biomatr

Organism-centric: Functions take species names directly, no need to search for datasets
Multiple databases: Can query RefSeq, GenBank, or Ensembl
Multiple data types: Genome, proteome, CDS, GFF, RNA
Availability checking: Check what's available before downloading
Assembly stats: Get quality metrics for assemblies
Better error handling: Clear messages about what's available and what failed

Based on the biomartr documentation, here are suggested next steps:

Sequence Analysis - Use the retrieved sequences for comparative genomics
Functional Annotation - Extract GO terms and protein domains
Homology Analysis - Find orthologs across species
Phylogenetic Analysis - Use myTAI for evolutionary transcriptomics
Would you like me to implement any of these next steps? The most logical continuation would be:

Functional annotation - Extract GO terms and protein domains from the successfully retrieved A. palmata data
Sequence statistics - Analyze the proteome and CDS sequences
Integration with occurrence data - Link genomic data with the spatial occurrence data you already have

Now when you run the pipeline:

It will check all databases (refseq, genbank, ensembl)
Generate a comprehensive availability report
Download all available data
Assess data quality
Generate a beautiful HTML report with:
Species information
Database availability tables
Success/failure summaries
Data size statistics
Visual heatmaps
Proper citations for all software and data sources
The report will be saved as reports/genomic_data_retrieval.html and can be opened in any browser!

 Next Steps for Analysis
Based on the biomartr documentation, here are suggested next steps:

Sequence Analysis - Use the retrieved sequences for comparative genomics
Functional Annotation - Extract GO terms and protein domains
Homology Analysis - Find orthologs across species
Phylogenetic Analysis - Use myTAI for evolutionary transcriptomics
Would you like me to implement any of these next steps? The most logical continuation would be:

Functional annotation - Extract GO terms and protein domains from the successfully retrieved A. palmata data
Sequence statistics - Analyze the proteome and CDS sequences
Integration with occurrence data - Link genomic data with the spatial occurrence data you already have

## Resultados

El ambiente (software, dependencias del sistema, librer√≠as, etc.) se controla con el archivo '[build_env.R](https://github.com/santi-rios/NereidaPipeline/blob/main/build_env.R)'. el cu√°l facilita la ejecuci√≥n de distintas tareas controladas por los scripts del directorio '[scripts/](https://github.com/santi-rios/NereidaPipeline/tree/main/scripts/)'

La pipline es controlada con el archivo '[_targets.R](https://github.com/santi-rios/NereidaPipeline/blob/main/_targets.R)', el cual controla el flujo utilizando diversas funciones escritas en el direcotio '[R/](https://github.com/santi-rios/NereidaPipeline/tree/main/R)':

- '[R/data_acquisition.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_acquisition.R)': Integrating OBIS, GBIF, biomaRt, wikitaxa, and PRISM for comprehensive data collection.
- '[R/data_cleaning.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_cleaning.R)': Standardizing and validating marine occurrence records from biological collection databases.
- '[R/data_visualization.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/data_visualization.R)'
- '[R/database_integration.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/database_integration.R)': Efficient storage and querying of heterogeneous marine biodiversity data.
- '[R/evolutionary_analysis.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/evolutionary_analysis.R)': phylostratigraphic analysis and evolutionary studies in marine organisms.
- '[R/geospatial_processing.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/geospatial_processing.R)': For handling marine occurrence data with spatial coordinates and environmental layers.
- '[R/metagenomic_analysis.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/metagenomic_analysis.R)'
- '[R/taxonomic_management.R](https://github.com/santi-rios/NereidaPipeline/blob/main/R/taxonomic_management.R)': standardized handling of marine biodiversity taxonomic information.

### Sujetos Experimentales

```{r}
#| label: executive-summary
#| results: asis

if (safe_check("pipeline_report")) {
  cat("**Especies analizadas:** ", length(pipeline_report$species_analyzed), "\n\n")
  cat("**Registros procesados:** ", pipeline_report$data_acquisition$cleaned_records, "\n\n")
  cat("**Estado del pipeline:** ", pipeline_report$pipeline_status, "\n\n")
  cat("**Fecha de an√°lisis:** ", format(pipeline_report$pipeline_completion_date, "%d/%m/%Y"), "\n\n")
} else {
  cat("‚ö†Ô∏è **Pipeline en progreso o no completado**\n\n")
  cat("Para generar este reporte completo, primero ejecuta:\n\n")
  cat("```r\n")
  cat("library(targets)\n")
  cat("tar_make()\n")
  cat("```\n\n")
}
```

Species Analyzed

```{r species-list}
species_list <- c(
  "Acropora cervicornis",
  "Acropora palmata",
  "Porites astreoides"
)

kable(data.frame(
  "Species" = species_list,
  "Common Name" = c("Staghorn Coral", "Elkhorn Coral", "Mustard Hill Coral")
), caption = "Target Species for Genomic Analysis") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```



### Datos disponibles

```{r}
#| label: data-overview

# Create overview of available data
data_overview <- data.frame(
  Objeto = names(loaded_targets),
  Disponible = sapply(loaded_targets, function(x) if(x) "S√≠" else "No"),
  Tipo = sapply(names(loaded_targets), function(name) {
    if (loaded_targets[[name]] && exists(name)) {
      obj <- get(name)
      if (is.data.frame(obj)) {
        paste("data.frame (", nrow(obj), " filas)")
      } else if (is.list(obj)) {
        paste("lista (", length(obj), " elementos)")
      } else {
        class(obj)[1]
      }
    } else {
      "No disponible"
    }
  })
)

kable(
  data_overview,
  caption = "Resumen de objetos del pipeline",
  format = "html"
) 
# |>
#   kable_styling(bootstrap_options = c("striped", "hover"))
```

### An√°lisis de Datos de Ocurrencia

```{r}
#| label: occurrence-analysis

if (safe_check("cleaned_occurrences")) {
  
  # Basic summary
  cat("### Resumen de Datos Limpios\n\n")
  cat("- **Total de registros:** ", nrow(cleaned_occurrences), "\n")
  cat("- **Especies √∫nicas:** ", length(unique(cleaned_occurrences$scientificName)), "\n")
  cat("- **Rango temporal:** ", min(cleaned_occurrences$year, na.rm = TRUE), "-", 
      max(cleaned_occurrences$year, na.rm = TRUE), "\n\n")
  
  # Species summary
  species_summary <- cleaned_occurrences |>
    group_by(scientificName) |>
    summarise(
      Registros = n(),
      `A√±os √∫nicos` = length(unique(year[!is.na(year)])),
      `Lat m√≠n` = round(min(decimalLatitude, na.rm = TRUE), 2),
      `Lat m√°x` = round(max(decimalLatitude, na.rm = TRUE), 2),
      `Lon m√≠n` = round(min(decimalLongitude, na.rm = TRUE), 2),
      `Lon m√°x` = round(max(decimalLongitude, na.rm = TRUE), 2),
      .groups = "drop"
    )
  
  kable(
    species_summary,
    caption = "Resumen por especie",
    format = "html"
  ) 
  # |>
  #   kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
  
} else {
  cat("‚ö†Ô∏è Datos de ocurrencia no disponibles. Ejecuta el pipeline primero.\n")
}
```


Summary by Species and Data Type

```{r retrieval-summary}
if (!is.null(genomic_quality)) {
  kable(genomic_quality, 
        caption = "Genomic data retrieval summary",
        digits = 2) %>%
    kable_styling(bootstrap_options = c("striped", "hover")) %>%
    column_spec(2, bold = TRUE) %>%
    column_spec(3, color = ifelse(genomic_quality$data_completeness == 100, 
                                   "green", "orange"))
}
```



### Datos √ìmicos

Se encontraron estas especies en las bases de datos 

<!-- TODO: poner autom√°ticamente las bases de datos) -->

```{r availability-summary}
if (!is.null(genome_availability)) {
  avail_summary <- genome_availability %>%
    group_by(species, database) %>%
    summarise(
      assemblies = n(),
      .groups = "drop"
    ) %>%
    tidyr::pivot_wider(
      names_from = database,
      values_from = assemblies,
      values_fill = 0
    )
  
  kable(avail_summary, 
        caption = "Number of genome assemblies found in each database") %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
} else {
  cat("No availability data available.\n")
}
```

Detailed Assembly Information

```{r detailed-availability}
if (!is.null(genome_availability)) {
  genome_availability %>%
    select(species, database, organism_name, assembly_accession, 
           bioproject, biosample, seq_rel_date) %>%
    kable(caption = "Detailed genome assembly information") %>%
    kable_styling(
      bootstrap_options = c("striped", "hover", "condensed"),
      font_size = 10
    ) %>%
    scroll_box(width = "100%", height = "400px")
}
```

#### Resumen por especies y tipo de datos

Data Completeness Visualization

```{r completeness-viz, fig.width=10, fig.height=6}
if (!is.null(genomic_quality)) {
  # Reshape data for plotting
  plot_data <- genomic_quality %>%
    tidyr::pivot_longer(
      cols = c(proteome_available, cds_available, gff_available),
      names_to = "data_type",
      values_to = "available"
    ) %>%
    mutate(
      data_type = gsub("_available", "", data_type),
      data_type = tools::toTitleCase(data_type)
    )
  
  ggplot(plot_data, aes(x = species, y = data_type, fill = available)) +
    geom_tile(color = "white", size = 1) +
    scale_fill_manual(
      values = c("TRUE" = "#2ecc71", "FALSE" = "#e74c3c"),
      labels = c("TRUE" = "Available", "FALSE" = "Not Available")
    ) +
    labs(
      title = "Genomic Data Retrieval Status",
      subtitle = "Data types retrieved for each species",
      x = "Species",
      y = "Data Type",
      fill = "Status"
    ) +
    theme_minimal() +
    theme(
      axis.text.x = element_text(angle = 45, hjust = 1, size = 11),
      axis.text.y = element_text(size = 11),
      plot.title = element_text(face = "bold", size = 16),
      plot.subtitle = element_text(size = 12),
      legend.position = "bottom"
    )
}
```

#### Data Size Statistics

```{r data-sizes}
if (!is.null(genomic_quality)) {
  size_data <- genomic_quality %>%
    filter(!is.na(proteome_size)) %>%
    select(species, proteome_size, cds_count)
  
  if (nrow(size_data) > 0) {
    kable(size_data,
          col.names = c("Species", "Proteome Size (MB)", "CDS Count"),
          caption = "Downloaded data statistics",
          digits = 2) %>%
      kable_styling(bootstrap_options = c("striped", "hover"))
  }
}
```

#### Successfully Retrieved Data

```{r successful-retrievals}
if (!is.null(genomic_sequences)) {
  successful <- data.frame(
    species = character(),
    database = character(),
    data_type = character(),
    file_path = character(),
    stringsAsFactors = FALSE
  )
  
  for (sp_name in names(genomic_sequences)) {
    if (sp_name == "availability" || sp_name == "retrieval_date" || 
        sp_name == "database") next
    
    sp_data <- genomic_sequences[[sp_name]]
    
    for (dtype in c("proteome", "cds", "gff")) {
      if (!is.null(sp_data[[dtype]]) && sp_data[[dtype]]$status == "success") {
        successful <- rbind(successful, data.frame(
          species = sp_name,
          database = sp_data$database_used,
          data_type = dtype,
          file_path = basename(sp_data[[dtype]]$file),
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
  if (nrow(successful) > 0) {
    kable(successful,
          caption = "Successfully retrieved genomic data files") %>%
      kable_styling(bootstrap_options = c("striped", "hover")) %>%
      column_spec(3, bold = TRUE, color = "#27ae60")
  } else {
    cat("No data successfully retrieved.\n")
  }
}
```

#### Data Not Found

```{r failed-retrievals}
if (!is.null(genomic_sequences)) {
  failed <- data.frame(
    species = character(),
    data_type = character(),
    reason = character(),
    stringsAsFactors = FALSE
  )
  
  for (sp_name in names(genomic_sequences)) {
    if (sp_name == "availability" || sp_name == "retrieval_date" || 
        sp_name == "database") next
    
    sp_data <- genomic_sequences[[sp_name]]
    
    # Check if species was not available at all
    if (!is.null(sp_data$status) && sp_data$status == "not_available") {
      failed <- rbind(failed, data.frame(
        species = sp_name,
        data_type = "All",
        reason = "Species not found in any database",
        stringsAsFactors = FALSE
      ))
      next
    }
    
    for (dtype in c("proteome", "cds", "gff")) {
      if (!is.null(sp_data[[dtype]]) && sp_data[[dtype]]$status == "error") {
        failed <- rbind(failed, data.frame(
          species = sp_name,
          data_type = dtype,
          reason = sp_data[[dtype]]$message,
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
  if (nrow(failed) > 0) {
    kable(failed,
          caption = "Data retrieval failures") %>%
      kable_styling(bootstrap_options = c("striped", "hover")) %>%
      column_spec(2, color = "#e74c3c")
  } else {
    cat("‚úì All requested data successfully retrieved!\n")
  }
}
```


### Distribuci√≥n Espacial de Especies

```{r}
#| label: species-distribution
#| fig-cap: "Distribuci√≥n espacial de registros de ocurrencia por especie"

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # Filtrar coordenadas v√°lidas
  valid_coords <- cleaned_occurrences |>
    filter(
      !is.na(decimalLatitude), 
      !is.na(decimalLongitude),
      !is.na(scientificName),
      decimalLatitude >= -90, decimalLatitude <= 90,
      decimalLongitude >= -180, decimalLongitude <= 180
    )
  
  if (nrow(valid_coords) > 0) {
    # Crear mapa interactivo
    species_colors <- c("#E31A1C", "#1F78B4", "#33A02C", "#FF7F00", "#6A3D9A")
    species_list <- unique(valid_coords$scientificName)
    
    m <- leaflet(valid_coords) |>
      addProviderTiles(providers$CartoDB.Positron) |>
      setView(
        lng = mean(valid_coords$decimalLongitude), 
        lat = mean(valid_coords$decimalLatitude), 
        zoom = 6
      )
    
    # A√±adir puntos por especie
    for (i in seq_along(species_list)) {
      species_data <- valid_coords |> filter(scientificName == species_list[i])
      
      m <- m |>
        addCircleMarkers(
          data = species_data,
          lng = ~decimalLongitude,
          lat = ~decimalLatitude,
          color = species_colors[i],
          radius = 3,
          stroke = FALSE,
          fillOpacity = 0.7,
          group = species_list[i],
          popup = ~paste0(
            "<b>Especie:</b> ", scientificName, "<br>",
            "<b>Fuente:</b> ", data_source, "<br>",
            "<b>A√±o:</b> ", year
          )
        )
    }
    
    # A√±adir control de capas
    m <- m |>
      addLayersControl(
        overlayGroups = species_list,
        options = layersControlOptions(collapsed = FALSE)
      )
    
    m
  }
}
```

### An√°lisis de Idoneidad de H√°bitat

```{r}
#| label: fig-habitat-suitability
#| fig-cap: "Mapa de idoneidad de h√°bitat para las especies analizadas"

if (exists("habitat_models") && !is.null(habitat_models) && length(habitat_models) > 0) {
  
  # Crear visualizaci√≥n de idoneidad por especies usando solo datos serializables
  species_with_models <- names(habitat_models)[!sapply(habitat_models, is.null)]
  
  if (length(species_with_models) > 0) {
    
    # Crear gr√°fico de barras usando informaci√≥n del modelo sin acceder a SpatRaster
    suitability_data <- purrr::map_dfr(species_with_models, function(species) {
      model <- habitat_models[[species]]
      
      # Verificar que el modelo existe y tiene datos del modelo
      if (!is.null(model) && !is.null(model$model_data)) {
        
        # Calcular estad√≠sticas b√°sicas del modelo sin acceder a predicciones raster
        presence_points <- sum(model$model_data$presence == 1, na.rm = TRUE)
        background_points <- sum(model$model_data$presence == 0, na.rm = TRUE)
        
        # Calcular probabilidades del modelo en los puntos de datos
        if (!is.null(model$model) && inherits(model$model, c("lm", "glm"))) {
          predicted_probs <- predict(model$model, type = "response")
          
          data.frame(
            Especie = gsub("_", " ", species),
            `Puntos de presencia` = presence_points,
            `Puntos de fondo` = background_points,
            `Probabilidad media` = mean(predicted_probs, na.rm = TRUE),
            `Probabilidad m√°xima` = max(predicted_probs, na.rm = TRUE),
            `AIC del modelo` = AIC(model$model)
          )
        } else {
          # Si no hay modelo v√°lido, usar solo informaci√≥n b√°sica
          data.frame(
            Especie = gsub("_", " ", species),
            `Puntos de presencia` = presence_points,
            `Puntos de fondo` = background_points,
            `Probabilidad media` = NA,
            `Probabilidad m√°xima` = NA,
            `AIC del modelo` = NA
          )
        }
      } else {
        NULL
      }
    })
    
    if (!is.null(suitability_data) && nrow(suitability_data) > 0) {
      
      # Crear tabla resumen
      kable(
        suitability_data,
        caption = "Resumen de modelos de idoneidad de h√°bitat por especie",
        format = "html",
        digits = 3
      ) 
      # |>
      # kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
      
      # Crear gr√°fico de barras para m√©tricas del modelo
      if (any(!is.na(suitability_data$`Probabilidad media`))) {
        
        # Preparar datos para el gr√°fico
        plot_data <- suitability_data %>%
          select(Especie, `Puntos de presencia`, `Probabilidad media`, `AIC del modelo`) %>%
          pivot_longer(cols = -Especie, names_to = "M√©trica", values_to = "Valor") %>%
          filter(!is.na(Valor))
        
        if (nrow(plot_data) > 0) {
          p <- plot_data %>%
            ggplot(aes(x = Especie, y = Valor, fill = Especie)) +
            geom_col(alpha = 0.8) +
            facet_wrap(~M√©trica, scales = "free_y", ncol = 2) +
            scale_fill_viridis_d() +
            labs(
              title = "M√©tricas de Modelos de Idoneidad de H√°bitat",
              x = "Especie",
              y = "Valor"
            ) +
            theme_minimal() +
            theme(
              axis.text.x = element_text(angle = 45, hjust = 1),
              legend.position = "none"
            )
          
          print(p)
        }
      }
      
    } else {
      cat("No se pudieron calcular m√©tricas de idoneidad debido a problemas en los datos del modelo.\n")
    }
    
  } else {
    cat("No hay modelos de h√°bitat disponibles para visualizaci√≥n.\n")
  }
  
} else {
  cat("Los modelos de idoneidad de h√°bitat no est√°n disponibles.\n")
}
```

### An√°lisis Espacial Comprehensivo

```{r}
#| label: spatial-analysis-results

if (exists("spatial_analysis") && !is.null(spatial_analysis)) {
  
  # Extraer estad√≠sticas espaciales
  spatial_stats <- spatial_analysis$spatial_statistics
  
  if (!is.null(spatial_stats)) {
    
    # Crear tabla de m√©tricas espaciales
    spatial_metrics <- data.frame(
      M√©trica = c(
        "N√∫mero de ocurrencias",
        "Rango latitudinal (¬∞)",
        "Rango longitudinal (¬∞)",
        "Centroide latitudinal (¬∞)",
        "Centroide longitudinal (¬∞)",
        "√Årea de ocupaci√≥n (km¬≤)",
        "N√∫mero de celdas de grilla"
      ),
      Valor = c(
        spatial_stats$n_occurrences,
        round(diff(spatial_stats$lat_range), 2),
        round(diff(spatial_stats$lon_range), 2),
        round(spatial_stats$centroid_lat, 3),
        round(spatial_stats$centroid_lon, 3),
        spatial_stats$area_of_occupancy_km2,
        spatial_stats$n_grid_cells
      )
    )
    
    kable(
      spatial_metrics,
      caption = "M√©tricas espaciales del an√°lisis de distribuci√≥n",
      format = "html"
    ) 
    # |>
    # kable_styling(bootstrap_options = c("striped", "hover"))
  }
}
```

### Nicho Ambiental

```{r}
#| label: environmental-niche-analysis
#| fig-cap: "An√°lisis del nicho ambiental de las especies marinas"

if (exists("spatial_analysis") && !is.null(spatial_analysis$environmental_niche)) {
  
  env_niche <- spatial_analysis$environmental_niche
  
  # Convertir a formato largo para visualizaci√≥n
  niche_data <- purrr::map_dfr(names(env_niche), function(var) {
    var_stats <- env_niche[[var]]
    
    data.frame(
      Variable = case_when(
        var == "depth_m" ~ "Profundidad (m)",
        var == "sst_celsius" ~ "Temperatura (¬∞C)",
        var == "salinity_psu" ~ "Salinidad (PSU)",
        var == "chlorophyll_mg_m3" ~ "Clorofila-a (mg/m¬≥)",
        TRUE ~ var
      ),
      Media = var_stats$mean,
      Desviacion_estandar = var_stats$sd,  # <-- SIN ESPACIOS
      Minimo = var_stats$min,
      Maximo = var_stats$max,
      Mediana = var_stats$median,
      Q25 = var_stats$q25,
      Q75 = var_stats$q75
    )
  })
  
  if (nrow(niche_data) > 0) {
    # Crear nombres m√°s amigables para la tabla
    niche_data_display <- niche_data
    names(niche_data_display) <- c(
      "Variable", "Media", "Desviaci√≥n est√°ndar", 
      "M√≠nimo", "M√°ximo", "Mediana", "Q25", "Q75"
    )
    
    # Tabla de estad√≠sticas
    kable(
      niche_data_display,
      caption = "Preferencias ambientales de las especies analizadas",
      format = "html",
      digits = 2
    ) 
    # |>
    # kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
    
    # Gr√°fico usando los nombres sin espacios
    niche_long <- niche_data |>
      select(Variable, Media, Desviacion_estandar, Minimo, Maximo, Mediana) |>  # <-- NOMBRES SIN ESPACIOS
      pivot_longer(cols = -Variable, names_to = "Estadistica", values_to = "Valor")
    
    if (nrow(niche_long) > 0) {
      p_niche <- niche_long |>
        filter(Estadistica %in% c("Minimo", "Mediana", "Maximo")) |>  # <-- SIMPLIFICAR
        ggplot(aes(x = Variable, y = Valor, fill = Estadistica)) +
        geom_col(position = "dodge", alpha = 0.8) +
        scale_fill_viridis_d() +
        labs(
          title = "Distribuci√≥n de Preferencias Ambientales",
          x = "Variable Ambiental",
          y = "Valor",
          fill = "Estad√≠stica"
        ) +
        theme_minimal() +
        theme(
          axis.text.x = element_text(angle = 45, hjust = 1),
          legend.position = "bottom"
        ) +
        facet_wrap(~Variable, scales = "free", ncol = 2)
      
      print(p_niche)
    }
    
    cat("\n\n*Nota: Las estad√≠sticas ambientales muestran las condiciones preferidas por las especies en sus h√°bitats naturales.*\n\n")
  }
  
} else {
  cat("Los datos de nicho ambiental no est√°n disponibles.\n\n")
  cat("Esto puede ocurrir si:\n")
  cat("- El an√°lisis espacial no se complet√≥ correctamente\n")
  cat("- No hay suficientes datos ambientales\n")
  cat("- Los objetos SpatRaster no se cargaron correctamente\n\n")
}
```

## Base de Datos Integrada

```{r}
#| label: database-summary

if (exists("data_summaries") && !is.null(data_summaries)) {
  
  # Resumen de la base de datos
  if (!is.null(data_summaries$species)) {
    species_summary <- data_summaries$species
    
    if (nrow(species_summary) > 0) {
      
      # Crear tabla interactiva de especies
      datatable(
        species_summary,
        caption = "Resumen de especies en la base de datos integrada",
        options = list(
          pageLength = 10,
          scrollX = TRUE,
          dom = 'Bfrtip',
          buttons = c('copy', 'csv', 'excel')
        ),
        filter = 'top',
        rownames = FALSE
      ) |>
      formatRound(columns = c('n_records'), digits = 0)
    }
  }
}
```

La base de datos integrada utiliza **DuckDB** como backend NoSQL, permitiendo:

- **Almacenamiento eficiente** de datos heterog√©neos (ocurrencias, taxonom√≠a, ambiente)
- **Consultas flexibles** usando sintaxis SQL y JSON
- **Exportaci√≥n m√∫ltiple** a formatos CSV, JSON, y Parquet
- **Escalabilidad** para grandes vol√∫menes de datos

## I===mplicaciones para la Conservaci√≥n

### Especies Prioritarias

```{r}
#| label: conservation-priorities

if (exists("cleaned_occurrences") && !is.null(cleaned_occurrences) && nrow(cleaned_occurrences) > 0) {
  
  # An√°lisis de prioridades de conservaci√≥n CON DATOS TEMPORALES
  conservation_analysis <- cleaned_occurrences |>
    # Conversi√≥n simple y segura de year
    mutate(
      year_numeric = as.numeric(year)  # ¬°As√≠ de simple!
    ) |>
    group_by(scientificName) |>
    summarise(
      `Registros totales` = n(),
      `Rango geogr√°fico` = round(
        sqrt((max(decimalLatitude, na.rm = TRUE) - min(decimalLatitude, na.rm = TRUE))^2 + 
             (max(decimalLongitude, na.rm = TRUE) - min(decimalLongitude, na.rm = TRUE))^2), 2
      ),
      `A√±os de registro` = max(year_numeric, na.rm = TRUE) - min(year_numeric, na.rm = TRUE),
      `A√±o m√°s reciente` = max(year_numeric, na.rm = TRUE),
      `A√±o m√°s antiguo` = min(year_numeric, na.rm = TRUE),
      `Profundidad media (m)` = round(mean(depth, na.rm = TRUE), 1),
      .groups = "drop"
    ) |>
    mutate(
      `Prioridad conservaci√≥n` = case_when(
        `Registros totales` < 50 & `Rango geogr√°fico` < 2 ~ "ALTA",
        `Registros totales` < 100 & `Rango geogr√°fico` < 5 ~ "MEDIA",
        `A√±o m√°s reciente` < 2015 & `Registros totales` < 200 ~ "MEDIA",  # Especies sin registros recientes
        TRUE ~ "BAJA"
      )
    ) |>
    arrange(desc(`Prioridad conservaci√≥n`), desc(`A√±o m√°s reciente`))
  
  # Mostrar la tabla
  if (nrow(conservation_analysis) > 0) {
    kable(
      conservation_analysis,
      caption = "An√°lisis de prioridades de conservaci√≥n basado en datos de ocurrencia",
      format = "html"
    )
    
    # Agregar an√°lisis temporal adicional
    cat("\n\n### An√°lisis Temporal\n\n")
    
    temporal_summary <- cleaned_occurrences |>
      mutate(year_numeric = as.numeric(year)) |>
      summarise(
        `Per√≠odo de estudio` = paste(min(year_numeric, na.rm = TRUE), "-", max(year_numeric, na.rm = TRUE)),
        `A√±os con datos` = length(unique(year_numeric)),
        `Registros por d√©cada` = paste(
          "1970s:", sum(year_numeric >= 1970 & year_numeric < 1980, na.rm = TRUE), "|",
          "1980s:", sum(year_numeric >= 1980 & year_numeric < 1990, na.rm = TRUE), "|", 
          "1990s:", sum(year_numeric >= 1990 & year_numeric < 2000, na.rm = TRUE), "|",
          "2000s:", sum(year_numeric >= 2000 & year_numeric < 2010, na.rm = TRUE), "|",
          "2010s:", sum(year_numeric >= 2010 & year_numeric < 2020, na.rm = TRUE), "|",
          "2020s:", sum(year_numeric >= 2020, na.rm = TRUE)
        )
      )
    
    cat("**Resumen temporal del dataset:**\n\n")
    cat("- Per√≠odo:", temporal_summary$`Per√≠odo de estudio`, "\n")
    cat("- A√±os √∫nicos con datos:", temporal_summary$`A√±os con datos`, "\n") 
    cat("- Distribuci√≥n por d√©cadas:", temporal_summary$`Registros por d√©cada`, "\n\n")
    
  } else {
    cat("No se pudieron calcular las prioridades de conservaci√≥n debido a problemas en los datos.\n")
  }
  
} else {
  cat("‚ö†Ô∏è Datos de ocurrencia no disponibles para an√°lisis de conservaci√≥n.\n")
}
```

### Recomendaciones de Manejo

Bas√°ndose en los resultados del an√°lisis, se proponen las siguientes estrategias de conservaci√≥n:

#### 1. **Protecci√≥n de H√°bitats Cr√≠ticos**
- Establecer √°reas marinas protegidas en zonas de alta idoneidad
- Monitorear cambios en las condiciones ambientales clave
- Implementar medidas de mitigaci√≥n contra el cambio clim√°tico

#### 2. **Monitoreo y Vigilancia**
- Desarrollar programas de monitoreo a largo plazo
- Utilizar tecnolog√≠as de detecci√≥n temprana
- Capacitar a comunidades locales en identificaci√≥n de especies

#### 3. **Restauraci√≥n Ecol√≥gica**
- Identificar √°reas hist√≥ricamente importantes pero actualmente degradadas
- Implementar programas de restauraci√≥n basados en ciencia
- Evaluar el √©xito de las intervenciones de restauraci√≥n

## An√°lisis Evolutivo y Adaptaci√≥n

```{r}
#| label: evolutionary-insights

# Mostrar insights evolutivos si est√°n disponibles
cat("### Perspectivas Evolutivas\n\n")
cat("El an√°lisis filoestratigr√°fico revela patrones importantes sobre la evoluci√≥n y adaptaci√≥n de las especies marinas:\n\n")

cat("- **Genes antiguos** conservados indican funciones esenciales para la supervivencia marina\n")
cat("- **Genes de origen reciente** pueden representar adaptaciones espec√≠ficas al ambiente marino\n") 
cat("- **Patrones de expresi√≥n** durante el desarrollo sugieren estrategias evolutivas de supervivencia\n\n")

cat("Estas perspectivas son cruciales para:\n\n")
cat("1. **Programas de cr√≠a selectiva** para aumentar la resistencia clim√°tica\n")
cat("2. **Estrategias de restauraci√≥n** basadas en diversidad gen√©tica\n")
cat("3. **Predicci√≥n de respuestas** a cambios ambientales futuros\n\n")
```

## Limitaciones y Perspectivas Futuras

### Limitaciones del Estudio

1. **Sesgos de muestreo**: Los datos de ocurrencia pueden estar sesgados hacia √°reas de f√°cil acceso
2. **Resoluci√≥n temporal**: La variabilidad interanual y estacional no est√° completamente capturada
3. **Variables ambientales**: Limitadas a las disponibles en bases de datos globales
4. **Validaci√≥n de modelos**: Se requiere validaci√≥n independiente con datos de campo

### Direcciones Futuras

```{r}
#| label: future-directions

future_research <- data.frame(
  `√Årea de investigaci√≥n` = c(
    "Integraci√≥n de datos gen√≥micos",
    "Modelado de cambio clim√°tico", 
    "An√°lisis de conectividad",
    "Monitoreo en tiempo real",
    "Inteligencia artificial"
  ),
  `Descripci√≥n` = c(
    "Incorporar datos de secuenciaci√≥n para an√°lisis poblacionales",
    "Proyectar distribuciones futuras bajo escenarios clim√°ticos",
    "Analizar flujo gen√©tico y dispersi√≥n larval",
    "Implementar sensores IoT para datos continuos",
    "Desarrollar modelos de aprendizaje autom√°tico avanzados"
  ),
  `Prioridad` = c("Alta", "Alta", "Media", "Media", "Baja")
)

kable(
  future_research,
  caption = "Direcciones prioritarias para investigaci√≥n futura",
  format = "html"
) 
# |>
# kable_styling(bootstrap_options = c("striped", "hover", "responsive"))
```

## Conclusiones

El pipeline integrado de an√°lisis de biodiversidad marina ha demostrado ser una herramienta poderosa para:

1. **Integrar m√∫ltiples fuentes de datos** de manera eficiente y reproducible
2. **Validar y limpiar datos** usando est√°ndares internacionales
3. **Generar modelos predictivos** de distribuci√≥n de especies
4. **Identificar prioridades de conservaci√≥n** basadas en evidencia cient√≠fica
5. **Proporcionar insights evolutivos** relevantes para la conservaci√≥n

Los resultados resaltan la importancia de enfoques integrados que combinen datos de ocurrencia, variables ambientales, y an√°lisis evolutivos para informar estrategias efectivas de conservaci√≥n marina.

### Impacto para la Conservaci√≥n

- **Identificaci√≥n de especies vulnerables** con distribuciones restringidas
- **Mapeo de h√°bitats cr√≠ticos** que requieren protecci√≥n prioritaria  
- **Desarrollo de estrategias adaptativas** para el cambio clim√°tico
- **Optimizaci√≥n de esfuerzos de monitoreo** y recursos limitados

Este enfoque metodol√≥gico puede replicarse para otras regiones y grupos taxon√≥micos, contribuyendo al conocimiento global sobre biodiversidad marina y su conservaci√≥n.

---

## Resumen Ejecutivo

### Estado del pipeline

```{r}
#| label: pipeline-status
#| echo: true

# Show pipeline status
cat("Estado de los objetos del pipeline:\n\n")
for (target in names(loaded_targets)) {
  status <- if (loaded_targets[[target]]) "‚úì Disponible" else "‚úó No disponible"
  cat("-", target, ":", status, "\n")
}

# Check targets status
if (requireNamespace("targets", quietly = TRUE)) {
  cat("\nEstado general del pipeline:\n")
  tar_progress() |>
    count(progress) |>
    kable(col.names = c("Estado", "N√∫mero de targets"))
}
```

## Referencias

*Las referencias bibliogr√°ficas se incluir√≠an aqu√≠ en un an√°lisis completo, citando las fuentes de datos, m√©todos estad√≠sticos, y literatura cient√≠fica relevante.*

## Informaci√≥n de Sesi√≥n

```{r}
#| label: session-info
#| echo: true

sessionInfo()
```

---

**Nota**: Este reporte fue generado autom√°ticamente usando el pipeline integrado de an√°lisis de biodiversidad marina. Para m√°s informaci√≥n sobre la metodolog√≠a y c√≥digo fuente, consultar el repositorio del proyecto.


## Exploraci√≥n para solucionar fechas

```{r}
# Chunk de diagn√≥stico de datos temporales
if (exists("cleaned_occurrences")) {
  cat("### Diagn√≥stico de Datos Temporales\n\n")
  
  # Examinar la columna year
  cat("**Tipo de datos en 'year':**", class(cleaned_occurrences$year), "\n")
  cat("**Valores √∫nicos (primeros 10):**", head(unique(cleaned_occurrences$year), 10), "\n")
  cat("**Valores NA:**", sum(is.na(cleaned_occurrences$year)), "de", nrow(cleaned_occurrences), "\n\n")
  
  # Si hay eventDate, examinarlo tambi√©n
  if ("eventDate" %in% names(cleaned_occurrences)) {
    cat("**Tipo de datos en 'eventDate':**", class(cleaned_occurrences$eventDate), "\n")
    cat("**Valores √∫nicos (primeros 5):**", head(unique(cleaned_occurrences$eventDate), 5), "\n")
  }
}
```

## Discusi√≥n

### Manejo de ambientes

La implementaci√≥n de avanzadas herramientas bioinform√°ticas y la gesti√≥n eficiente de pipelines han revolucionado el estudio de la biodiversidad y la evoluci√≥n de la biota marina. En este marco, el proyecto propone una aplicaci√≥n de R que hace uso de Nix por medio del paquete R {rix}, convergiendo en una soluci√≥n √≥ptima para el manejo de diversas funciones, incluyendo el procesamiento de datos, la gesti√≥n de c√≥digo y la integraci√≥n de informaci√≥n en repositorios.

Gracias al uso de Nix, un sistema de paquetes de c√≥digo abierto que adopta una nueva manera de manejar las dependencias, proporcionando un entorno de trabajo coherentemente reproducible. Asegura que los paquetes se construyen e instalan de manera aislada, lo que permite un control de versiones y una gesti√≥n de las dependencias finamente afinada (Dolstra, E., 2006). As√≠, Nix se impone como una herramienta fundamental cambio de contextos acad√©micos y empresariales, permitiendo la creaci√≥n de entornos de trabajo estables y reproducibles.

### Perspectivas y limitaciones

Potential Applications

- Personalized Medicine: Identifying genetic predispositions to diseases and tailoring treatments based on an individual's genetic makeup. 
- Cancer Research: Identifying mutations in tumors to guide cancer treatment strategies and monitor treatment responses. 
- Infectious Disease: Tracking outbreaks by sequencing pathogens and understanding their genetic relationships. 
- Drug Discovery: Uncovering new drug targets and identifying potential drug repurposing opportunities by understanding disease-causing genetic mechanisms. 

√Årea de Mejora: La principal fricci√≥n a futuro es el acoplamiento manual entre _targets.R y build_env.R. Actualmente, si usted a√±ade library(nuevo_paquete) en su pipeline _targets.R, debe recordar manualmente a√±adir "nuevo_paquete" al vector targets_packages en build_env.R. A medida que el proyecto crezca y colaboren m√°s personas, es casi seguro que este paso se olvidar√°, lo que provocar√° fallos en las pruebas (make test) y frustraci√≥n.

Para que este flujo de trabajo sea verdaderamente a prueba de futuro, el siguiente paso l√≥gico es automatizar la sincronizaci√≥n de paquetes.

Se podr√≠a modificar el script regenerate.sh (o crear uno nuevo) para que, antes de ejecutar build_env.R, primero analice el archivo _targets.R en busca de todas las llamadas library(...). Luego, puede pasar esa lista de paquetes como un argumento a su script de R o escribirla en un archivo temporal que build_env.R pueda leer.

Esto desacoplar√≠a completamente la definici√≥n del ambiente de la pipeline, adhiri√©ndose al principio de "Fuente √önica de Verdad" (Single Source of Truth). Su pipeline en _targets.R se convierte en la √∫nica fuente que define qu√© paquetes se necesitan, y el resto del sistema reacciona autom√°ticamente.

Sin embargo, la arquitectura actual es excelente, avanzada y se adelanta a la mayor√≠a de los flujos de trabajo acad√©micos. Es altamente reproducible y f√°cil de usar. Al implementar la sincronizaci√≥n autom√°tica de paquetes, lo convertir√° en un sistema pr√°cticamente infalible y listo para escalar a cualquier complejidad.

## Conclusiones


‚úÖ Reproducibilidad (Fortaleza Mayor)

Este es el punto m√°s fuerte de su sistema. Al definir el entorno computacional de forma declarativa con rix en build_env.R y materializarlo con Nix, ha eliminado pr√°cticamente la variabilidad del entorno. Cualquier investigador, en cualquier m√°quina con Nix, puede ejecutar make regenerate y nix-shell para recrear un ambiente de software bit a bit id√©ntico. Esto es el est√°ndar de oro para la reproducibilidad computacional.

‚úÖ Facilidad de Uso y Mantenimiento (Fortaleza)

El Makefile es la clave aqu√≠. Proporciona una "API" de l√≠nea de comandos simple y legible para su proyecto. Un nuevo colaborador no necesita entender los detalles de Nix o los scripts de shell; solo necesita leer la salida de make help para empezar a trabajar. Esto reduce dr√°sticamente la curva de aprendizaje y los posibles errores.

üìà Escalabilidad (Fortaleza con un √°rea de mejora)

    Fortaleza: El uso de {targets} es ideal para la escalabilidad del an√°lisis. Su naturaleza basada en dependencias garantiza que solo se recalculen los pasos necesarios, ahorrando un tiempo de c√≥mputo inmenso a medida que el an√°lisis crece en complejidad.

    

## Citations

### Software Citations

Please cite the following software packages used in this analysis:

#### biomartr

> Drost HG, Paszkowski J. Biomartr: genomic data retrieval with R. 
> Bioinformatics (2017) 33(8): 1216-1217. 
> doi:10.1093/bioinformatics/btw821

#### targets

> Landau, W. M., (2021). The targets R package: a dynamic Make-like 
> function-oriented pipeline toolkit for reproducibility and 
> high-performance computing. Journal of Open Source Software, 6(57), 2959,
> https://doi.org/10.21105/joss.02959

#### Biostrings

> Pag√®s H, Aboyoun P, Gentleman R, DebRoy S (2023). Biostrings: Efficient 
> manipulation of biological strings. R package version 2.68.0,
> https://bioconductor.org/packages/Biostrings.

### Data Sources

```{r data-sources}
if (!is.null(genomic_sequences)) {
  databases_used <- unique(unlist(lapply(genomic_sequences, function(x) {
    if (is.list(x) && !is.null(x$database_used)) {
      return(x$database_used)
    }
    return(NULL)
  })))
  
  databases_used <- databases_used[!is.null(databases_used)]
  
  if (length(databases_used) > 0) {
    cat("Data retrieved from the following NCBI databases:\n\n")
    for (db in databases_used) {
      if (db == "refseq") {
        cat("- **NCBI RefSeq**: https://www.ncbi.nlm.nih.gov/refseq/\n")
        cat("  > O'Leary NA, et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation. Nucleic Acids Res. 2016;44(D1):D733-45.\n\n")
      } else if (db == "genbank") {
        cat("- **NCBI GenBank**: https://www.ncbi.nlm.nih.gov/genbank/\n")
        cat("  > Sayers EW, et al. Database resources of the National Center for Biotechnology Information. Nucleic Acids Res. 2022;50(D1):D20-D26.\n\n")
      }
    }
  }
}
```


### File Locations

All downloaded genomic data files are stored in:

- **Proteomes**: `data/raw/genomic/proteomes/`
- **CDS**: `data/raw/genomic/cds/`
- **GFF annotations**: `data/raw/genomic/gff/`
- **Summary reports**: `data/processed/genomic/`

---

*Report generated on `r Sys.time()`*
